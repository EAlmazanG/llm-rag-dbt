{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import yaml\n",
    "import sqlparse\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_repo_root_path():\n",
    "    import os\n",
    "    import sys\n",
    "    repo_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "    if repo_root not in sys.path:\n",
    "        sys.path.append(repo_root)\n",
    "        \n",
    "add_repo_root_path()\n",
    "from src import generate_knowledge\n",
    "from src import create_rag_db\n",
    "from src import llm_chain_tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_columns', 50)\n",
    "#pd.set_option('display.width', None)\n",
    "#pd.set_option('display.max_colwidth', 10) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_knowledge.add_repo_root_path()\n",
    "import openai_setup\n",
    "\n",
    "OPENAI_API_KEY = openai_setup.conf['key']\n",
    "OPENAI_PROJECT = openai_setup.conf['project']\n",
    "OPENAI_ORGANIZATION = openai_setup.conf['organization']\n",
    "DEFAULT_LLM_MODEL = \"gpt-4o-mini\"\n",
    "\n",
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
    "os.environ['OPENAI_MODEL_NAME'] = DEFAULT_LLM_MODEL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "langchain_openai_embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY, model=\"text-embedding-ada-002\")\n",
    "langchain_openai_llm = ChatOpenAI(model=DEFAULT_LLM_MODEL, temperature=0.1, openai_api_key=OPENAI_API_KEY, openai_organization = OPENAI_ORGANIZATION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'interpretation_agent': {'role': 'Request Interpreter\\n', 'goal': 'Interpret user requests related to dbt projects and translate them into actionable decisions. Use expertise in dbt, data modeling, and analytics engineering to determine the type of action required.\\n', 'backstory': \"You specialize in analyzing requests to identify whether the action involves adding a field, modifying an existing model, or retrieving specific information. Your goal is to provide concise and actionable outputs tailored to the user's needs.\\n\", 'verbose': True, 'allow_delegation': False}, 'evaluation_agent': {'role': 'Evaluation Specialist\\n', 'goal': 'Evaluate user requests related to dbt projects and provide concise, actionable insights and steps required to address the request. Leverage expertise in data modeling, dbt project structure, and dependency analysis to ensure accurate evaluations.\\n', 'backstory': 'You specialize in analyzing interpreted requests and breaking them down into specific, actionable tasks while considering dependencies, performance, documentation, and configuration impacts within the dbt project. The goal is to ensure efficient and effective implementations.\\n', 'verbose': True, 'allow_delegation': False}}\n",
      "{'interpretation_task': {'description': \"Evaluate the user's request: {request} and determine the required action: - Adding a field: Identify where the field is currently available (if provided),\\n  determine how to propagate it through necessary models or transformations,\\n  and assess the impact on related models and dependencies.\\n- Modifying an existing model: Identify specific changes needed, evaluate the\\n  impact on structure, relationships, and downstream dependencies.\\n- Retrieving specific information: Identify models containing relevant data,\\n  analyze relationships, and determine queries or transformations needed.\\n\\nReflect on the request to generate a concise plan for the approach and provide a clear summary of the required action and its implications.\\n\", 'expected_output': 'Return one of the following actions: - ADD_COLUMN - MODIFY_MODEL - RETRIVE_INFO\\nAdditionally, include a short explanation of the work required for the action. The output should be minimal and focused, avoiding extra commentary or filler text.\\n'}, 'evaluation_task': {'description': 'Evaluate the interpretation of the user request and determine the necessary steps to handle it effectively.  Include only relevant considerations based on the following topics: - Target models or files - Field existence - Documentation needs - Dependencies and relationships - Performance/design considerations - Tests - dbt project config - Code or logic generation\\nPossible actions and considerations may include: - Identifying the target models or files affected by the request. - Analyzing upstream and downstream dependencies to locate the change points. - Assessing the need for adjustments in documentation, tests, or configuration files. - Evaluating performance impacts and ensuring maintainability. - Generating necessary SQL logic fragments (e.g., CTEs, columns). - Determining if an intermediate model is required for the changes. - Highlighting potential granularity changes or unique key impacts.\\n', 'expected_output': 'A concise summary of the high-level tasks required to fulfill the request. If additional context (e.g., project structure or lineage) is needed, clearly state this and provide only reflections based on the given information.\\nAvoid filler text or unnecessary explanations, and focus exclusively on actionable insights.'}}\n"
     ]
    }
   ],
   "source": [
    "# Define file paths for YAML configurations\n",
    "files = {\n",
    "    'agents': '../config/agents.yml',\n",
    "    'tasks': '../config/tasks.yml'\n",
    "}\n",
    "\n",
    "# Load configurations from YAML files\n",
    "configs = {}\n",
    "for config_type, file_path in files.items():\n",
    "    with open(file_path, 'r') as file:\n",
    "        configs[config_type] = yaml.safe_load(file)\n",
    "\n",
    "# Assign loaded configurations to specific variables\n",
    "agents_config = configs['agents']\n",
    "tasks_config = configs['tasks']\n",
    "\n",
    "print(agents_config)\n",
    "print(tasks_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from crewai import Agent, Task, Crew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:opentelemetry.trace:Overriding of current TracerProvider is not allowed\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Creating Agents\n",
    "interpretation_agent = Agent(\n",
    "  config=agents_config['interpretation_agent'],\n",
    ")\n",
    "\n",
    "evaluation_agent = Agent(\n",
    "  config=agents_config['evaluation_agent'],\n",
    ")\n",
    "\n",
    "# Creating Tasks\n",
    "interpretation_task = Task(\n",
    "  config=tasks_config['interpretation_task'],\n",
    "  agent=interpretation_agent\n",
    ")\n",
    "\n",
    "evaluation_task = Task(\n",
    "  config=tasks_config['evaluation_task'],\n",
    "  agent=evaluation_agent\n",
    ")\n",
    "\n",
    "crew = Crew(\n",
    "  agents=[\n",
    "    interpretation_agent,\n",
    "    evaluation_agent\n",
    "  ],\n",
    "  tasks=[\n",
    "    interpretation_task,\n",
    "    evaluation_task\n",
    "  ],\n",
    "  verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mRequest Interpreter\u001b[00m\n",
      "\u001b[95m## Task:\u001b[00m \u001b[92mEvaluate the user's request: Give me all the information about the models related with customers and determine the required action: - Adding a field: Identify where the field is currently available (if provided),\n",
      "  determine how to propagate it through necessary models or transformations,\n",
      "  and assess the impact on related models and dependencies.\n",
      "- Modifying an existing model: Identify specific changes needed, evaluate the\n",
      "  impact on structure, relationships, and downstream dependencies.\n",
      "- Retrieving specific information: Identify models containing relevant data,\n",
      "  analyze relationships, and determine queries or transformations needed.\n",
      "\n",
      "Reflect on the request to generate a concise plan for the approach and provide a clear summary of the required action and its implications.\n",
      "\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mRequest Interpreter\u001b[00m\n",
      "\u001b[95m## Final Answer:\u001b[00m \u001b[92m\n",
      "RETRIEVE_INFO - The required action involves identifying all models related to customers, analyzing their relationships, and determining the relevant queries or transformations needed to extract comprehensive information about these models.\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mEvaluation Specialist\u001b[00m\n",
      "\u001b[95m## Task:\u001b[00m \u001b[92mEvaluate the interpretation of the user request and determine the necessary steps to handle it effectively.  Include only relevant considerations based on the following topics: - Target models or files - Field existence - Documentation needs - Dependencies and relationships - Performance/design considerations - Tests - dbt project config - Code or logic generation\n",
      "Possible actions and considerations may include: - Identifying the target models or files affected by the request. - Analyzing upstream and downstream dependencies to locate the change points. - Assessing the need for adjustments in documentation, tests, or configuration files. - Evaluating performance impacts and ensuring maintainability. - Generating necessary SQL logic fragments (e.g., CTEs, columns). - Determining if an intermediate model is required for the changes. - Highlighting potential granularity changes or unique key impacts.\n",
      "\u001b[00m\n"
     ]
    }
   ],
   "source": [
    "user_input = 'Give me all the information about the models related with customers'\n",
    "\n",
    "inputs = {\n",
    "  'request': user_input\n",
    "}\n",
    "\n",
    "# Run the crew\n",
    "result = crew.kickoff(\n",
    "  inputs=inputs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Flows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from crewai import Flow\n",
    "from crewai.flow.flow import listen, start\n",
    "\n",
    "class dbtChatFlow(Flow):\n",
    "    @start()\n",
    "    async def interpret_prompt(self):\n",
    "        \"\"\"\n",
    "        Step 1: Process the user's prompt using the interpretation agent.\n",
    "        \"\"\"\n",
    "        # Asegúrate de que los inputs estén en el estado\n",
    "        if \"user_prompt\" not in self.state:\n",
    "            raise ValueError(\"No user_prompt provided in the inputs.\")\n",
    "\n",
    "        user_prompt = self.state[\"user_prompt\"]\n",
    "        interpretation_result = crew.agents[0].kickoff(user_prompt)\n",
    "        self.state[\"interpretation_result\"] = interpretation_result\n",
    "        return interpretation_result\n",
    "\n",
    "    @listen(lambda state: \"interpretation_result\" in state)\n",
    "    async def evaluate_interpretation(self):\n",
    "        \"\"\"\n",
    "        Step 2: Use the evaluation agent to process the interpretation result.\n",
    "        \"\"\"\n",
    "        interpretation_result = self.state.get(\"interpretation_result\")\n",
    "        if not interpretation_result:\n",
    "            raise ValueError(\"interpretation_result is missing in the state.\")\n",
    "\n",
    "        evaluation_result = crew.agents[1].kickoff(interpretation_result)\n",
    "        self.state[\"evaluation_result\"] = evaluation_result\n",
    "        return evaluation_result\n",
    "\n",
    "flow = dbtChatFlow()\n",
    "#flow.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = \"give me all the models related with the orders dbt model\"\n",
    "\n",
    "flow.update_state({\"user_prompt\": user_input})\n",
    "result = await flow.kickoff_async()\n",
    "\n",
    "print(\"Pipeline Result:\", result)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
