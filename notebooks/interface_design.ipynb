{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import yaml\n",
    "import sqlparse\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_repo_root_path():\n",
    "    import os\n",
    "    import sys\n",
    "    repo_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "    if repo_root not in sys.path:\n",
    "        sys.path.append(repo_root)\n",
    "        \n",
    "add_repo_root_path()\n",
    "from src import generate_knowledge\n",
    "from src import create_rag_db\n",
    "from src import llm_chain_tools\n",
    "from src.enhanced_retriever import EnhancedRetriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_columns', 50)\n",
    "#pd.set_option('display.width', None)\n",
    "#pd.set_option('display.max_colwidth', 10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_knowledge.add_repo_root_path()\n",
    "import openai_setup\n",
    "\n",
    "OPENAI_API_KEY = openai_setup.conf['key']\n",
    "OPENAI_PROJECT = openai_setup.conf['project']\n",
    "OPENAI_ORGANIZATION = openai_setup.conf['organization']\n",
    "DEFAULT_LLM_MODEL = \"gpt-4o-mini\"\n",
    "CHROMADB_DIRECTORY = '../chromadb'\n",
    "COLLECTION_NAME = \"my_chromadb\" \n",
    "\n",
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
    "os.environ['OPENAI_MODEL_NAME'] = DEFAULT_LLM_MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "langchain_openai_embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY, model=\"text-embedding-ada-002\")\n",
    "langchain_openai_llm = ChatOpenAI(model=DEFAULT_LLM_MODEL, temperature=0.1, openai_api_key=OPENAI_API_KEY, openai_organization = OPENAI_ORGANIZATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overriding of current TracerProvider is not allowed\n",
      "Overriding of current TracerProvider is not allowed\n",
      "Overriding of current TracerProvider is not allowed\n",
      "Overriding of current TracerProvider is not allowed\n",
      "Overriding of current TracerProvider is not allowed\n",
      "Overriding of current TracerProvider is not allowed\n",
      "Overriding of current TracerProvider is not allowed\n",
      "Overriding of current TracerProvider is not allowed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plot saved as crewai_flow.html\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "loaded_vectorstore = Chroma(\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    persist_directory=CHROMADB_DIRECTORY,\n",
    "    embedding_function=langchain_openai_embeddings\n",
    ")\n",
    "\n",
    "_, repo_name = generate_knowledge.extract_owner_and_repo('https://github.com/dbt-labs/jaffle-shop')\n",
    "dbt_models_df = pd.read_csv('../data/dbt_models_' + repo_name + '.csv')\n",
    "dbt_project_df = pd.read_csv('../data/dbt_project_' + repo_name + '.csv')\n",
    "dbt_repo_knowledge_df = create_rag_db.merge_dbt_models_and_project_dfs(dbt_models_df, dbt_project_df)\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "\n",
    "import importlib\n",
    "import src.llm_agents_flow\n",
    "importlib.reload(src.llm_agents_flow)\n",
    "from src.llm_agents_flow import dbtChatFlow\n",
    "\n",
    "files = {\n",
    "    'agents': '../config/agents.yml',\n",
    "    'tasks': '../config/tasks.yml'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "dbtChatFlow.__init__() missing 1 required positional argument: 'files'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m flow \u001b[38;5;241m=\u001b[39m \u001b[43mdbtChatFlow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m flow\u001b[38;5;241m.\u001b[39mplot()\n\u001b[1;32m      4\u001b[0m user_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI want to add a new column \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moverdue\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to the model orders that come from raw_orders source, and have it available in customers. the overdue column is directly available in raw_orders, is not necessairy to calcylate it \u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mTypeError\u001b[0m: dbtChatFlow.__init__() missing 1 required positional argument: 'files'"
     ]
    }
   ],
   "source": [
    "flow = dbtChatFlow(files)\n",
    "flow.plot()\n",
    "\n",
    "user_input = \"I want to add a new column 'overdue' to the model orders that come from raw_orders source, and have it available in customers. the overdue column is directly available in raw_orders, is not necessairy to calcylate it \"\n",
    "result = flow.kickoff(inputs={\"request\": user_input, \"dbt_repo_knowledge_df\": dbt_repo_knowledge_df, \"vectorstore\": loaded_vectorstore, \"embedding_function\":langchain_openai_embeddings})\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test local model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from crewai import LLM, Agent, Task, Crew\n",
    "\n",
    "local_llm = LLM(model=\"lm_studio/Llama-3.2-3B-Instruct-4bit\", base_url=\"http://127.0.0.1:1234/v1\", api_key=\"asdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(\n",
    "    role=\"Data Analyst\",\n",
    "    goal=\"Analyze eCommerce sales data\",\n",
    "    backstory=\"Expert in data analytics with years of experience\",\n",
    "    llm=local_llm\n",
    ")\n",
    "\n",
    "task = Task(\n",
    "    description=\"Analyze sales trends from the last quarter and identify key insights.\",\n",
    "    agent=agent,\n",
    "    expected_output=\"A detailed report summarizing sales trends, key insights, and recommendations.\"\n",
    ")\n",
    "\n",
    "crew = Crew(\n",
    "    agents=[agent],\n",
    "    tasks=[task],\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "result = crew.kickoff() \n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_flow = dbtChatFlow(custom_llm = local_llm)\n",
    "local_flow.plot()\n",
    "\n",
    "user_input = \"I want to add a new column 'overdue' to the model orders that come from raw_orders source, and have it available in customers. the overdue column is directly available in raw_orders, is not necessairy to calcylate it \"\n",
    "result = local_flow.kickoff(inputs={\"request\": user_input, \"dbt_repo_knowledge_df\": dbt_repo_knowledge_df, \"vectorstore\": loaded_vectorstore, \"embedding_function\":langchain_openai_embeddings})\n",
    "result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
