{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import yaml\n",
    "import sqlparse\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_repo_root_path():\n",
    "    import os\n",
    "    import sys\n",
    "    repo_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "    if repo_root not in sys.path:\n",
    "        sys.path.append(repo_root)\n",
    "        \n",
    "add_repo_root_path()\n",
    "from src import generate_knowledge\n",
    "from src import create_rag_db\n",
    "from src import llm_chain_tools\n",
    "from src.enhanced_retriever import EnhancedRetriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_columns', 50)\n",
    "#pd.set_option('display.width', None)\n",
    "#pd.set_option('display.max_colwidth', 10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_knowledge.add_repo_root_path()\n",
    "import openai_setup\n",
    "\n",
    "OPENAI_API_KEY = openai_setup.conf['key']\n",
    "OPENAI_PROJECT = openai_setup.conf['project']\n",
    "OPENAI_ORGANIZATION = openai_setup.conf['organization']\n",
    "DEFAULT_LLM_MODEL = \"gpt-4o-mini\"\n",
    "CHROMADB_DIRECTORY = '../chromadb'\n",
    "COLLECTION_NAME = \"my_chromadb\" \n",
    "\n",
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
    "os.environ['OPENAI_MODEL_NAME'] = DEFAULT_LLM_MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "langchain_openai_embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY, model=\"text-embedding-ada-002\")\n",
    "langchain_openai_llm = ChatOpenAI(model=DEFAULT_LLM_MODEL, temperature=0.1, openai_api_key=OPENAI_API_KEY, openai_organization = OPENAI_ORGANIZATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "loaded_vectorstore = Chroma(\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    persist_directory=CHROMADB_DIRECTORY,\n",
    "    embedding_function=langchain_openai_embeddings\n",
    ")\n",
    "\n",
    "_, repo_name = generate_knowledge.extract_owner_and_repo('https://github.com/dbt-labs/jaffle-shop')\n",
    "dbt_models_df = pd.read_csv('../data/dbt_models_' + repo_name + '.csv')\n",
    "dbt_project_df = pd.read_csv('../data/dbt_project_' + repo_name + '.csv')\n",
    "dbt_repo_knowledge_df = create_rag_db.merge_dbt_models_and_project_dfs(dbt_models_df, dbt_project_df)\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "import importlib\n",
    "import src.llm_agents_flow\n",
    "importlib.reload(src.llm_agents_flow)\n",
    "from src.llm_agents_flow import dbtChatFlow\n",
    "\n",
    "files = {\n",
    "    'agents': '../config/agents.yml',\n",
    "    'tasks': '../config/tasks.yml'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXECUTE FLOW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### User OpenAI LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow = dbtChatFlow(files)\n",
    "flow.plot()\n",
    "\n",
    "user_input = \"I want to add a new column 'overdue' to the model orders that come from raw_orders source, and have it available in customers. the overdue column is directly available in raw_orders, is not necessairy to calcylate it \"\n",
    "result = flow.kickoff(inputs={\"request\": user_input, \"dbt_repo_knowledge_df\": dbt_repo_knowledge_df, \"vectorstore\": loaded_vectorstore, \"embedding_function\":langchain_openai_embeddings})\n",
    "display(Markdown(f\"<div style='font-size: 18px;'><b>User input:</b> <i>{user_input}</i></div><hr>\"))\n",
    "display(Markdown(result.raw))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use local LLM model with LM Studio server mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from crewai import LLM, Agent, Task, Crew\n",
    "#local_llm_name = \"Llama-3.2-3B-Instruct-4bit\"\n",
    "local_llm_name = \"qwen2.5-coder-7b-instruct\"\n",
    "local_llm = LLM(model=\"lm_studio/\"+local_llm_name, base_url=\"http://127.0.0.1:1234/v1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test local model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(\n",
    "    role=\"Data Analyst\",\n",
    "    goal=\"Analyze eCommerce sales data\",\n",
    "    backstory=\"Expert in data analytics with years of experience\",\n",
    "    llm=local_llm\n",
    ")\n",
    "\n",
    "task = Task(\n",
    "    description=\"Analyze sales trends from the last quarter and identify key insights.\",\n",
    "    agent=agent,\n",
    "    expected_output=\"A detailed report summarizing sales trends, key insights, and recommendations.\"\n",
    ")\n",
    "\n",
    "crew = Crew(\n",
    "    agents=[agent],\n",
    "    tasks=[task],\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "result = crew.kickoff() \n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Execute flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_flow = dbtChatFlow(files, local_llm)\n",
    "local_flow.plot()\n",
    "\n",
    "user_input = \"I want to add a new column 'overdue' to the model orders that come from raw_orders source, and have it available in customers. the overdue column is directly available in raw_orders, is not necessairy to calcylate it \"\n",
    "result = local_flow.kickoff(inputs={\"request\": user_input, \"dbt_repo_knowledge_df\": dbt_repo_knowledge_df, \"vectorstore\": loaded_vectorstore, \"embedding_function\":langchain_openai_embeddings})\n",
    "display(Markdown(f\"<div style='font-size: 18px;'><b>User input:</b> <i>{user_input}</i></div><hr>\"))\n",
    "display(Markdown(result.raw))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streamlit interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_dbt_elements = select_dbt_elements_by_extension(repo_elements)\n",
    "repo_dbt_models = select_dbt_models(repo_dbt_elements)\n",
    "dbt_project_df = select_dbt_project_files(repo_dbt_elements)\n",
    "dbt_models_df = generate_dbt_models_df(repo_dbt_models)\n",
    "dbt_project_df, dbt_models_df = move_snapshots_to_models(dbt_project_df, dbt_models_df)\n",
    "dbt_models_df = add_model_code_column(dbt_models_df, is_online = True, online_dbt_repo = repo_path)\n",
    "dbt_models_df = add_config_column(dbt_models_df)\n",
    "dbt_models_df['materialized'] = dbt_models_df['config'].apply(extract_materialized_value)\n",
    "dbt_models_df['is_snapshot'] = dbt_models_df['config'].apply(check_is_snapshot)\n",
    "dbt_models_df['materialized'] = dbt_models_df.apply(lambda row: 'snapshot' if row['is_snapshot'] else row['materialized'] ,1)\n",
    "dbt_models_df['has_jinja_code'] = dbt_models_df['sql_code'].apply(contains_jinja_code)\n",
    "dbt_models_df['model_category'] = dbt_models_df['name'].apply(categorize_model)\n",
    "dbt_models_df['vertical'] = dbt_models_df.apply(lambda row: get_vertical(row['name'], row['model_category']), axis=1)\n",
    "dbt_models_df = assign_yml_rows_to_each_model(dbt_models_df)\n",
    "dbt_models_df['tests'] = dbt_models_df['yml_code'].apply(extract_tests)\n",
    "dbt_models_df['has_tests'] = dbt_models_df['tests'].apply(lambda x: x is not None)\n",
    "dbt_models_df['sql_ids'] = dbt_models_df['sql_code'].apply(extract_ids_from_query)\n",
    "dbt_models_df['has_select_all_in_last_select'] = dbt_models_df['sql_code'].apply(has_select_all_in_last_select)\n",
    "dbt_models_df['has_group_by'] = dbt_models_df['sql_code'].apply(has_group_by)\n",
    "dbt_models_df['primary_key'] = dbt_models_df['tests'].apply(find_primary_key)\n",
    "dbt_models_df['filters'] = dbt_models_df['sql_code'].apply(extract_sql_filters)\n",
    "dbt_models_df['is_filtered'] = dbt_models_df['filters'].apply(lambda x: x is not None)\n",
    "dbt_models_df['macros'] = dbt_models_df['sql_code'].apply(extract_dbt_macros)\n",
    "dbt_models_df['has_macros'] = dbt_models_df['macros'].apply(lambda x: x is not None)\n",
    "dbt_models_enriched_df = enrich_dbt_models(dbt_models_df)\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema import HumanMessage\n",
    "\n",
    "llm = ChatOpenAI(model=DEFAULT_LLM_MODEL, temperature=0.1, openai_api_key=OPENAI_API_KEY, openai_organization = OPENAI_ORGANIZATION)\n",
    "dbt_models_enriched_df['model_description'] = dbt_models_enriched_df.progress_apply(\n",
    "    lambda row: generate_model_description(llm, row),\n",
    "    axis=1\n",
    ")\n",
    "dbt_models_enriched_df['jinja_description'] = dbt_models_enriched_df.progress_apply(\n",
    "    lambda row: generate_jinja_description(llm, row),\n",
    "    axis=1\n",
    ")\n",
    "dbt_project_df = add_project_code_column(dbt_project_df, is_online, online_dbt_repo)\n",
    "dbt_project_df['is_seed'] = dbt_project_df['path'].apply(is_seed)\n",
    "dbt_project_df['is_macro'] = dbt_project_df['path'].apply(is_macro)\n",
    "dbt_project_df['is_test'] = dbt_project_df['path'].apply(is_test)\n",
    "dbt_project_df['packages'] = dbt_project_df.apply(extract_packages, 1)\n",
    "dbt_project_df['description'] = None\n",
    "dbt_project_df['description'] = dbt_project_df.progress_apply(\n",
    "    lambda row: generate_packages_description(llm, row),\n",
    "    axis=1\n",
    ")\n",
    "dbt_project_df['description'] = dbt_project_df.progress_apply(\n",
    "    lambda row: generate_macro_description(llm, row),\n",
    "    axis=1\n",
    ")\n",
    "dbt_project_df['description'] = dbt_project_df.progress_apply(\n",
    "    lambda row: generate_dbt_config_summary(llm, row),\n",
    "    axis=1\n",
    ")\n",
    "dbt_project_df['description'] = dbt_project_df.progress_apply(\n",
    "    lambda row: generate_tests_description(llm, row),\n",
    "    axis=1\n",
    ")\n",
    "_, repo_name = extract_owner_and_repo(repo_path)\n",
    "print(repo_name)\n",
    "\n",
    "dbt_models_enriched_df.to_csv('../data/dbt_models_' + repo_name + '.csv', index=False)\n",
    "dbt_project_df.to_csv('../data/dbt_project_' + repo_name + '.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
