interpretation_task:
  description: >
    Evaluate the user's request:
    {request}
    and based on the evaluation, determine the required action:
    1) adding a field -> return ADD_COLUMN
    2) modifying an existing model -> return MODIFY_MODEL
    3) retrieving and returning specific information. -> return RETRIVE_INFO
    
    Reflect on the request and provide a concise plan for the approach:
    - If the action involves adding a field: Identify where the field is currently available, if provided.
    - Determine how to propagate the field through the necessary models or transformations to integrate it into the target model.
    - Consider the impact on related models and dependencies.
    - If the action involves modifying an existing model: Identify the specific changes required.
    - Assess how these changes affect the structure, relationships, and downstream dependencies of the model.
    - If the action involves retrieving or returning information: Identify the models containing the relevant data.
    - Analyze how these models are related, and determine the queries or transformations needed to extract the requested information.

  expected_output: >
    Return one of the following actions:
    - ADD_COLUMN
    - MODIFY_MODEL
    - RETRIEVE_INFO

    Return the required action from the three proposed.
    REMEMBER: Provide no extra commentary or explanation, just the minimal information required,  

evaluation_task:
  description: >
    Based on the interpretation of an expert dbt and problem interpreter: 
    {interpretation}

    of the original request
    {request}
    
    Include only these topics if relevant:
    - Target models or files
    - Field existence
    - Documentation needs
    - Dependencies and relationships
    - Performance/design considerations
    - Tests
    - dbt project config
    - Code or logic generation
    
    Summarize only the necessary actions, no filler. Think about all the considerations and steps required to handle this request effectively.
    Your evaluation could include the following actions:
    - Identify the target model or models: Analyze the dependency tree to locate the model where the change should occur.
    - Understand its upstream sources (seeds, sources or base models)
    - Understand downstream dependencies of the model.
    - Documentation: Assess if it's going to be necessary to add adjustments to the documentation, and which would be this changes.
    - Unique keys and IDs: Examine the available unique keys and identifiers in the initial, intermediate, and final models.
    - Decide how these can be used to integrate the field or establish relationships between models.
    - Check if the granularity of the model can be altered through the changes added.
    - Evaluate performance and design: Review the data pipeline from start to finish.
    - Decide where the change or addition would be most optimal in terms of performance, data modeling, and maintainability.
    - Project state and impact: Consider the current state of the dbt project and how the changes might affect the broader model chain.
    - Macros and seeds: Check if relevant macros or seed data exist that can help transform or derive the required field or model.
    - Tests: Identify existing tests for the field or model and determine whether new tests need to be added or adjusted to validate the changes.
    - dbt project configuration: Review the general configuration (e.g., variables, environments, conventions) in the dbt_project.yml file to ensure the changes align with project standards and won't disrupt the schema.
    - Code generation: fragments of SQL logic needed, including CTEs or columns.
    - Evaluate whether an intermediate model is necessary or if the logic can be handled within the existing pipeline.
    - Documentation generation: Specify the documentation needed for any fields, models, or logic added or updated as part of this request. Only the things to add or change.

  expected_output: >
    Provide a concise summary of the high-level tasks based on your analysis, with the reflection of each one of them to be prepared to completed in the next steps when context is provided. If you don't have the info to perform the action because it's necessary context of the project, code or the lineage of the models, don't answer it, in next steps context will be provide as input to answer it properly. Mark it as it's necessart the context to responde it, and only do the reflection part using your logic as dbt expert.
    No extra checks or steps that are not on this list, select only the needed actions for the request.
    Return only useful information, no additional filler text or unnecessary explanations, get to the point.
          
    REMEMBER: Provide no extra commentary or explanation, just the minimal information required.

lineage_task:
  description: >
    Based on this user request:
    {request}

    , the dbt expert evaluation of the request:
    {evaluation}
    
    and the retrieved context about the request, 
    {retrieved_context}
    
    (remember the meaning of all the context data:
    CONTEXT INFO AND METADATA MEANING:
    - knowledge_type: Specifies whether the resource is about files within the dbt project configuration, or data models used in the pipeline.
    - name: Name of the file.
    - path: Path within the repository where the file or resource is located, relative to the project root.
    - source: The original dataset, from which the model or resource pulls its data. Only for the first layer of the model.
    - parents: dbt models  that serve as input dependencies for the current model or file.
    - children: dbt models that depend on this resource as an input for their logic or data processing.
    - config: Configuration parameters defined in the file, specifying behaviors, sort keys, materialization...
    - materialized: Indicates how the data model is materialized (e.g., as a table, view, or ephemeral model) in the pipeline.
    - is_snapshot: Boolean flag that identifies whether the file represents a snapshot dbt model.
    - model_category: The logical dbt categorization of the model inside the project, such as base, staging, intermediate, marts...
    - vertical: Business domain or vertical to which the resource belongs, such as e-commerce, finance, supply...
    - has_tests: Boolean flag indicating if there are tests associated with this resource in the model .yaml file or in the tests folder.
    - has_select_all_in_last_select: Specifies if the final SQL query in the file uses a SELECT * statement. So all columns of previous CTEs will be consider as output of the model.
    - has_group_by: Boolean flag indicating if the SQL code includes a GROUP BY clause for aggregating data.
    - is_filtered: Boolean flag that specifies whether the resource applies filters to its data with WHERE, HAVING, JOIN...
    - is_source_model: The model uses the source macro to extract the data from the dbt project sources. It belongs to the first layer of all dbt models.
    - is_seed: Specifies if the resource is a seed file.
    - is_end_model: Boolean flag identifying if this is a terminal model in the data pipeline, representing the final output.
    - is_macro: Boolean flag indicating if the file defines a macro, used for reusable logic across the project. It does not count macros ref to call other models or source to connect to sources.
    - is_test: Indicates whether the file is a test sql definition used for testing purposes of other dbt models.
    - macros: Name of the macros used in the dbt model if any. It does not count macros ref to call other models or source to connect to sources.
    - packages: List of external packages or dependencies required for the resource or project functionality. Only not None in the packages.yml	row.
    )
    
    DETERMINE:
    - The primary dbt model directly affected (e.g., where a field is added, a modification is made, or information is requested).
    - Whether upstream models (UP), downstream models (DOWN), or both (ALL) are necessary to handle this request correctly.
    
    Consider the following cases:
    1. If a new field is added, specify the model where the field will be added and indicate UP for upstream models needed to populate the field.
    2. If an existing field is modified, specify the model where the change occurs and indicate DOWN for downstream models affected by the change.
    3. If information is requested, specify the model containing the requested information and indicate UP, DOWN, or ALL based on the context of the data needed.
    4. If a field or model is removed, specify the model being affected and indicate DOWN for downstream dependencies impacted.
          
  expected_output: >
    Return the following json format: {{'model':model name, 'scope':UP/DOWN/ALL}}
    REMEMBER: Provide no extra commentary or explanation, just the minimal information required.

plan_task:
  description: >
    Based on this evaluation: 
    {evaluation}

    of the user_request:
    {request}

    The analysis of the most impacted model and the part of the lineage that is affected or has related info 
    (UP = Upstream models(parent models)/DOWN = Downstream models(childen models)/ALL = both): 
    {lineage_analysis}

    RETRIEVED CONTEXT: 
    {retrieved_context}

    the retrieved context, remember the meaning of all the context data:
    CONTEXT INFO AND METADATA MEANING:
    - knowledge_type: Specifies whether the resource is about files within the dbt project configuration, or data models used in the pipeline.
    - name: Name of the file.
    - path: Path within the repository where the file or resource is located, relative to the project root.
    - source: The original dataset, from which the model or resource pulls its data. Only for the first layer of the model.
    - parents: dbt models  that serve as input dependencies for the current model or file.
    - children: dbt models that depend on this resource as an input for their logic or data processing.
    - config: Configuration parameters defined in the file, specifying behaviors, sort keys, materialization...
    - materialized: Indicates how the data model is materialized (e.g., as a table, view, or ephemeral model) in the pipeline.
    - is_snapshot: Boolean flag that identifies whether the file represents a snapshot dbt model.
    - model_category: The logical dbt categorization of the model inside the project, such as base, staging, intermediate, marts...
    - vertical: Business domain or vertical to which the resource belongs, such as e-commerce, finance, supply...
    - has_tests: Boolean flag indicating if there are tests associated with this resource in the model .yaml file or in the tests folder.
    - has_select_all_in_last_select: Specifies if the final SQL query in the file uses a SELECT * statement. So all columns of previous CTEs will be consider as output of the model.
    - has_group_by: Boolean flag indicating if the SQL code includes a GROUP BY clause for aggregating data.
    - is_filtered: Boolean flag that specifies whether the resource applies filters to its data with WHERE, HAVING, JOIN...
    - is_source_model: The model uses the source macro to extract the data from the dbt project sources. It belongs to the first layer of all dbt models.
    - is_seed: Specifies if the resource is a seed file.
    - is_end_model: Boolean flag identifying if this is a terminal model in the data pipeline, representing the final output.
    - is_macro: Boolean flag indicating if the file defines a macro, used for reusable logic across the project. It does not count macros ref to call other models or source to connect to sources.
    - is_test: Indicates whether the file is a test sql definition used for testing purposes of other dbt models.
    - macros: Name of the macros used in the dbt model if any. It does not count macros ref to call other models or source to connect to sources.
    - packages: List of external packages or dependencies required for the resource or project functionality. Only not None in the packages.yml	row.

    and the examples about the some of the sources and the seeds (if any):
    {retrieved_csv_sources_context}

    Create a detailed step-by-step plan of the changes required in the existing models or files within the repository.
    To implement the requested change accurately.
    1. Ensure that you only refer to files, models, or fields that are explicitly mentioned in the retrieved information.
    2. Do not invent new models, fields, or dependencies. 
    3. Focus on:
      - Identifying the exact files or models that need modifications, based on the retrieved context.
      - Specifying what changes should be made, such as adding fields, updating logic, or modifying relationships.
      - Highlighting any dependencies between models or files and describing how these should be handled.
      - Extract children or parent models affected.
      - If code fragments are provided in the retrieved context, incorporate them where applicable and explain their role.
      - If no specific code or file is mentioned in the retrieved information, state that no changes should be made to existing files.
      - Ensure the changes align with the dbt project's standards, such as conventions in `dbt_project.yml`, and do not introduce schema-breaking modifications.
    
    4. No extra checks or steps that are not on this list.
    5. Provide precise and actionable recommendations, avoiding any assumptions beyond the retrieved information.
          
  expected_output: >
    Return a summary of all the process, with the reflection, plan and context and the changes tht are neeeded to perfor
          - The original request.
          - The interpretarion of the request.
          - The affected models with the info of the context and the lineage.
          - All the necceasry changes step by step with a clear and short explanaiton of why is needed.
    Return only useful information, no additional filler text or unnecessary explanations, get to the point.
    REMEMBER: Provide no extra commentary or explanation, just the minimal information required.

check_model_task:
  description: > 
    Verify if the request explicitly mentions a model that requires information retrieval or changes.
    Request:
    {request}
    Current dbt lineage of the dbt project:
    {lineage}
  expected_output: > 
    A dict with "status", indicating whether a model was detected ("DETECTED" or "NOT DETECTED") and "identified_model" if applicable. The identified model must be ALWAYS one of the models of the dbt lineage.
    
search_model_task:
  description: > 
    Locate the models most relevant to the user's request by analyzing lineage and matching the context.
    User request:
    {request}
    Main impacted models names:
    {impacted_models}
    Main impacted models details and code:
    {impacted_models_documents}
    Lineage of the impacted models:
    {lineage}
  expected_output: > 
    The name of the identified model and a brief summary of why it matches the request.

generate_info_report_task:
  description: > 
    Retrieve all relevant technical and contextual information about the request of the user, base on the information gather about the dbt model, including dependencies, structure, and other details, base on:
    Original user request:
    {request}
    Info extracted about the impacted model:
    {search_impacted_models_ouput}
    Additional documentation about the model:
    {impacted_models_documents}

  expected_output: > 
    Stick to always answering the user's request as the main subject of the reply.
    Don't create new information that is not available in the one that you have available, if the info does not exist, dont put it in the report.
    Only if needed, generate a detailed summary of the model, including its description, columns, materialization, lineage relationships, and other key details.


search_models_needed_task:
  description: > 
    Analyze the provided dbt lineage and identify all additional models (upstream and downstream) that need changes to maintain consistency when modifications are made to the primary model.
    You must give me the models whose code and context I have to take into account to make the changes in the main model.
    Remember that you cant invent information that you don't have context or evidence about.
    Inputs:
      - Request, Original user request: 
        {request}
      - Identified model, The primary model where changes are requested: 
        {identified_model}
      - Model info, Information about impacted models from previous tasks: 
        {search_impacted_models_ouput}
      - Lineage, The complete lineage DataFrame of the dbt project: 
        {lineage_df}    
  expected_output: > 
    The output will be JUST a JSON file with the extracted information as in the example. Depending on what is specified in target, the file will contain the models whose context is necessary to take into account when designing the changes, it will give only models found upstream that contain information and context necessary to design the solution, no models whose context is not necessary to implement the change.
    For example:
      - The direct parents of the target model being modified, as they provide necessary columns or calculations used in the change.
      - The model from which the new column originates (e.g., the original source system or staging model) and its direct upstream dependencies, up to the raw data source.
      - Models involved in a join within a new CTE to calculate additional metrics or dimensions.
      - Staging models providing data enrichment or formatting for the fields being added or changed in the target model.

    The target model will not be included in either case.
     - json example:
        {
          "upstream_models": [
            {
              "model_name": "<Model name>",
              "requirement": ["<Context needed from this model to ensure that the solution design for the request is 100% correct, be super concise>"]
            },
            {
              "model_name": "<Model name>",
              "requirement": ["<Context needed from this model to ensure that the solution design for the request is 100% correct, be super concise>"]
            }, ...
          ]
        }

search_models_impacted_task:
  description: > 
    Analyze the provided dbt lineage and identify all additional models (upstream and downstream) that need changes to maintain consistency when modifications are made to the primary model.
    You must give me the models I have to modify to make the change in the main model in order not to break anything and that the changes are set correctly, for example:
      - Ensuring added fields propagate downstream to all dependent models.
      - Identifying and addressing missing fields in upstream models required by downstream models.
      - Ensuring removed fields are also deleted from all dependent models.
    Remember that you cant invent information that you don't have context or evidence about.
    Inputs:
    - Request, Original user request: 
      {request}
    - Identified model, The primary model where changes are requested: 
      {identified_model}
    - Model info, Information about impacted models from previous tasks: 
      {search_impacted_models_ouput}
    - Lineage, The complete lineage DataFrame of the dbt project: 
      {lineage_df}    
  expected_output: > 
    The output will be JUST a JSON file with the extracted information as in the example. Depending on what is specified in target, the file will contain the models that need to be adjusted after making changes to the main model, you will return the models that need to be modified so that the changes to the main model are correct and the changes go as far as they need to go. Remember, only the models where adjustments need to be made.
    The changes can be made in upstream and downstream models, for example:
      - All downstream models that depend on the modified model, ensuring they include the new column or adjusted logic (e.g., reporting models or dashboards).
      - Intermediate models between the source and the target model, where the new column or calculation needs to propagate.
      - Aggregation models downstream that use the modified model to calculate final metrics or summaries.
      - Documentation models or validation models that need to be updated to account for the changes in schema or logic.

    The target model will not be included in either case.
     - json example:
        {
          "upstream_models": [
            {
              "model_name": "<Model name>",
              "requirement": ["<List of fields to add/remove/modify>"]
            }, ...
          ],
          "downstream_models": [
            {
              "model_name": "<Model name>",
              "requirement": ["<List of fields to add/remove/modify>"]
            }, ...
          ]
        }

solution_design_task:
  description: > 
    As dbt expert developer, design the needed changes in the model described. to fulfilled the user request: {request}
    The available context to make the changes includes:
      - Info about the model:
        {search_impacted_models_ouput}
      - Code and all the details about the code and the model:
        {identified_model_documents}
      - All the info of their sources and upstream models that you must have into account to design a proper solution:
        {retrieved_context_complete}
      - Lineage of the dbt project:
        {lineage_df}

    When designing the solution:
      - Identify direct parent models of the target model to ensure all necessary transformations or calculations are accurate.
      - Analyze the upstream models providing data to the modified model, ensuring proper propagation of fields or logic (e.g., adding a new column from the raw source up to the target model).
      - Take into account staging models or intermediary transformations that handle formatting, data type changes, or enrichments required for the change.
      - Evaluate joins, CTEs, or subqueries in the modified model to adjust logic or include new fields as needed.
      - Ensure that all dependencies affecting or relying on the model's inputs are consistent with best practices for dbt (e.g., maintaining modularity and avoiding redundancy).
      - Consider any performance implications of the changes, such as query execution time, index usage, or resource efficiency, to optimize performance while maintaining correctness.
      - Maintain clarity and consistency in naming conventions, documentation, and test coverage to align with the dbt project's standards.
  expected_output: >
    Changes that are necessary to correctly adjust the main model, stick only to changes in this model.

solution_design_models_impacted_task:
  description: > 
    As dbt expert developer, design the needed changes in the model described. to fulfilled the user request: {request}
    The available context to make the changes includes:
      - Request, Original user request: 
        {request}
      - Solutions and changes that are going to be made to the main affected model:
        {design_solution_main_model_ouput}
      - Models affected that must be adjusted:
        {search_models_impacted_by_change_ouput}
      - Info of this models that you must have into account to design a proper solution:
        {retrieve_context_for_solution_impacted_models}
      - Lineage of the dbt project:
        {lineage_df}

    When designing the solution:
      - Identify direct parent models of the target model to ensure all necessary transformations or calculations are accurate.
      - Analyze the upstream models providing data to the modified model, ensuring proper propagation of fields or logic (e.g., adding a new column from the raw source up to the target model).
      - Take into account staging models or intermediary transformations that handle formatting, data type changes, or enrichments required for the change.
      - Evaluate joins, CTEs, or subqueries in the modified model to adjust logic or include new fields as needed.
      - Ensure that all dependencies affecting or relying on the model's inputs are consistent with best practices for dbt (e.g., maintaining modularity and avoiding redundancy).
      - Consider any performance implications of the changes, such as query execution time, index usage, or resource efficiency, to optimize performance while maintaining correctness.
      - Maintain clarity and consistency in naming conventions, documentation, and test coverage to align with the dbt project's standards.
  expected_output: >
    Changes that are necessary to correctly adjust the secondary models, stick only to changes in these affected models and avoid the main model, whose change has been already deffined.

concilation_and_testing_task:
  description: > 
    As dbt expert QA developer, your are going to receive the technical solution and changes from a couple of dbt expertes to fulfilled the user request: {request}
    Your mission is to verify that the changes proposed to the main model and the affected models are correct and coherent, adjust them if needed and make a checklist to the tests that are needed to check that everything is correct,
    for example:
      - Test that the changes hadn't added dupplicates in the primary keys.
      - Check that the new columns are not completly null.
      - Check that number of registers and the granularity of the model is the same (if the change is not related with change the granularity of the model).
      - Check that the number of rows are the same.

    The available context to do the checks:
      - Request, Original user request: 
        {request}
      - Solutions and changes that are going to be made to the main affected model:
        {design_solution_main_model_ouput}
      - Solutions and changes that are going to be made to the affected models to maintain consistency and ensure that the changes are well propagated:
        {design_solution_impacted_models_output}
      - Info of this models that you must have into account to design a proper solution:
        {retrieve_context_for_solution_impacted_models}
      - Lineage of the dbt project:
        {lineage_df}

    Remember that you cant invent or hallucinate information that you don't have context or evidence about.
  expected_output: >
    Report the changes to the main models, the changes to the secondary models, and the tests (description and sql code) that are needed to check that the changes proposed are correct.
    Be clear an concise in both, avoid large and complex explanations with empty words, select only the needed tests to check that everything is ok takeing into account the proposed changes.