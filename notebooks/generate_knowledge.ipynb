{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import yaml\n",
    "import sqlparse\n",
    "import os\n",
    "import pandas as pd\n",
    "import requests\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract repo elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_owner_and_repo(github_url):\n",
    "    try:\n",
    "        # Remove the base URL and split the rest\n",
    "        parts = github_url.replace(\"https://github.com/\", \"\").split(\"/\")\n",
    "        # Validate structure\n",
    "        if len(parts) >= 2:\n",
    "            owner = parts[0]\n",
    "            repo = parts[1]\n",
    "            return owner, repo\n",
    "        else:\n",
    "            raise ValueError(\"Invalid GitHub URL structure.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None, None\n",
    "\n",
    "def list_local_repo_structure(repo_path):\n",
    "    paths = []\n",
    "    for root, dirs, files in os.walk(repo_path):\n",
    "        rel_dir = os.path.relpath(root, repo_path)\n",
    "        if rel_dir == '.':\n",
    "            rel_dir = ''\n",
    "        if rel_dir:\n",
    "            paths.append(rel_dir + '/')\n",
    "        for f in files:\n",
    "            file_path = f\"{rel_dir}/{f}\" if rel_dir else f\n",
    "            paths.append(file_path)\n",
    "    return paths\n",
    "\n",
    "def list_online_repo_structure(owner, repo):\n",
    "    url = f\"https://api.github.com/repos/{owner}/{repo}/contents/\"\n",
    "    stack = [(url, '')]\n",
    "    paths = []\n",
    "    while stack:\n",
    "        current_url, current_path = stack.pop()\n",
    "        response = requests.get(current_url)\n",
    "        if response.status_code == 200:\n",
    "            items = response.json()\n",
    "            for item in items:\n",
    "                if item['type'] == 'dir':\n",
    "                    paths.append(current_path + item['name'] + '/')\n",
    "                    stack.append((item['url'], current_path + item['name'] + '/'))\n",
    "                else:\n",
    "                    paths.append(current_path + item['name'])\n",
    "    return paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_online_repo(path):\n",
    "    return path.startswith(\"http://\") or path.startswith(\"https://\")\n",
    "\n",
    "local_dbt_repo = ''\n",
    "online_dbt_repo = 'https://github.com/dbt-labs/jaffle-shop'\n",
    "\n",
    "# Use local repo?\n",
    "if False:\n",
    "    repo_path = local_dbt_repo\n",
    "else:\n",
    "    repo_path = online_dbt_repo\n",
    "\n",
    "is_online = is_online_repo(repo_path)\n",
    "if is_online:\n",
    "    owner, repo = extract_owner_and_repo(online_dbt_repo)\n",
    "    repo_elements = list_online_repo_structure(owner,repo)\n",
    "else:\n",
    "    repo_elements = list_local_repo_structure(local_dbt_repo)\n",
    "\n",
    "print(repo_elements)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select dbt elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbt_extensions = ['.sql', '.yml', '.yaml', '.csv']\n",
    "\n",
    "def select_dbt_elements_by_extension(dbt_extensions, repo_elements):\n",
    "    # Filter elements with relevant extensions\n",
    "    return [element for element in repo_elements if any(element.endswith(ext) for ext in dbt_extensions)]\n",
    "\n",
    "repo_dbt_elements = select_dbt_elements_by_extension(dbt_extensions, repo_elements)\n",
    "print(repo_dbt_elements)\n",
    "\n",
    "def select_dbt_models(dbt_extensions, repo_dbt_elements):\n",
    "    return [\n",
    "        element for element in repo_dbt_elements\n",
    "        if element.startswith('models/') and any(element.endswith(ext) for ext in dbt_extensions)\n",
    "    ]\n",
    "\n",
    "repo_dbt_models = select_dbt_models(dbt_extensions, repo_dbt_elements)\n",
    "print(repo_dbt_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbt_config_elements = ['packages.yml', 'dbt_project.yml']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dbt_models_df(repo_dbt_models):\n",
    "    data = []\n",
    "    for path in repo_dbt_models:\n",
    "        name = os.path.basename(path)\n",
    "        extension = os.path.splitext(name)[1]\n",
    "        data.append({'path': path, 'name': name, 'extension': extension})\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "dbt_models_df = generate_dbt_models_df(repo_dbt_models)\n",
    "display(dbt_models_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    def get_base_url(repo_url):\n",
    "        if repo_url.startswith(\"https://github.com\"):\n",
    "            parts = repo_url.replace(\"https://github.com/\", \"\").split(\"/\")\n",
    "            owner, repo = parts[0], parts[1]\n",
    "            return f\"https://raw.githubusercontent.com/{owner}/{repo}/main\"\n",
    "        else:\n",
    "            raise ValueError(\"URL not valid.\")\n",
    "\n",
    "    def extract_file_content(path, is_online = False, repo_base_url = None):\n",
    "        try:\n",
    "            if is_online:\n",
    "                # Build complete URL\n",
    "                file_url = f\"{repo_base_url}/{path}\" if repo_base_url else path\n",
    "                response = requests.get(file_url)\n",
    "                if response.status_code == 200:\n",
    "                    return response.text\n",
    "                else:\n",
    "                    return f\"Error: {response.status_code} {response.reason}\"\n",
    "            else:\n",
    "                # Read content\n",
    "                with open(path, 'r', encoding='utf-8') as file:\n",
    "                    return file.read()\n",
    "        except Exception as e:\n",
    "            return f\"Error: {e}\"\n",
    "\n",
    "    def add_code_column(df, is_online = False, repo_url = None):\n",
    "        if is_online:\n",
    "            repo_base_url = get_base_url(repo_url)\n",
    "        else:\n",
    "            repo_base_url = ''\n",
    "\n",
    "        df['code'] = df['path'].apply(lambda path: extract_file_content(path, is_online, repo_base_url))\n",
    "        return df\n",
    "\n",
    "    dbt_models_df = add_code_column(dbt_models_df, is_online, online_dbt_repo)\n",
    "    dbt_models_df.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_base_url(repo_url):\n",
    "    if repo_url.startswith(\"https://github.com\"):\n",
    "        parts = repo_url.replace(\"https://github.com/\", \"\").split(\"/\")\n",
    "        owner, repo = parts[0], parts[1]\n",
    "        return f\"https://raw.githubusercontent.com/{owner}/{repo}/main\"\n",
    "    else:\n",
    "        raise ValueError(\"URL not valid.\")\n",
    "\n",
    "def extract_file_content(path, is_online=False, repo_base_url=None):\n",
    "    try:\n",
    "        if is_online:\n",
    "            # Build complete URL\n",
    "            file_url = f\"{repo_base_url}/{path}\" if repo_base_url else path\n",
    "            response = requests.get(file_url)\n",
    "            if response.status_code == 200:\n",
    "                content = response.text\n",
    "            else:\n",
    "                return f\"Error: {response.status_code} {response.reason}\"\n",
    "        else:\n",
    "            # Read content locally\n",
    "            with open(path, 'r', encoding='utf-8') as file:\n",
    "                content = file.read()\n",
    "\n",
    "        # Process content based on file type\n",
    "        if path.endswith(('.yml', '.yaml')):\n",
    "            try:\n",
    "                return yaml.safe_load(content)  # Parse YAML and return as dictionary\n",
    "            except yaml.YAMLError as e:\n",
    "                return f\"Error parsing YAML: {e}\"\n",
    "        elif path.endswith('.sql'):\n",
    "            try:\n",
    "                return sqlparse.format(content, reindent=True, keyword_case='upper')  # Format SQL\n",
    "            except Exception as e:\n",
    "                return f\"Error parsing SQL: {e}\"\n",
    "        else:\n",
    "            return content  # Return plain text for other types\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "def add_code_column(df, is_online=False, repo_url=None):\n",
    "    if is_online:\n",
    "        repo_base_url = get_base_url(repo_url)\n",
    "    else:\n",
    "        repo_base_url = ''\n",
    "\n",
    "    # Extract content for each file and process it based on type\n",
    "    df['code'] = df['path'].apply(lambda path: extract_file_content(path, is_online, repo_base_url))\n",
    "    return df\n",
    "\n",
    "dbt_models_df = add_code_column(dbt_models_df, is_online, online_dbt_repo)\n",
    "dbt_models_df.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbt_models_df.iloc[1].code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add config block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_config_block(sql_code):\n",
    "    pattern = r\"{{\\s*config\\((.*?)\\)\\s*}}\"\n",
    "    match = re.search(pattern, sql_code, re.DOTALL)\n",
    "    return match.group(0) if match else None\n",
    "\n",
    "def add_config_column(df):\n",
    "    df['config'] = df.apply(\n",
    "        lambda row: extract_config_block(row['code']) if row['extension'] == '.sql' else None,\n",
    "        axis=1\n",
    "    )\n",
    "    return df\n",
    "\n",
    "dbt_models_df = add_config_column(dbt_models_df)\n",
    "dbt_models_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = \"\"\"\n",
    "{{\n",
    "    config(\n",
    "        materialized=\"table\"\n",
    "    )\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "dbt_models_df.at[0, 'config'] = test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_materialized_value(config_text):\n",
    "    if config_text:\n",
    "        match = re.search(r\"materialized\\s*=\\s*[\\\"']([^\\\"']+)[\\\"']\", config_text)\n",
    "        return match.group(1) if match else None\n",
    "    return None\n",
    "\n",
    "def check_is_snapshot(config_text):\n",
    "    if config_text:\n",
    "        return 'strategy' in config_text\n",
    "    return False\n",
    "\n",
    "dbt_models_df['materialized'] = dbt_models_df['config'].apply(extract_materialized_value)\n",
    "dbt_models_df['is_snapshot'] = dbt_models_df['config'].apply(check_is_snapshot)\n",
    "dbt_models_df['materialized'] = dbt_models_df.apply(lambda row: 'snapshot' if row['is_snapshot'] else row['materialized'] ,1)\n",
    "\n",
    "def contains_jinja_code(code_text):\n",
    "    if isinstance(code_text, str):\n",
    "        return bool(re.search(r\"{%|{#\", code_text))\n",
    "    return False\n",
    "\n",
    "dbt_models_df['has_jinja_code'] = dbt_models_df['code'].apply(contains_jinja_code)\n",
    "\n",
    "\n",
    "def categorize_model(name):\n",
    "    if name.startswith(\"base\"):\n",
    "        return \"base\"\n",
    "    elif name.startswith(\"stg\"):\n",
    "        return \"stg\"\n",
    "    elif name.startswith(\"int\"):\n",
    "        return \"int\"\n",
    "    elif name.startswith(\"test\"):\n",
    "        return \"test\"\n",
    "    elif name.startswith(\"snap\"):\n",
    "        return \"snap\"\n",
    "    elif name.startswith(\"__sources\"):\n",
    "        return \"sources\"\n",
    "    else:\n",
    "        return \"other\"\n",
    "\n",
    "dbt_models_df['model_category'] = dbt_models_df['name'].apply(categorize_model)\n",
    "\n",
    "def get_vertical(name, model_category):\n",
    "    base_name = re.sub(r'\\.[^.]+$', '', name)\n",
    "    \n",
    "    if model_category == 'sources':\n",
    "        return 'sources'\n",
    "    \n",
    "    known_categories = ['stg', 'int']\n",
    "    if model_category not in known_categories:\n",
    "        # Para model_category = other u otras no conocidas, devolver base_name sin extensión\n",
    "        return base_name\n",
    "    \n",
    "    # Para stg o int, extraer vertical antes de \"__\" o \".\"\n",
    "    pattern = rf'^{re.escape(model_category)}_([a-z0-9_]+?)(?:__|\\.|$)'\n",
    "    match = re.search(pattern, base_name)\n",
    "    return match.group(1) if match else base_name\n",
    "\n",
    "dbt_models_df['vertical'] = dbt_models_df.apply(lambda row: get_vertical(row['name'], row['model_category']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbt_models_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_yml_rows_to_each_model(dbt_models_df):\n",
    "    dbt_models_df['yml_code'] = None\n",
    "\n",
    "    yml_df = dbt_models_df[dbt_models_df['extension'] == '.yml'].copy()\n",
    "    yml_df['delete'] = False\n",
    "\n",
    "    for idx, row in yml_df.iterrows():\n",
    "        base_name = row['name'].rsplit('.', 1)[0]\n",
    "\n",
    "        sql_match = dbt_models_df[(dbt_models_df['name'] == base_name + '.sql')]\n",
    "\n",
    "        if not sql_match.empty:\n",
    "            dbt_models_df.at[sql_match.index[0], 'yml_code'] = row['code']\n",
    "            yml_df.at[idx, 'delete'] = True\n",
    "        else:\n",
    "            yml_df.at[idx, 'yml_code'] = row['code']\n",
    "            yml_df.at[idx, 'code'] = None\n",
    "\n",
    "    yml_df = yml_df[~yml_df['delete']]\n",
    "\n",
    "    dbt_models_df = dbt_models_df[dbt_models_df['extension'] != '.yml']\n",
    "\n",
    "    yml_df = yml_df.drop(columns=['delete'])\n",
    "    dbt_models_df = pd.concat([dbt_models_df, yml_df], ignore_index=True)\n",
    "\n",
    "    return dbt_models_df\n",
    "\n",
    "dbt_models_df = assign_yml_rows_to_each_model(dbt_models_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tests(yml_code):\n",
    "    if not isinstance(yml_code, dict):\n",
    "        return None\n",
    "\n",
    "    tests_dict = {'columns': {}, 'unit_tests': []}\n",
    "\n",
    "    # Extract tests from all models\n",
    "    for model in yml_code.get('models', []):\n",
    "        for column in model.get('columns', []):\n",
    "            column_name = column.get('name')\n",
    "            if column_name:\n",
    "                # Combine 'tests' and 'data_tests' if present\n",
    "                tests = column.get('tests', []) + column.get('data_tests', [])\n",
    "                if tests:\n",
    "                    tests_dict['columns'][column_name] = tests\n",
    "\n",
    "    # Extract unit tests\n",
    "    if 'unit_tests' in yml_code:\n",
    "        unit_test_names = [test.get('name') for test in yml_code['unit_tests'] if test.get('name')]\n",
    "        if unit_test_names:\n",
    "            tests_dict['unit_tests'] = unit_test_names\n",
    "\n",
    "    return tests_dict if tests_dict['columns'] or tests_dict['unit_tests'] else None\n",
    "\n",
    "dbt_models_df['tests'] = dbt_models_df['yml_code'].apply(extract_tests)\n",
    "dbt_models_df['has_tests'] = dbt_models_df['tests'].apply(lambda x: x is not None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ids_from_query(code):\n",
    "    if not isinstance(code, str):\n",
    "        return None\n",
    "    \n",
    "    # Parse the SQL query\n",
    "    parsed = sqlparse.parse(code)\n",
    "    if not parsed:\n",
    "        return None\n",
    "    \n",
    "    # Regular expression to find columns ending in '_id'\n",
    "    id_pattern = re.compile(r'\\b(\\w+_id)\\b')\n",
    "    \n",
    "    cte_ids = set()\n",
    "    output_ids = set()\n",
    "    \n",
    "    for statement in parsed:\n",
    "        # Flatten tokens to handle nested structures\n",
    "        token_list = sqlparse.sql.TokenList(statement.tokens).flatten()\n",
    "        inside_cte = False\n",
    "        \n",
    "        for token in token_list:\n",
    "            # Detect CTE start (with keyword 'WITH')\n",
    "            if token.ttype is sqlparse.tokens.Keyword and token.value.upper() == 'WITH':\n",
    "                inside_cte = True\n",
    "            \n",
    "            # Detect SELECT after a WITH block ends\n",
    "            if inside_cte and token.ttype is sqlparse.tokens.Keyword and token.value.upper() == 'SELECT':\n",
    "                inside_cte = False\n",
    "            \n",
    "            if token.ttype is sqlparse.tokens.Name or token.ttype is None:\n",
    "                match = id_pattern.search(token.value)\n",
    "                if match:\n",
    "                    if inside_cte:\n",
    "                        cte_ids.add(match.group(1))\n",
    "                    else:\n",
    "                        output_ids.add(match.group(1))\n",
    "    ids = {\n",
    "        'cte_ids': list(cte_ids),\n",
    "        'output_ids': list(output_ids)\n",
    "    }\n",
    "    return ids['output_ids'] if ids['output_ids'] != [] else None\n",
    "\n",
    "dbt_models_df['sql_ids'] = dbt_models_df['code'].apply(extract_ids_from_query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_select_all_in_last_select(code):\n",
    "    if not isinstance(code, str):\n",
    "        return False\n",
    "\n",
    "    parsed = sqlparse.parse(code)\n",
    "    if not parsed:\n",
    "        return False\n",
    "\n",
    "    select_statements = [stmt for stmt in parsed if stmt.get_type() == 'SELECT']\n",
    "    if not select_statements:\n",
    "        return False\n",
    "    last_select = select_statements[-1]\n",
    "\n",
    "    for token in last_select.tokens:\n",
    "        if token.ttype is sqlparse.tokens.Wildcard and token.value == '*':\n",
    "            return True\n",
    "\n",
    "    return False\n",
    "\n",
    "dbt_models_df['has_select_all_in_last_select'] = dbt_models_df['code'].apply(has_select_all_in_last_select)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_group_by(code):\n",
    "    if not isinstance(code, str):\n",
    "        return False\n",
    "\n",
    "    parsed = sqlparse.parse(code)\n",
    "    if not parsed:\n",
    "        return False\n",
    "    return 'group by' in code.lower()\n",
    "\n",
    "\n",
    "dbt_models_df['has_group_by'] = dbt_models_df['code'].apply(has_group_by)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_primary_key(tests_dict):\n",
    "    if not isinstance(tests_dict, dict) or 'columns' not in tests_dict:\n",
    "        return None\n",
    "\n",
    "    for column, tests in tests_dict.get('columns', {}).items():\n",
    "        # Check if the column has the required tests for a primary key\n",
    "        if tests == ['not_null', 'unique'] or 'dbt_constraints.primary_key' in tests:\n",
    "            return column\n",
    "    \n",
    "    return None\n",
    "\n",
    "dbt_models_df['primary_key'] = dbt_models_df['tests'].apply(find_primary_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
