{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import yaml\n",
    "import sqlparse\n",
    "import os\n",
    "import pandas as pd\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_columns', 30)\n",
    "#pd.set_option('display.width', None)\n",
    "#pd.set_option('display.max_colwidth', 10) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract repo elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_owner_and_repo(github_url):\n",
    "    try:\n",
    "        # Remove the base URL and split the rest\n",
    "        parts = github_url.replace(\"https://github.com/\", \"\").split(\"/\")\n",
    "        # Validate structure\n",
    "        if len(parts) >= 2:\n",
    "            owner = parts[0]\n",
    "            repo = parts[1]\n",
    "            return owner, repo\n",
    "        else:\n",
    "            raise ValueError(\"Invalid GitHub URL structure.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None, None\n",
    "\n",
    "def list_local_repo_structure(repo_path):\n",
    "    paths = []\n",
    "    for root, dirs, files in os.walk(repo_path):\n",
    "        rel_dir = os.path.relpath(root, repo_path)\n",
    "        if rel_dir == '.':\n",
    "            rel_dir = ''\n",
    "        if rel_dir:\n",
    "            paths.append(rel_dir + '/')\n",
    "        for f in files:\n",
    "            file_path = f\"{rel_dir}/{f}\" if rel_dir else f\n",
    "            paths.append(file_path)\n",
    "    return paths\n",
    "\n",
    "def list_online_repo_structure(owner, repo):\n",
    "    url = f\"https://api.github.com/repos/{owner}/{repo}/contents/\"\n",
    "    stack = [(url, '')]\n",
    "    paths = []\n",
    "    while stack:\n",
    "        current_url, current_path = stack.pop()\n",
    "        response = requests.get(current_url)\n",
    "        if response.status_code == 200:\n",
    "            items = response.json()\n",
    "            for item in items:\n",
    "                if item['type'] == 'dir':\n",
    "                    paths.append(current_path + item['name'] + '/')\n",
    "                    stack.append((item['url'], current_path + item['name'] + '/'))\n",
    "                else:\n",
    "                    paths.append(current_path + item['name'])\n",
    "    return paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_online_repo(path):\n",
    "    return path.startswith(\"http://\") or path.startswith(\"https://\")\n",
    "\n",
    "local_dbt_repo = ''\n",
    "online_dbt_repo = 'https://github.com/dbt-labs/jaffle-shop'\n",
    "\n",
    "# Use local repo?\n",
    "if False:\n",
    "    repo_path = local_dbt_repo\n",
    "else:\n",
    "    repo_path = online_dbt_repo\n",
    "\n",
    "is_online = is_online_repo(repo_path)\n",
    "if is_online:\n",
    "    owner, repo = extract_owner_and_repo(online_dbt_repo)\n",
    "    repo_elements = list_online_repo_structure(owner,repo)\n",
    "else:\n",
    "    repo_elements = list_local_repo_structure(local_dbt_repo)\n",
    "\n",
    "print(repo_elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_dbt_elements_by_extension(repo_elements):\n",
    "    dbt_extensions = ['.sql', '.yml', '.yaml', '.csv']\n",
    "    # Filter elements with relevant extensions\n",
    "    return [element for element in repo_elements if any(element.endswith(ext) for ext in dbt_extensions)]\n",
    "\n",
    "repo_dbt_elements = select_dbt_elements_by_extension(repo_elements)\n",
    "print(repo_dbt_elements)\n",
    "\n",
    "def select_dbt_models(repo_dbt_elements):\n",
    "    dbt_extensions = ['.sql', '.yml', '.yaml', '.csv']\n",
    "    return [\n",
    "        element for element in repo_dbt_elements\n",
    "        if element.startswith('models/') and any(element.endswith(ext) for ext in dbt_extensions)\n",
    "    ]\n",
    "\n",
    "repo_dbt_models = select_dbt_models(repo_dbt_elements)\n",
    "print(repo_dbt_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_dbt_project_files(repo_elements):\n",
    "    valid_extensions = [\".sql\", \".yml\", \".csv\"]\n",
    "    exclude_folders = [\"models/\"]\n",
    "\n",
    "    # Filter repo elements\n",
    "    filtered_elements = [\n",
    "        element for element in repo_elements\n",
    "        if any(element.endswith(ext) for ext in valid_extensions)\n",
    "        and not any(folder in element for folder in exclude_folders)\n",
    "        and not element.startswith(\".\")\n",
    "    ]\n",
    "\n",
    "    # Create DataFrame\n",
    "    repo_df = pd.DataFrame(filtered_elements, columns=[\"path\"])\n",
    "\n",
    "    # Add columns for useful details\n",
    "    repo_df[\"file_name\"] = repo_df[\"path\"].apply(lambda x: x.split(\"/\")[-1])\n",
    "    repo_df[\"extension\"] = repo_df[\"path\"].apply(lambda x: \".\" + x.split(\".\")[-1])\n",
    "    return repo_df\n",
    "\n",
    "dbt_project_df = select_dbt_project_files(repo_dbt_elements)\n",
    "display(dbt_project_df.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dbt_models_df(repo_dbt_models):\n",
    "    data = []\n",
    "    for path in repo_dbt_models:\n",
    "        name = os.path.basename(path)\n",
    "        extension = os.path.splitext(name)[1]\n",
    "        data.append({'path': path, 'name': name, 'extension': extension})\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "dbt_models_df = generate_dbt_models_df(repo_dbt_models)\n",
    "display(dbt_models_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Snapshots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_snapshots_to_models(dbt_project_df, dbt_models_df):\n",
    "    snapshots_filter = dbt_project_df['path'].str.contains(r'(snapshots/|^snap)', case=False, regex=True)\n",
    "\n",
    "    snapshots_rows = dbt_project_df[snapshots_filter]\n",
    "    dbt_project_df = dbt_project_df[~snapshots_filter]\n",
    "\n",
    "    dbt_models_df = pd.concat([dbt_models_df, snapshots_rows], ignore_index=True)\n",
    "\n",
    "    return dbt_project_df, dbt_models_df\n",
    "\n",
    "dbt_project_df, dbt_models_df = move_snapshots_to_models(dbt_project_df, dbt_models_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dbt models knowledge db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add sql code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    def get_base_url(repo_url):\n",
    "        if repo_url.startswith(\"https://github.com\"):\n",
    "            parts = repo_url.replace(\"https://github.com/\", \"\").split(\"/\")\n",
    "            owner, repo = parts[0], parts[1]\n",
    "            return f\"https://raw.githubusercontent.com/{owner}/{repo}/main\"\n",
    "        else:\n",
    "            raise ValueError(\"URL not valid.\")\n",
    "\n",
    "    def extract_file_content(path, is_online = False, repo_base_url = None):\n",
    "        try:\n",
    "            if is_online:\n",
    "                # Build complete URL\n",
    "                file_url = f\"{repo_base_url}/{path}\" if repo_base_url else path\n",
    "                response = requests.get(file_url)\n",
    "                if response.status_code == 200:\n",
    "                    return response.text\n",
    "                else:\n",
    "                    return f\"Error: {response.status_code} {response.reason}\"\n",
    "            else:\n",
    "                # Read content\n",
    "                with open(path, 'r', encoding='utf-8') as file:\n",
    "                    return file.read()\n",
    "        except Exception as e:\n",
    "            return f\"Error: {e}\"\n",
    "\n",
    "    def add_code_column(df, is_online = False, repo_url = None):\n",
    "        if is_online:\n",
    "            repo_base_url = get_base_url(repo_url)\n",
    "        else:\n",
    "            repo_base_url = ''\n",
    "\n",
    "        df['sql_code'] = df['path'].apply(lambda path: extract_file_content(path, is_online, repo_base_url))\n",
    "        return df\n",
    "\n",
    "    dbt_models_df = add_code_column(dbt_models_df, is_online, online_dbt_repo)\n",
    "    dbt_models_df.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_base_url(repo_url):\n",
    "    if repo_url.startswith(\"https://github.com\"):\n",
    "        parts = repo_url.replace(\"https://github.com/\", \"\").split(\"/\")\n",
    "        owner, repo = parts[0], parts[1]\n",
    "        return f\"https://raw.githubusercontent.com/{owner}/{repo}/main\"\n",
    "    else:\n",
    "        raise ValueError(\"URL not valid.\")\n",
    "\n",
    "def extract_model_file_content(path, is_online=False, repo_base_url=None):\n",
    "    try:\n",
    "        if is_online:\n",
    "            # Build complete URL\n",
    "            file_url = f\"{repo_base_url}/{path}\" if repo_base_url else path\n",
    "            response = requests.get(file_url)\n",
    "            if response.status_code == 200:\n",
    "                content = response.text\n",
    "            else:\n",
    "                return f\"Error: {response.status_code} {response.reason}\"\n",
    "        else:\n",
    "            # Read content locally\n",
    "            with open(path, 'r', encoding='utf-8') as file:\n",
    "                content = file.read()\n",
    "\n",
    "        # Process content based on file type\n",
    "        if path.endswith(('.yml', '.yaml')):\n",
    "            try:\n",
    "                return yaml.safe_load(content)  # Parse YAML and return as dictionary\n",
    "            except yaml.YAMLError as e:\n",
    "                return f\"Error parsing YAML: {e}\"\n",
    "        elif path.endswith('.sql'):\n",
    "            try:\n",
    "                return sqlparse.format(content, reindent=True, keyword_case='upper')  # Format SQL\n",
    "            except Exception as e:\n",
    "                return f\"Error parsing SQL: {e}\"\n",
    "        else:\n",
    "            return content  # Return plain text for other types\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "def add_model_code_column(df, is_online=False, repo_url=None):\n",
    "    if is_online:\n",
    "        repo_base_url = get_base_url(repo_url)\n",
    "    else:\n",
    "        repo_base_url = ''\n",
    "\n",
    "    # Extract content for each file and process it based on type\n",
    "    df['sql_code'] = df['path'].apply(lambda path: extract_model_file_content(path, is_online, repo_base_url))\n",
    "    return df\n",
    "\n",
    "dbt_models_df = add_model_code_column(dbt_models_df, is_online, online_dbt_repo)\n",
    "dbt_models_df.head(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add config block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_config_block(sql_code):\n",
    "    pattern = r\"{{\\s*config\\((.*?)\\)\\s*}}\"\n",
    "    match = re.search(pattern, sql_code, re.DOTALL)\n",
    "    return match.group(0) if match else None\n",
    "\n",
    "def add_config_column(df):\n",
    "    df['config'] = df.apply(\n",
    "        lambda row: extract_config_block(row['sql_code']) if row['extension'] == '.sql' else None,\n",
    "        axis=1\n",
    "    )\n",
    "    return df\n",
    "\n",
    "dbt_models_df = add_config_column(dbt_models_df)\n",
    "dbt_models_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = \"\"\"\n",
    "{{\n",
    "    config(\n",
    "        materialized=\"table\"\n",
    "    )\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "dbt_models_df.at[0, 'config'] = test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add model metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_materialized_value(config_text):\n",
    "    if config_text:\n",
    "        match = re.search(r\"materialized\\s*=\\s*[\\\"']([^\\\"']+)[\\\"']\", config_text)\n",
    "        return match.group(1) if match else None\n",
    "    return None\n",
    "\n",
    "def check_is_snapshot(config_text):\n",
    "    if config_text:\n",
    "        return 'strategy' in config_text\n",
    "    return False\n",
    "\n",
    "dbt_models_df['materialized'] = dbt_models_df['config'].apply(extract_materialized_value)\n",
    "dbt_models_df['is_snapshot'] = dbt_models_df['config'].apply(check_is_snapshot)\n",
    "dbt_models_df['materialized'] = dbt_models_df.apply(lambda row: 'snapshot' if row['is_snapshot'] else row['materialized'] ,1)\n",
    "\n",
    "def contains_jinja_code(code_text):\n",
    "    if isinstance(code_text, str):\n",
    "        return bool(re.search(r\"{%|{#\", code_text))\n",
    "    return False\n",
    "\n",
    "dbt_models_df['has_jinja_code'] = dbt_models_df['sql_code'].apply(contains_jinja_code)\n",
    "\n",
    "\n",
    "def categorize_model(name):\n",
    "    if name.startswith(\"base\"):\n",
    "        return \"base\"\n",
    "    elif name.startswith(\"stg\"):\n",
    "        return \"stg\"\n",
    "    elif name.startswith(\"int\"):\n",
    "        return \"int\"\n",
    "    elif name.startswith(\"test\"):\n",
    "        return \"test\"\n",
    "    elif name.startswith(\"snap\"):\n",
    "        return \"snap\"\n",
    "    elif name.startswith(\"__sources\"):\n",
    "        return \"sources\"\n",
    "    else:\n",
    "        return \"other\"\n",
    "\n",
    "dbt_models_df['model_category'] = dbt_models_df['name'].apply(categorize_model)\n",
    "\n",
    "def get_vertical(name, model_category):\n",
    "    base_name = re.sub(r'\\.[^.]+$', '', name)\n",
    "    \n",
    "    if model_category == 'sources':\n",
    "        return 'sources'\n",
    "    \n",
    "    known_categories = ['stg', 'int']\n",
    "    if model_category not in known_categories:\n",
    "        # Para model_category = other u otras no conocidas, devolver base_name sin extensi√≥n\n",
    "        return base_name\n",
    "    \n",
    "    # Para stg o int, extraer vertical antes de \"__\" o \".\"\n",
    "    pattern = rf'^{re.escape(model_category)}_([a-z0-9_]+?)(?:__|\\.|$)'\n",
    "    match = re.search(pattern, base_name)\n",
    "    return match.group(1) if match else base_name\n",
    "\n",
    "dbt_models_df['vertical'] = dbt_models_df.apply(lambda row: get_vertical(row['name'], row['model_category']), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Zip the dataframe by models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_yml_rows_to_each_model(dbt_models_df):\n",
    "    dbt_models_df['yml_code'] = None\n",
    "\n",
    "    yml_df = dbt_models_df[dbt_models_df['extension'] == '.yml'].copy()\n",
    "    yml_df['delete'] = False\n",
    "\n",
    "    for idx, row in yml_df.iterrows():\n",
    "        base_name = row['name'].rsplit('.', 1)[0]\n",
    "\n",
    "        sql_match = dbt_models_df[(dbt_models_df['name'] == base_name + '.sql')]\n",
    "\n",
    "        if not sql_match.empty:\n",
    "            dbt_models_df.at[sql_match.index[0], 'yml_code'] = row['sql_code']\n",
    "            yml_df.at[idx, 'delete'] = True\n",
    "        else:\n",
    "            yml_df.at[idx, 'yml_code'] = row['sql_code']\n",
    "            yml_df.at[idx, 'sql_code'] = None\n",
    "\n",
    "    yml_df = yml_df[~yml_df['delete']]\n",
    "\n",
    "    dbt_models_df = dbt_models_df[dbt_models_df['extension'] != '.yml']\n",
    "\n",
    "    yml_df = yml_df.drop(columns=['delete'])\n",
    "    dbt_models_df = pd.concat([dbt_models_df, yml_df], ignore_index=True)\n",
    "\n",
    "    return dbt_models_df\n",
    "\n",
    "dbt_models_df = assign_yml_rows_to_each_model(dbt_models_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract sql code info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tests(yml_code):\n",
    "    if not isinstance(yml_code, dict):\n",
    "        return None\n",
    "\n",
    "    tests_dict = {'columns': {}, 'unit_tests': []}\n",
    "\n",
    "    # Extract tests from all models\n",
    "    for model in yml_code.get('models', []):\n",
    "        for column in model.get('columns', []):\n",
    "            column_name = column.get('name')\n",
    "            if column_name:\n",
    "                # Combine 'tests' and 'data_tests' if present\n",
    "                tests = column.get('tests', []) + column.get('data_tests', [])\n",
    "                if tests:\n",
    "                    tests_dict['columns'][column_name] = tests\n",
    "\n",
    "    # Extract unit tests\n",
    "    if 'unit_tests' in yml_code:\n",
    "        unit_test_names = [test.get('name') for test in yml_code['unit_tests'] if test.get('name')]\n",
    "        if unit_test_names:\n",
    "            tests_dict['unit_tests'] = unit_test_names\n",
    "\n",
    "    return tests_dict if tests_dict['columns'] or tests_dict['unit_tests'] else None\n",
    "\n",
    "dbt_models_df['tests'] = dbt_models_df['yml_code'].apply(extract_tests)\n",
    "dbt_models_df['has_tests'] = dbt_models_df['tests'].apply(lambda x: x is not None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ids_from_query(code):\n",
    "    if not isinstance(code, str):\n",
    "        return None\n",
    "    \n",
    "    # Parse the SQL query\n",
    "    parsed = sqlparse.parse(code)\n",
    "    if not parsed:\n",
    "        return None\n",
    "    \n",
    "    # Regular expression to find columns ending in '_id'\n",
    "    id_pattern = re.compile(r'\\b(\\w+_id)\\b')\n",
    "    \n",
    "    cte_ids = set()\n",
    "    output_ids = set()\n",
    "    \n",
    "    for statement in parsed:\n",
    "        # Flatten tokens to handle nested structures\n",
    "        token_list = sqlparse.sql.TokenList(statement.tokens).flatten()\n",
    "        inside_cte = False\n",
    "        \n",
    "        for token in token_list:\n",
    "            # Detect CTE start (with keyword 'WITH')\n",
    "            if token.ttype is sqlparse.tokens.Keyword and token.value.upper() == 'WITH':\n",
    "                inside_cte = True\n",
    "            \n",
    "            # Detect SELECT after a WITH block ends\n",
    "            if inside_cte and token.ttype is sqlparse.tokens.Keyword and token.value.upper() == 'SELECT':\n",
    "                inside_cte = False\n",
    "            \n",
    "            if token.ttype is sqlparse.tokens.Name or token.ttype is None:\n",
    "                match = id_pattern.search(token.value)\n",
    "                if match:\n",
    "                    if inside_cte:\n",
    "                        cte_ids.add(match.group(1))\n",
    "                    else:\n",
    "                        output_ids.add(match.group(1))\n",
    "    ids = {\n",
    "        'cte_ids': list(cte_ids),\n",
    "        'output_ids': list(output_ids)\n",
    "    }\n",
    "    return ids['output_ids'] if ids['output_ids'] != [] else None\n",
    "\n",
    "dbt_models_df['sql_ids'] = dbt_models_df['sql_code'].apply(extract_ids_from_query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_select_all_in_last_select(code):\n",
    "    if not isinstance(code, str):\n",
    "        return False\n",
    "\n",
    "    parsed = sqlparse.parse(code)\n",
    "    if not parsed:\n",
    "        return False\n",
    "\n",
    "    select_statements = [stmt for stmt in parsed if stmt.get_type() == 'SELECT']\n",
    "    if not select_statements:\n",
    "        return False\n",
    "    last_select = select_statements[-1]\n",
    "\n",
    "    for token in last_select.tokens:\n",
    "        if token.ttype is sqlparse.tokens.Wildcard and token.value == '*':\n",
    "            return True\n",
    "\n",
    "    return False\n",
    "\n",
    "dbt_models_df['has_select_all_in_last_select'] = dbt_models_df['sql_code'].apply(has_select_all_in_last_select)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_group_by(code):\n",
    "    if not isinstance(code, str):\n",
    "        return False\n",
    "\n",
    "    parsed = sqlparse.parse(code)\n",
    "    if not parsed:\n",
    "        return False\n",
    "    return 'group by' in code.lower()\n",
    "\n",
    "\n",
    "dbt_models_df['has_group_by'] = dbt_models_df['sql_code'].apply(has_group_by)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_primary_key(tests_dict):\n",
    "    if not isinstance(tests_dict, dict) or 'columns' not in tests_dict:\n",
    "        return None\n",
    "\n",
    "    for column, tests in tests_dict.get('columns', {}).items():\n",
    "        # Check if the column has the required tests for a primary key\n",
    "        if tests == ['not_null', 'unique'] or 'dbt_constraints.primary_key' in tests:\n",
    "            return column\n",
    "    \n",
    "    return None\n",
    "\n",
    "dbt_models_df['primary_key'] = dbt_models_df['tests'].apply(find_primary_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sql_filters(sql_query):\n",
    "    if not isinstance(sql_query, str) or not sql_query.strip():\n",
    "        return None\n",
    "\n",
    "    sql_query_clean = ' '.join(sql_query.split()).lower()\n",
    "\n",
    "    filters_patterns = [\n",
    "        (r'\\bwhere\\b\\s+(.*?)(?=\\bgroup\\b|\\border\\b|\\blimit\\b|\\bhaving\\b|;|$)', 'where'),\n",
    "        (r'\\bon\\b\\s+(.*?)(?=\\bleft\\b|\\bright\\b|\\binner\\b|\\bouter\\b|\\bjoin\\b|\\bselect\\b|\\bwhere\\b|\\bgroup\\b|\\border\\b|\\blimit\\b|;|$)', 'join'),\n",
    "        (r'\\bhaving\\b\\s+(.*?)(?=\\bgroup\\b|\\border\\b|\\blimit\\b|;|$)', 'having')\n",
    "    ]\n",
    "\n",
    "    filters = []\n",
    "    joins = []\n",
    "\n",
    "    for pattern, clause_type in filters_patterns:\n",
    "        matches = re.findall(pattern, sql_query_clean, re.DOTALL)\n",
    "        for match in matches:\n",
    "            sub_conditions = re.split(r'\\band\\b|\\bor\\b', match)\n",
    "            for condition in sub_conditions:\n",
    "                cleaned = condition.strip().strip('()')\n",
    "                if cleaned:\n",
    "                    if clause_type == 'join':\n",
    "                        joins.append(cleaned)\n",
    "                    else:\n",
    "                        filters.append(cleaned)\n",
    "    all_filters = filters + joins\n",
    "    return all_filters if all_filters != [] else None\n",
    "\n",
    "dbt_models_df['filters'] = dbt_models_df['sql_code'].apply(extract_sql_filters)\n",
    "dbt_models_df['is_filtered'] = dbt_models_df['filters'].apply(lambda x: x is not None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_dbt_macros(sql_query):\n",
    "\n",
    "    if not isinstance(sql_query, str) or not sql_query.strip():\n",
    "        return None\n",
    "    \n",
    "    macro_pattern = r\"\\{\\{\\s*([\\w\\.]+)\\s*\\(.*?\\)\\s*\\}\\}\"\n",
    "    matches = re.findall(macro_pattern, sql_query)\n",
    "    filtered_macros = sorted(set(m for m in matches if m not in ('ref', 'source')))\n",
    "    \n",
    "    return filtered_macros if filtered_macros != [] else None\n",
    "\n",
    "dbt_models_df['macros'] = dbt_models_df['sql_code'].apply(extract_dbt_macros)\n",
    "dbt_models_df['has_macros'] = dbt_models_df['macros'].apply(lambda x: x is not None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate models structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_source_details(code, source_pattern):\n",
    "    if not isinstance(code, str):\n",
    "        return False, None\n",
    "    sources = re.findall(source_pattern, code)\n",
    "    if sources:\n",
    "        return True, [f\"{source[0]}.{source[1]}\" for source in sources]\n",
    "    return False, None\n",
    "\n",
    "def enrich_dbt_models(dbt_models_df):\n",
    "    # Helper regex patterns\n",
    "    source_pattern = r\"\\{\\{\\s*source\\(['\\\"](.*?)['\\\"],\\s*['\\\"](.*?)['\\\"]\\)\\s*\\}\\}\"\n",
    "    ref_pattern = r\"\\{\\{\\s*ref\\(['\\\"](.*?)['\\\"]\\)\\s*\\}\\}\"\n",
    "    \n",
    "    # Add 'parent_models' - extract all models referenced using 'ref'\n",
    "    dbt_models_df['parent_models'] = dbt_models_df['sql_code'].apply(\n",
    "        lambda code: re.findall(ref_pattern, code) if isinstance(code, str) else []\n",
    "    )\n",
    "    \n",
    "    dbt_models_df[['is_source_model', 'source']] = dbt_models_df['sql_code'].apply(\n",
    "        lambda code: pd.Series(extract_source_details(code, source_pattern))\n",
    "    )\n",
    "    \n",
    "    # Build a dictionary to track children relationships\n",
    "    model_children = {}\n",
    "    for idx, row in dbt_models_df.iterrows():\n",
    "        for parent in row['parent_models']:\n",
    "            model_children.setdefault(parent, []).append(row['name'].replace('.sql', ''))\n",
    "\n",
    "    # Add 'children_models' - list all models that depend on this model\n",
    "    dbt_models_df['children_models'] = dbt_models_df['name'].apply(\n",
    "        lambda name: model_children.get(name.replace('.sql', ''), [])\n",
    "    )\n",
    "    \n",
    "    # Add 'is_end_model' - True if there are no children\n",
    "    dbt_models_df['is_end_model'] = dbt_models_df['children_models'].apply(lambda children: len(children) == 0)\n",
    "    \n",
    "    return dbt_models_df\n",
    "\n",
    "dbt_models_enriched_df = enrich_dbt_models(dbt_models_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(dbt_models_enriched_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add descriptions using LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_repo_root_path():\n",
    "    import os\n",
    "    import sys\n",
    "    repo_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "    if repo_root not in sys.path:\n",
    "        sys.path.append(repo_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_repo_root_path()\n",
    "import openai_setup\n",
    "\n",
    "OPENAI_API_KEY = openai_setup.conf['key']\n",
    "OPENAI_PROJECT = openai_setup.conf['project']\n",
    "OPENAI_ORGANIZATION = openai_setup.conf['organization']\n",
    "DEFAULT_LLM_MODEL = \"gpt-4o-mini\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI  # Importa el modelo de OpenAI\n",
    "from langchain.schema import HumanMessage  # Para interactuar con mensajes\n",
    "\n",
    "llm = ChatOpenAI(model=DEFAULT_LLM_MODEL, temperature=0.1, openai_api_key=OPENAI_API_KEY, openai_organization = OPENAI_ORGANIZATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_query_description(llm, query, documentation = None):\n",
    "    # Context and prompt\n",
    "    prompt = f\"\"\"\n",
    "        You are an expert data analyst, dbt analytics engineer and technical writer. \n",
    "        Your task is to generate a concise, clear, and standardized description of the following SQL query of a dbt model.\n",
    "\n",
    "        SQL Query of the dbt model:\n",
    "        {query}\n",
    "\n",
    "        Additional Documentation in the model yaml file:\n",
    "        {documentation}\n",
    "\n",
    "        Guidelines:\n",
    "        1. Describe the **main purpose** of the query in 2 to 3 sentences.\n",
    "        2. Include the following details explicitly:\n",
    "        - Tables or data sources referenced.\n",
    "        - Filters, conditions, and joins applied.\n",
    "        - Any aggregations (e.g., SUM, COUNT) or calculations performed.\n",
    "        3. Avoid technical jargon and vague expressions.\n",
    "        4. Limit the description to around **50 words** maximum.\n",
    "        5. Maintain a similar level of detail and length for all responses to ensure consistency.\n",
    "        6. Don't use \"This query\" or \"This model\", all the info must usefull and coherent.\n",
    "        \n",
    "        Format Example:\n",
    "        \"Retrieves all customer records from the 'customers' table where the country is 'US'. It joins the 'orders' table on 'customer_id' to calculate the total order amount per customer using SUM(). The result is grouped by 'customer_id'.\"\n",
    "    \"\"\"\n",
    "\n",
    "    # Interact\n",
    "    response = llm([HumanMessage(content=prompt)])\n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_query = dbt_models_enriched_df.iloc[1].sql_code\n",
    "print(example_query)\n",
    "example_doc = dbt_models_enriched_df.iloc[1].yml_code\n",
    "print(example_doc)\n",
    "\n",
    "generate_query_description(llm, example_query, example_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "def generate_model_description(row):\n",
    "    if pd.notna(row['sql_code']) or pd.notna(row['yml_code']):\n",
    "        sql_code = row['sql_code'] if pd.notna(row['sql_code']) else \"\"\n",
    "        yml_code = row['yml_code'] if pd.notna(row['yml_code']) else \"\"\n",
    "        return generate_query_description(llm, sql_code, yml_code)\n",
    "    return None\n",
    "\n",
    "dbt_models_enriched_df['model_description'] = dbt_models_enriched_df.progress_apply(generate_model_description, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_jinja_code_description(llm, query, documentation = None):\n",
    "    # Context and prompt\n",
    "    prompt = f\"\"\"\n",
    "        You are an expert data analyst, dbt analytics engineer, technical writer, and Jinja programmer. \n",
    "        Your task is to generate a concise, clear, and standardized description of the Jinja code within the dbt model.\n",
    "\n",
    "        SQL Query of the dbt model with the Jinja code:\n",
    "        {query}\n",
    "\n",
    "        Additional Documentation in the model yaml file:\n",
    "        {documentation}\n",
    "\n",
    "        Guidelines:\n",
    "        1. Focus only on what the Jinja code does, ignoring the logic or dependencies related to refs or source functions.\n",
    "        2. Clearly explain the **main purpose** of the Jinja code in plain language.\n",
    "        3. Avoid technical jargon and vague expressions.\n",
    "        4. Limit the description to around **50 words** maximum.\n",
    "        5. Ensure all responses are coherent, useful, and follow a consistent format.\n",
    "        6. Avoid using phrases like \"This ...\" or \"The code ...\". Focus on describing the purpose directly.\n",
    "        7. If has multiple sections, describe each section separetly.\n",
    "\n",
    "        Examples Format:\n",
    "        - Calculates the rolling average of sales over the last 30 days for each product.\n",
    "        - Formats the date column to a standard YYYY-MM-DD format.\n",
    "        - Dynamically generates filter conditions based on user inputs.\n",
    "\n",
    "        Provide the description of the Jinja code:\n",
    "    \"\"\"\n",
    "\n",
    "    # Interact\n",
    "    response = llm([HumanMessage(content=prompt)])\n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_jinja_description(row):\n",
    "    if row['has_jinja_code']:\n",
    "        sql_code = row['sql_code'] if pd.notna(row['sql_code']) else \"\"\n",
    "        yml_code = row['yml_code'] if pd.notna(row['yml_code']) else \"\"\n",
    "        return generate_jinja_code_description(llm, sql_code, yml_code)\n",
    "    return None\n",
    "\n",
    "dbt_models_enriched_df['jinja_description'] = dbt_models_enriched_df.progress_apply(generate_jinja_description, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbt_models_enriched_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dbt project knowledge db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select and extract project elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import StringIO\n",
    "\n",
    "def extract_project_file_content(file_path, is_online=False, repo_base_url=None):\n",
    "    try:\n",
    "        # Read content from local or online\n",
    "        if is_online:\n",
    "            # Build complete URL\n",
    "            file_url = f\"{repo_base_url}/{file_path}\" if repo_base_url else file_path\n",
    "            response = requests.get(file_url)\n",
    "            if response.status_code == 200:\n",
    "                content = response.text\n",
    "            else:\n",
    "                return f\"Error: {response.status_code} {response.reason}\"\n",
    "        else:\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                content = f.read()\n",
    "\n",
    "        # Process content based on file type\n",
    "        if file_path.endswith(('.yml', '.yaml')):\n",
    "            try:\n",
    "                yaml_content = yaml.safe_load(content)\n",
    "                return yaml.dump(yaml_content, sort_keys=False, default_flow_style=False)\n",
    "            except yaml.YAMLError as e:\n",
    "                return f\"Error parsing YAML: {e}\"\n",
    "        elif file_path.endswith('.sql'):\n",
    "            try:\n",
    "                return sqlparse.format(content, reindent=True, keyword_case=\"lower\")\n",
    "            except Exception as e:\n",
    "                return f\"Error parsing SQL: {e}\"\n",
    "        elif file_path.endswith('.csv'):\n",
    "            try:\n",
    "                from io import StringIO\n",
    "                df = pd.read_csv(StringIO(content))\n",
    "                df_cleaned = df.dropna().reset_index(drop=True)  # Clean missing values and reset index\n",
    "                df_cleaned.columns = [col.strip().lower() for col in df_cleaned.columns]  # Standardize column names\n",
    "                return df_cleaned.head(3).to_string(index=False)\n",
    "            except Exception as e:\n",
    "                return f\"Error reading CSV: {e}\"\n",
    "        else:\n",
    "            return content  # Return plain text for unsupported types\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "    \n",
    "def add_project_code_column(df, is_online=False, repo_url=None):\n",
    "    if is_online:\n",
    "        repo_base_url = get_base_url(repo_url)\n",
    "    else:\n",
    "        repo_base_url = ''\n",
    "\n",
    "    # Extract content for each file and process it based on type\n",
    "    df['code'] = df['path'].apply(lambda path: extract_project_file_content(path, is_online, repo_base_url))\n",
    "    return df\n",
    "\n",
    "dbt_project_df = add_project_code_column(dbt_project_df, is_online, online_dbt_repo)\n",
    "dbt_project_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_seed(file_path):\n",
    "    file_name = os.path.basename(file_path)\n",
    "    return \"seeds/\" in file_path or file_name.lower().startswith(\"seed_\")\n",
    "\n",
    "def is_macro(file_path):\n",
    "    return file_path.endswith(\".sql\") and \"macros/\" in file_path\n",
    "\n",
    "def is_test(file_path):\n",
    "    file_name = os.path.basename(file_path)\n",
    "    return \"tests/\" in file_path or file_name.lower().startswith(\"test_\")\n",
    "\n",
    "dbt_project_df['is_seed'] = dbt_project_df['path'].apply(is_seed)\n",
    "dbt_project_df['is_macro'] = dbt_project_df['path'].apply(is_macro)\n",
    "dbt_project_df['is_test'] = dbt_project_df['path'].apply(is_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_packages(row):\n",
    "    if 'file_name' in row and row['file_name'] == 'packages.yml':\n",
    "        try:\n",
    "            packages_content = yaml.safe_load(row['code'])\n",
    "            packages = []\n",
    "            for pkg in packages_content.get(\"packages\", []):\n",
    "                if \"package\" in pkg:\n",
    "                    packages.append(f\"{pkg['package']}=={pkg.get('version', 'unknown_version')}\")\n",
    "                elif \"git\" in pkg:\n",
    "                    revision = pkg.get(\"revision\", \"unknown_revision\")\n",
    "                    packages.append(f\"{pkg['git']}@{revision}\")\n",
    "            return packages\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing packages content: {e}\")\n",
    "            return []\n",
    "    return None\n",
    "\n",
    "dbt_project_df['packages'] = dbt_project_df.apply(extract_packages, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbt_project_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add descriptions using llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_macro_code_description(llm, query):\n",
    "    # Context and prompt\n",
    "    prompt = f\"\"\"\n",
    "        You are a dbt expert and Jinja programmer. Your task is to generate a concise description of the macro code provided.\n",
    "\n",
    "        Macro Jinja Code:\n",
    "        {query}\n",
    "\n",
    "        Guidelines:\n",
    "        1. Focus on the main purpose of the macro in simple and clear language.\n",
    "        2. Avoid explaining dbt-specific functions like `ref`, `source`, or `config`, unless they are central to the macro's purpose.\n",
    "        3. Write in a professional tone, avoiding redundant phrases like \"This macro\" or \"The code\".\n",
    "        4. Limit the description to a maximum of **50 words**.\n",
    "        5. Describe multiple functional sections separately if applicable, using a clear and structured format.\n",
    "\n",
    "        Examples:\n",
    "        - Creates dynamic SQL for filtering data by date range and category.\n",
    "        - Defines a reusable calculation for profit margin across models.\n",
    "        - Dynamically formats column names to snake_case based on inputs.\n",
    "        - Generates a pivot table structure for specified dimensions.\n",
    "\n",
    "        Provide a clear and concise description of the macro:\n",
    "    \"\"\"\n",
    "\n",
    "    # Interact\n",
    "    response = llm([HumanMessage(content=prompt)])\n",
    "    return response.content\n",
    "\n",
    "def generate_macro_description(row):\n",
    "    if row['is_macro']:\n",
    "        sql_code = row['code'] if pd.notna(row['code']) else \"\"\n",
    "        return generate_macro_code_description(llm, sql_code)\n",
    "    return row['description']\n",
    "\n",
    "dbt_project_df['description'] = None\n",
    "dbt_project_df['description'] = dbt_project_df.progress_apply(generate_macro_description, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dbt_config_code_summary(llm, config_content):\n",
    "    prompt = f\"\"\"\n",
    "        You are an expert in dbt configurations. Your task is to extract and summarize the key configurations from the provided dbt project configuration file.\n",
    "\n",
    "        dbt Project Configuration Content:\n",
    "        {config_content}\n",
    "\n",
    "        Guidelines:\n",
    "        1. Extract and list the key configurations such as project name, version, schema, paths, and any custom settings.\n",
    "        2. Highlight specific values or settings for important configurations (e.g., `name`, `version`, `target-path`, `source-paths`).\n",
    "        3. Ignore general information or default values unless explicitly overridden.\n",
    "        4. Format the output as a clear and concise bulleted list of configurations.\n",
    "        5. Avoid explanations or context; only list the configurations and their values.\n",
    "\n",
    "        Example Output:\n",
    "        - Name: jaffle_shop\n",
    "        - Version: 1.2\n",
    "        - Target Path: target/\n",
    "        - Source Paths: models/\n",
    "        - Schema: analytics\n",
    "        - Custom Setting: Materialized as incremental\n",
    "\n",
    "        Provide a concise bulleted summary of the dbt project configuration:\n",
    "    \"\"\"\n",
    "    response = llm([HumanMessage(content=prompt)])\n",
    "    return response.content\n",
    "\n",
    "def generate_dbt_config_summary(row):\n",
    "    if row['file_name'] == 'dbt_project.yml' and pd.notna(row['code']):\n",
    "        return generate_dbt_config_code_summary(llm, row['code'])\n",
    "    return row['description']\n",
    "\n",
    "dbt_project_df['description'] = dbt_project_df.progress_apply(generate_dbt_config_summary, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbt_project_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_test_code_description(llm, tests_content):\n",
    "    prompt = f\"\"\"\n",
    "        You are a dbt expert. Your task is to generate a concise and clear description of the tests applied in the dbt project.\n",
    "\n",
    "        Tests Content:\n",
    "        {tests_content}\n",
    "\n",
    "        Guidelines:\n",
    "        1. Summarize the purpose and functionality of each test, focusing on what it validates or ensures.\n",
    "        2. Use a professional tone, avoiding redundancy and overly technical details.\n",
    "        3. Highlight key aspects such as data quality, constraints, and validation objectives.\n",
    "        4. Limit the description to **50 words** per test, ensuring clarity and relevance.\n",
    "        5. If applicable, group similar tests and describe their collective purpose.\n",
    "\n",
    "        Examples:\n",
    "        - Test 1: Validates that all primary keys in the `orders` table are unique.\n",
    "        - Test 2: Ensures that no null values exist in critical columns like `customer_id` and `order_date`.\n",
    "        - Grouped Tests: Validate referential integrity between `orders` and `customers`.\n",
    "\n",
    "        Provide a clear and concise description of the tests:\n",
    "    \"\"\"\n",
    "    response = llm([HumanMessage(content=prompt)])\n",
    "    return response.content\n",
    "\n",
    "def generate_tests_description(row):\n",
    "    if row['is_test']:\n",
    "        return generate_test_code_description(llm, row['code'])\n",
    "    return row['description']\n",
    "\n",
    "dbt_project_df['description'] = dbt_project_df.progress_apply(generate_tests_description, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
