{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import yaml\n",
    "import sqlparse\n",
    "import os\n",
    "import pandas as pd\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 20)\n",
    "pd.set_option('display.max_columns', 30)\n",
    "#pd.set_option('display.width', None)\n",
    "#pd.set_option('display.max_colwidth', 10) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract repo elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_owner_and_repo(github_url):\n",
    "    try:\n",
    "        # Remove the base URL and split the rest\n",
    "        parts = github_url.replace(\"https://github.com/\", \"\").split(\"/\")\n",
    "        # Validate structure\n",
    "        if len(parts) >= 2:\n",
    "            owner = parts[0]\n",
    "            repo = parts[1]\n",
    "            return owner, repo\n",
    "        else:\n",
    "            raise ValueError(\"Invalid GitHub URL structure.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None, None\n",
    "\n",
    "def list_local_repo_structure(repo_path):\n",
    "    paths = []\n",
    "    for root, dirs, files in os.walk(repo_path):\n",
    "        rel_dir = os.path.relpath(root, repo_path)\n",
    "        if rel_dir == '.':\n",
    "            rel_dir = ''\n",
    "        if rel_dir:\n",
    "            paths.append(rel_dir + '/')\n",
    "        for f in files:\n",
    "            file_path = f\"{rel_dir}/{f}\" if rel_dir else f\n",
    "            paths.append(file_path)\n",
    "    return paths\n",
    "\n",
    "def list_online_repo_structure(owner, repo):\n",
    "    url = f\"https://api.github.com/repos/{owner}/{repo}/contents/\"\n",
    "    stack = [(url, '')]\n",
    "    paths = []\n",
    "    while stack:\n",
    "        current_url, current_path = stack.pop()\n",
    "        response = requests.get(current_url)\n",
    "        if response.status_code == 200:\n",
    "            items = response.json()\n",
    "            for item in items:\n",
    "                if item['type'] == 'dir':\n",
    "                    paths.append(current_path + item['name'] + '/')\n",
    "                    stack.append((item['url'], current_path + item['name'] + '/'))\n",
    "                else:\n",
    "                    paths.append(current_path + item['name'])\n",
    "    return paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_online_repo(path):\n",
    "    return path.startswith(\"http://\") or path.startswith(\"https://\")\n",
    "\n",
    "local_dbt_repo = ''\n",
    "online_dbt_repo = 'https://github.com/dbt-labs/jaffle-shop'\n",
    "\n",
    "# Use local repo?\n",
    "if False:\n",
    "    repo_path = local_dbt_repo\n",
    "else:\n",
    "    repo_path = online_dbt_repo\n",
    "\n",
    "is_online = is_online_repo(repo_path)\n",
    "if is_online:\n",
    "    owner, repo = extract_owner_and_repo(online_dbt_repo)\n",
    "    repo_elements = list_online_repo_structure(owner,repo)\n",
    "else:\n",
    "    repo_elements = list_local_repo_structure(local_dbt_repo)\n",
    "\n",
    "print(repo_elements)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dbt models knowledge db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select dbt elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbt_extensions = ['.sql', '.yml', '.yaml', '.csv']\n",
    "\n",
    "def select_dbt_elements_by_extension(dbt_extensions, repo_elements):\n",
    "    # Filter elements with relevant extensions\n",
    "    return [element for element in repo_elements if any(element.endswith(ext) for ext in dbt_extensions)]\n",
    "\n",
    "repo_dbt_elements = select_dbt_elements_by_extension(dbt_extensions, repo_elements)\n",
    "print(repo_dbt_elements)\n",
    "\n",
    "def select_dbt_models(dbt_extensions, repo_dbt_elements):\n",
    "    return [\n",
    "        element for element in repo_dbt_elements\n",
    "        if element.startswith('models/') and any(element.endswith(ext) for ext in dbt_extensions)\n",
    "    ]\n",
    "\n",
    "repo_dbt_models = select_dbt_models(dbt_extensions, repo_dbt_elements)\n",
    "print(repo_dbt_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbt_config_elements = ['packages.yml', 'dbt_project.yml']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dbt_models_df(repo_dbt_models):\n",
    "    data = []\n",
    "    for path in repo_dbt_models:\n",
    "        name = os.path.basename(path)\n",
    "        extension = os.path.splitext(name)[1]\n",
    "        data.append({'path': path, 'name': name, 'extension': extension})\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "dbt_models_df = generate_dbt_models_df(repo_dbt_models)\n",
    "display(dbt_models_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add sql code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    def get_base_url(repo_url):\n",
    "        if repo_url.startswith(\"https://github.com\"):\n",
    "            parts = repo_url.replace(\"https://github.com/\", \"\").split(\"/\")\n",
    "            owner, repo = parts[0], parts[1]\n",
    "            return f\"https://raw.githubusercontent.com/{owner}/{repo}/main\"\n",
    "        else:\n",
    "            raise ValueError(\"URL not valid.\")\n",
    "\n",
    "    def extract_file_content(path, is_online = False, repo_base_url = None):\n",
    "        try:\n",
    "            if is_online:\n",
    "                # Build complete URL\n",
    "                file_url = f\"{repo_base_url}/{path}\" if repo_base_url else path\n",
    "                response = requests.get(file_url)\n",
    "                if response.status_code == 200:\n",
    "                    return response.text\n",
    "                else:\n",
    "                    return f\"Error: {response.status_code} {response.reason}\"\n",
    "            else:\n",
    "                # Read content\n",
    "                with open(path, 'r', encoding='utf-8') as file:\n",
    "                    return file.read()\n",
    "        except Exception as e:\n",
    "            return f\"Error: {e}\"\n",
    "\n",
    "    def add_code_column(df, is_online = False, repo_url = None):\n",
    "        if is_online:\n",
    "            repo_base_url = get_base_url(repo_url)\n",
    "        else:\n",
    "            repo_base_url = ''\n",
    "\n",
    "        df['sql_code'] = df['path'].apply(lambda path: extract_file_content(path, is_online, repo_base_url))\n",
    "        return df\n",
    "\n",
    "    dbt_models_df = add_code_column(dbt_models_df, is_online, online_dbt_repo)\n",
    "    dbt_models_df.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_base_url(repo_url):\n",
    "    if repo_url.startswith(\"https://github.com\"):\n",
    "        parts = repo_url.replace(\"https://github.com/\", \"\").split(\"/\")\n",
    "        owner, repo = parts[0], parts[1]\n",
    "        return f\"https://raw.githubusercontent.com/{owner}/{repo}/main\"\n",
    "    else:\n",
    "        raise ValueError(\"URL not valid.\")\n",
    "\n",
    "def extract_file_content(path, is_online=False, repo_base_url=None):\n",
    "    try:\n",
    "        if is_online:\n",
    "            # Build complete URL\n",
    "            file_url = f\"{repo_base_url}/{path}\" if repo_base_url else path\n",
    "            response = requests.get(file_url)\n",
    "            if response.status_code == 200:\n",
    "                content = response.text\n",
    "            else:\n",
    "                return f\"Error: {response.status_code} {response.reason}\"\n",
    "        else:\n",
    "            # Read content locally\n",
    "            with open(path, 'r', encoding='utf-8') as file:\n",
    "                content = file.read()\n",
    "\n",
    "        # Process content based on file type\n",
    "        if path.endswith(('.yml', '.yaml')):\n",
    "            try:\n",
    "                return yaml.safe_load(content)  # Parse YAML and return as dictionary\n",
    "            except yaml.YAMLError as e:\n",
    "                return f\"Error parsing YAML: {e}\"\n",
    "        elif path.endswith('.sql'):\n",
    "            try:\n",
    "                return sqlparse.format(content, reindent=True, keyword_case='upper')  # Format SQL\n",
    "            except Exception as e:\n",
    "                return f\"Error parsing SQL: {e}\"\n",
    "        else:\n",
    "            return content  # Return plain text for other types\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "def add_code_column(df, is_online=False, repo_url=None):\n",
    "    if is_online:\n",
    "        repo_base_url = get_base_url(repo_url)\n",
    "    else:\n",
    "        repo_base_url = ''\n",
    "\n",
    "    # Extract content for each file and process it based on type\n",
    "    df['sql_code'] = df['path'].apply(lambda path: extract_file_content(path, is_online, repo_base_url))\n",
    "    return df\n",
    "\n",
    "dbt_models_df = add_code_column(dbt_models_df, is_online, online_dbt_repo)\n",
    "dbt_models_df.head(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add config block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_config_block(sql_code):\n",
    "    pattern = r\"{{\\s*config\\((.*?)\\)\\s*}}\"\n",
    "    match = re.search(pattern, sql_code, re.DOTALL)\n",
    "    return match.group(0) if match else None\n",
    "\n",
    "def add_config_column(df):\n",
    "    df['config'] = df.apply(\n",
    "        lambda row: extract_config_block(row['sql_code']) if row['extension'] == '.sql' else None,\n",
    "        axis=1\n",
    "    )\n",
    "    return df\n",
    "\n",
    "dbt_models_df = add_config_column(dbt_models_df)\n",
    "dbt_models_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = \"\"\"\n",
    "{{\n",
    "    config(\n",
    "        materialized=\"table\"\n",
    "    )\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "dbt_models_df.at[0, 'config'] = test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add model metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_materialized_value(config_text):\n",
    "    if config_text:\n",
    "        match = re.search(r\"materialized\\s*=\\s*[\\\"']([^\\\"']+)[\\\"']\", config_text)\n",
    "        return match.group(1) if match else None\n",
    "    return None\n",
    "\n",
    "def check_is_snapshot(config_text):\n",
    "    if config_text:\n",
    "        return 'strategy' in config_text\n",
    "    return False\n",
    "\n",
    "dbt_models_df['materialized'] = dbt_models_df['config'].apply(extract_materialized_value)\n",
    "dbt_models_df['is_snapshot'] = dbt_models_df['config'].apply(check_is_snapshot)\n",
    "dbt_models_df['materialized'] = dbt_models_df.apply(lambda row: 'snapshot' if row['is_snapshot'] else row['materialized'] ,1)\n",
    "\n",
    "def contains_jinja_code(code_text):\n",
    "    if isinstance(code_text, str):\n",
    "        return bool(re.search(r\"{%|{#\", code_text))\n",
    "    return False\n",
    "\n",
    "dbt_models_df['has_jinja_code'] = dbt_models_df['sql_code'].apply(contains_jinja_code)\n",
    "\n",
    "\n",
    "def categorize_model(name):\n",
    "    if name.startswith(\"base\"):\n",
    "        return \"base\"\n",
    "    elif name.startswith(\"stg\"):\n",
    "        return \"stg\"\n",
    "    elif name.startswith(\"int\"):\n",
    "        return \"int\"\n",
    "    elif name.startswith(\"test\"):\n",
    "        return \"test\"\n",
    "    elif name.startswith(\"snap\"):\n",
    "        return \"snap\"\n",
    "    elif name.startswith(\"__sources\"):\n",
    "        return \"sources\"\n",
    "    else:\n",
    "        return \"other\"\n",
    "\n",
    "dbt_models_df['model_category'] = dbt_models_df['name'].apply(categorize_model)\n",
    "\n",
    "def get_vertical(name, model_category):\n",
    "    base_name = re.sub(r'\\.[^.]+$', '', name)\n",
    "    \n",
    "    if model_category == 'sources':\n",
    "        return 'sources'\n",
    "    \n",
    "    known_categories = ['stg', 'int']\n",
    "    if model_category not in known_categories:\n",
    "        # Para model_category = other u otras no conocidas, devolver base_name sin extensi√≥n\n",
    "        return base_name\n",
    "    \n",
    "    # Para stg o int, extraer vertical antes de \"__\" o \".\"\n",
    "    pattern = rf'^{re.escape(model_category)}_([a-z0-9_]+?)(?:__|\\.|$)'\n",
    "    match = re.search(pattern, base_name)\n",
    "    return match.group(1) if match else base_name\n",
    "\n",
    "dbt_models_df['vertical'] = dbt_models_df.apply(lambda row: get_vertical(row['name'], row['model_category']), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Zip the dataframe by models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_yml_rows_to_each_model(dbt_models_df):\n",
    "    dbt_models_df['yml_code'] = None\n",
    "\n",
    "    yml_df = dbt_models_df[dbt_models_df['extension'] == '.yml'].copy()\n",
    "    yml_df['delete'] = False\n",
    "\n",
    "    for idx, row in yml_df.iterrows():\n",
    "        base_name = row['name'].rsplit('.', 1)[0]\n",
    "\n",
    "        sql_match = dbt_models_df[(dbt_models_df['name'] == base_name + '.sql')]\n",
    "\n",
    "        if not sql_match.empty:\n",
    "            dbt_models_df.at[sql_match.index[0], 'yml_code'] = row['sql_code']\n",
    "            yml_df.at[idx, 'delete'] = True\n",
    "        else:\n",
    "            yml_df.at[idx, 'yml_code'] = row['sql_code']\n",
    "            yml_df.at[idx, 'sql_code'] = None\n",
    "\n",
    "    yml_df = yml_df[~yml_df['delete']]\n",
    "\n",
    "    dbt_models_df = dbt_models_df[dbt_models_df['extension'] != '.yml']\n",
    "\n",
    "    yml_df = yml_df.drop(columns=['delete'])\n",
    "    dbt_models_df = pd.concat([dbt_models_df, yml_df], ignore_index=True)\n",
    "\n",
    "    return dbt_models_df\n",
    "\n",
    "dbt_models_df = assign_yml_rows_to_each_model(dbt_models_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract sql code info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tests(yml_code):\n",
    "    if not isinstance(yml_code, dict):\n",
    "        return None\n",
    "\n",
    "    tests_dict = {'columns': {}, 'unit_tests': []}\n",
    "\n",
    "    # Extract tests from all models\n",
    "    for model in yml_code.get('models', []):\n",
    "        for column in model.get('columns', []):\n",
    "            column_name = column.get('name')\n",
    "            if column_name:\n",
    "                # Combine 'tests' and 'data_tests' if present\n",
    "                tests = column.get('tests', []) + column.get('data_tests', [])\n",
    "                if tests:\n",
    "                    tests_dict['columns'][column_name] = tests\n",
    "\n",
    "    # Extract unit tests\n",
    "    if 'unit_tests' in yml_code:\n",
    "        unit_test_names = [test.get('name') for test in yml_code['unit_tests'] if test.get('name')]\n",
    "        if unit_test_names:\n",
    "            tests_dict['unit_tests'] = unit_test_names\n",
    "\n",
    "    return tests_dict if tests_dict['columns'] or tests_dict['unit_tests'] else None\n",
    "\n",
    "dbt_models_df['tests'] = dbt_models_df['yml_code'].apply(extract_tests)\n",
    "dbt_models_df['has_tests'] = dbt_models_df['tests'].apply(lambda x: x is not None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ids_from_query(code):\n",
    "    if not isinstance(code, str):\n",
    "        return None\n",
    "    \n",
    "    # Parse the SQL query\n",
    "    parsed = sqlparse.parse(code)\n",
    "    if not parsed:\n",
    "        return None\n",
    "    \n",
    "    # Regular expression to find columns ending in '_id'\n",
    "    id_pattern = re.compile(r'\\b(\\w+_id)\\b')\n",
    "    \n",
    "    cte_ids = set()\n",
    "    output_ids = set()\n",
    "    \n",
    "    for statement in parsed:\n",
    "        # Flatten tokens to handle nested structures\n",
    "        token_list = sqlparse.sql.TokenList(statement.tokens).flatten()\n",
    "        inside_cte = False\n",
    "        \n",
    "        for token in token_list:\n",
    "            # Detect CTE start (with keyword 'WITH')\n",
    "            if token.ttype is sqlparse.tokens.Keyword and token.value.upper() == 'WITH':\n",
    "                inside_cte = True\n",
    "            \n",
    "            # Detect SELECT after a WITH block ends\n",
    "            if inside_cte and token.ttype is sqlparse.tokens.Keyword and token.value.upper() == 'SELECT':\n",
    "                inside_cte = False\n",
    "            \n",
    "            if token.ttype is sqlparse.tokens.Name or token.ttype is None:\n",
    "                match = id_pattern.search(token.value)\n",
    "                if match:\n",
    "                    if inside_cte:\n",
    "                        cte_ids.add(match.group(1))\n",
    "                    else:\n",
    "                        output_ids.add(match.group(1))\n",
    "    ids = {\n",
    "        'cte_ids': list(cte_ids),\n",
    "        'output_ids': list(output_ids)\n",
    "    }\n",
    "    return ids['output_ids'] if ids['output_ids'] != [] else None\n",
    "\n",
    "dbt_models_df['sql_ids'] = dbt_models_df['sql_code'].apply(extract_ids_from_query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_select_all_in_last_select(code):\n",
    "    if not isinstance(code, str):\n",
    "        return False\n",
    "\n",
    "    parsed = sqlparse.parse(code)\n",
    "    if not parsed:\n",
    "        return False\n",
    "\n",
    "    select_statements = [stmt for stmt in parsed if stmt.get_type() == 'SELECT']\n",
    "    if not select_statements:\n",
    "        return False\n",
    "    last_select = select_statements[-1]\n",
    "\n",
    "    for token in last_select.tokens:\n",
    "        if token.ttype is sqlparse.tokens.Wildcard and token.value == '*':\n",
    "            return True\n",
    "\n",
    "    return False\n",
    "\n",
    "dbt_models_df['has_select_all_in_last_select'] = dbt_models_df['sql_code'].apply(has_select_all_in_last_select)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_group_by(code):\n",
    "    if not isinstance(code, str):\n",
    "        return False\n",
    "\n",
    "    parsed = sqlparse.parse(code)\n",
    "    if not parsed:\n",
    "        return False\n",
    "    return 'group by' in code.lower()\n",
    "\n",
    "\n",
    "dbt_models_df['has_group_by'] = dbt_models_df['sql_code'].apply(has_group_by)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_primary_key(tests_dict):\n",
    "    if not isinstance(tests_dict, dict) or 'columns' not in tests_dict:\n",
    "        return None\n",
    "\n",
    "    for column, tests in tests_dict.get('columns', {}).items():\n",
    "        # Check if the column has the required tests for a primary key\n",
    "        if tests == ['not_null', 'unique'] or 'dbt_constraints.primary_key' in tests:\n",
    "            return column\n",
    "    \n",
    "    return None\n",
    "\n",
    "dbt_models_df['primary_key'] = dbt_models_df['tests'].apply(find_primary_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sql_filters(sql_query):\n",
    "    if not isinstance(sql_query, str) or not sql_query.strip():\n",
    "        return None\n",
    "\n",
    "    sql_query_clean = ' '.join(sql_query.split()).lower()\n",
    "\n",
    "    filters_patterns = [\n",
    "        (r'\\bwhere\\b\\s+(.*?)(?=\\bgroup\\b|\\border\\b|\\blimit\\b|\\bhaving\\b|;|$)', 'where'),\n",
    "        (r'\\bon\\b\\s+(.*?)(?=\\bleft\\b|\\bright\\b|\\binner\\b|\\bouter\\b|\\bjoin\\b|\\bselect\\b|\\bwhere\\b|\\bgroup\\b|\\border\\b|\\blimit\\b|;|$)', 'join'),\n",
    "        (r'\\bhaving\\b\\s+(.*?)(?=\\bgroup\\b|\\border\\b|\\blimit\\b|;|$)', 'having')\n",
    "    ]\n",
    "\n",
    "    filters = []\n",
    "    joins = []\n",
    "\n",
    "    for pattern, clause_type in filters_patterns:\n",
    "        matches = re.findall(pattern, sql_query_clean, re.DOTALL)\n",
    "        for match in matches:\n",
    "            sub_conditions = re.split(r'\\band\\b|\\bor\\b', match)\n",
    "            for condition in sub_conditions:\n",
    "                cleaned = condition.strip().strip('()')\n",
    "                if cleaned:\n",
    "                    if clause_type == 'join':\n",
    "                        joins.append(cleaned)\n",
    "                    else:\n",
    "                        filters.append(cleaned)\n",
    "    all_filters = filters + joins\n",
    "    return all_filters if all_filters != [] else None\n",
    "\n",
    "dbt_models_df['filters'] = dbt_models_df['sql_code'].apply(extract_sql_filters)\n",
    "dbt_models_df['is_filtered'] = dbt_models_df['filters'].apply(lambda x: x is not None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_dbt_macros(sql_query):\n",
    "\n",
    "    if not isinstance(sql_query, str) or not sql_query.strip():\n",
    "        return None\n",
    "    \n",
    "    macro_pattern = r\"\\{\\{\\s*([\\w\\.]+)\\s*\\(.*?\\)\\s*\\}\\}\"\n",
    "    matches = re.findall(macro_pattern, sql_query)\n",
    "    filtered_macros = sorted(set(m for m in matches if m not in ('ref', 'source')))\n",
    "    \n",
    "    return filtered_macros if filtered_macros != [] else None\n",
    "\n",
    "dbt_models_df['macros'] = dbt_models_df['sql_code'].apply(extract_dbt_macros)\n",
    "dbt_models_df['has_macros'] = dbt_models_df['macros'].apply(lambda x: x is not None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate models structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_source_details(code, source_pattern):\n",
    "    if not isinstance(code, str):\n",
    "        return False, None\n",
    "    sources = re.findall(source_pattern, code)\n",
    "    if sources:\n",
    "        return True, [f\"{source[0]}.{source[1]}\" for source in sources]\n",
    "    return False, None\n",
    "\n",
    "def enrich_dbt_models(dbt_models_df):\n",
    "    # Helper regex patterns\n",
    "    source_pattern = r\"\\{\\{\\s*source\\(['\\\"](.*?)['\\\"],\\s*['\\\"](.*?)['\\\"]\\)\\s*\\}\\}\"\n",
    "    ref_pattern = r\"\\{\\{\\s*ref\\(['\\\"](.*?)['\\\"]\\)\\s*\\}\\}\"\n",
    "    \n",
    "    # Add 'parent_models' - extract all models referenced using 'ref'\n",
    "    dbt_models_df['parent_models'] = dbt_models_df['sql_code'].apply(\n",
    "        lambda code: re.findall(ref_pattern, code) if isinstance(code, str) else []\n",
    "    )\n",
    "    \n",
    "    dbt_models_df[['is_source_model', 'source']] = dbt_models_df['sql_code'].apply(\n",
    "        lambda code: pd.Series(extract_source_details(code, source_pattern))\n",
    "    )\n",
    "    \n",
    "    # Build a dictionary to track children relationships\n",
    "    model_children = {}\n",
    "    for idx, row in dbt_models_df.iterrows():\n",
    "        for parent in row['parent_models']:\n",
    "            model_children.setdefault(parent, []).append(row['name'].replace('.sql', ''))\n",
    "\n",
    "    # Add 'children_models' - list all models that depend on this model\n",
    "    dbt_models_df['children_models'] = dbt_models_df['name'].apply(\n",
    "        lambda name: model_children.get(name.replace('.sql', ''), [])\n",
    "    )\n",
    "    \n",
    "    # Add 'is_end_model' - True if there are no children\n",
    "    dbt_models_df['is_end_model'] = dbt_models_df['children_models'].apply(lambda children: len(children) == 0)\n",
    "    \n",
    "    return dbt_models_df\n",
    "\n",
    "dbt_models_enriched_df = enrich_dbt_models(dbt_models_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(dbt_models_enriched_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add descriptions using LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "repo_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "if repo_root not in sys.path:\n",
    "    sys.path.append(repo_root)\n",
    "\n",
    "import openai_setup\n",
    "\n",
    "OPENAI_API_KEY = openai_setup.conf['key']\n",
    "OPENAI_PROJECT = openai_setup.conf['project']\n",
    "OPENAI_ORGANIZATION = openai_setup.conf['organization']\n",
    "DEFAULT_LLM_MODEL = \"gpt-4o-mini\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI  # Importa el modelo de OpenAI\n",
    "from langchain.schema import HumanMessage  # Para interactuar con mensajes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=DEFAULT_LLM_MODEL, temperature=0.1, openai_api_key=OPENAI_API_KEY, openai_organization = OPENAI_ORGANIZATION,  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_query_description(llm, query, documentation = None):\n",
    "    # Context and prompt\n",
    "    prompt = f\"\"\"\n",
    "    Given the following SQL query and additional documentation, describe in a few sentences what the query does.\n",
    "\n",
    "    SQL Query:\n",
    "    {query}\n",
    "\n",
    "    Documentation:\n",
    "    {documentation if documentation else \"No additional documentation provided.\"}\n",
    "\n",
    "    Please provide a concise and clear description.\n",
    "    \"\"\"\n",
    "\n",
    "    # Interact\n",
    "    response = llm([HumanMessage(content=prompt)])\n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_query = dbt_models_enriched_df.iloc[1].sql_code\n",
    "print(example_query)\n",
    "example_doc = dbt_models_enriched_df.iloc[1].yml_code\n",
    "print(example_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_query_description(llm, example_query, example_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm:\n",
    "- resumen escrito de lo que hace la query\n",
    "- jinja: para que es el jinja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
