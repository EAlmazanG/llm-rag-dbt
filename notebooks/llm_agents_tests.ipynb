{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import yaml\n",
    "import sqlparse\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_repo_root_path():\n",
    "    import os\n",
    "    import sys\n",
    "    repo_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "    if repo_root not in sys.path:\n",
    "        sys.path.append(repo_root)\n",
    "        \n",
    "add_repo_root_path()\n",
    "from src import generate_knowledge\n",
    "from src import create_rag_db\n",
    "from src import llm_chain_tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_columns', 50)\n",
    "#pd.set_option('display.width', None)\n",
    "#pd.set_option('display.max_colwidth', 10) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_knowledge.add_repo_root_path()\n",
    "import openai_setup\n",
    "\n",
    "OPENAI_API_KEY = openai_setup.conf['key']\n",
    "OPENAI_PROJECT = openai_setup.conf['project']\n",
    "OPENAI_ORGANIZATION = openai_setup.conf['organization']\n",
    "DEFAULT_LLM_MODEL = \"gpt-4o-mini\"\n",
    "\n",
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
    "os.environ['OPENAI_MODEL_NAME'] = DEFAULT_LLM_MODEL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "langchain_openai_embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY, model=\"text-embedding-ada-002\")\n",
    "langchain_openai_llm = ChatOpenAI(model=DEFAULT_LLM_MODEL, temperature=0.1, openai_api_key=OPENAI_API_KEY, openai_organization = OPENAI_ORGANIZATION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'interpretation_agent': {'role': 'Request Interpreter\\n', 'goal': 'Interpret user requests related to dbt projects and translate them into actionable decisions. Use expertise in dbt, data modeling, and analytics engineering to determine the type of action required.\\n', 'backstory': \"You specialize in analyzing requests to identify whether the action involves adding a field, modifying an existing model, or retrieving specific information. Your goal is to provide concise and actionable outputs tailored to the user's needs.\\n\", 'verbose': True, 'allow_delegation': False}, 'evaluation_agent': {'role': 'Evaluation Specialist\\n', 'goal': 'Evaluate user requests related to dbt projects and provide concise, actionable insights and steps required to address the request. Leverage expertise in data modeling, dbt project structure, and dependency analysis to ensure accurate evaluations.\\n', 'backstory': 'You specialize in analyzing interpreted requests and breaking them down into specific, actionable tasks while considering dependencies, performance, documentation, and configuration impacts within the dbt project. The goal is to ensure efficient and effective implementations.\\n', 'verbose': True, 'allow_delegation': False}, 'lineage_agent': {'role': 'Lineage Analysis Agent\\n', 'goal': 'Determine the primary dbt model directly affected by the request and identify the scope of models (UP, DOWN, or ALL) that need to be considered for handling the request effectively.\\n', 'backstory': 'You are an expert in evaluating dbt model dependencies, analyzing lineage, and interpreting metadata to define the scope of the request.  Your expertise ensures precise and actionable recommendations for managing the relationships between dbt models and their impact.\\n', 'verbose': True, 'allow_delegation': False}, 'plan_agent': {'role': 'Planning Agent\\n', 'goal': 'Create a detailed, step-by-step plan for implementing requested changes in the dbt project based on the lineage analysis and retrieved context.\\n', 'backstory': \"You specialize in breaking down complex requests into actionable plans that ensure alignment with the dbt project's structure,  dependencies, and conventions, while preventing schema-breaking changes.\\n\", 'verbose': True, 'allow_delegation': False}}\n",
      "{'interpretation_task': {'description': \"Evaluate the user's request: {request} and based on the evaluation, determine the required action: 1) adding a field -> return ADD_COLUMN 2) modifying an existing model -> return MODIFY_MODEL 3) retrieving and returning specific information. -> return RETRIVE_INFO\\nReflect on the request and provide a concise plan for the approach: - If the action involves adding a field: Identify where the field is currently available, if provided. - Determine how to propagate the field through the necessary models or transformations to integrate it into the target model. - Consider the impact on related models and dependencies. - If the action involves modifying an existing model: Identify the specific changes required. - Assess how these changes affect the structure, relationships, and downstream dependencies of the model. - If the action involves retrieving or returning information: Identify the models containing the relevant data. - Analyze how these models are related, and determine the queries or transformations needed to extract the requested information.\\n\", 'expected_output': 'Return one of the following actions: - ADD_COLUMN - MODIFY_MODEL - RETRIEVE_INFO\\nReturn the required action from the three proposed, and a short explanation of the required work. REMEMBER: Provide no extra commentary or explanation, just the minimal information required,   return only useful information, no additional filler text or unnecessary explanations, get to the point.\\n'}, 'evaluation_task': {'description': \"Based on the interpretation of an expert dbt and problem interpreter:  {interpretation}\\nof the original request ORIGINAL REQUEST:  {request}\\nInclude only these topics if relevant: - Target models or files - Field existence - Documentation needs - Dependencies and relationships - Performance/design considerations - Tests - dbt project config - Code or logic generation\\nSummarize only the necessary actions, no filler. Think about all the considerations and steps required to handle this request effectively. Your evaluation could include the following actions: - Identify the target model or models: Analyze the dependency tree to locate the model where the change should occur. - Understand its upstream sources (seeds, sources or base models) - Understand downstream dependencies of the model. - Documentation: Assess if it's going to be necessary to add adjustments to the documentation, and which would be this changes. - Unique keys and IDs: Examine the available unique keys and identifiers in the initial, intermediate, and final models. - Decide how these can be used to integrate the field or establish relationships between models. - Check if the granularity of the model can be altered through the changes added. - Evaluate performance and design: Review the data pipeline from start to finish. - Decide where the change or addition would be most optimal in terms of performance, data modeling, and maintainability. - Project state and impact: Consider the current state of the dbt project and how the changes might affect the broader model chain. - Macros and seeds: Check if relevant macros or seed data exist that can help transform or derive the required field or model. - Tests: Identify existing tests for the field or model and determine whether new tests need to be added or adjusted to validate the changes. - dbt project configuration: Review the general configuration (e.g., variables, environments, conventions) in the dbt_project.yml file to ensure the changes align with project standards and won't disrupt the schema. - Code generation: fragments of SQL logic needed, including CTEs or columns. - Evaluate whether an intermediate model is necessary or if the logic can be handled within the existing pipeline. - Documentation generation: Specify the documentation needed for any fields, models, or logic added or updated as part of this request. Only the things to add or change.\\n\", 'expected_output': \"Provide a concise summary of the high-level tasks based on your analysis, with the reflection of each one of them to be prepared to completed in the next steps when context is provided. If you don't have the info to perform the action because it's necessary context of the project, code or the lineage of the models, don't answer it, in next steps context will be provide as input to answer it properly. Mark it as it's necessart the context to responde it, and only do the reflection part using your logic as dbt expert. No extra checks or steps that are not on this list, select only the needed actions for the request. Return only useful information, no additional filler text or unnecessary explanations, get to the point.\\n      \\nREMEMBER: Provide no extra commentary or explanation, just the minimal information required.\\n\"}, 'lineage_task': {'description': 'Based on this evaluation: {evaluation}\\nand the retrieved context, remember the meaning of all the context data:\\n      \\nRETRIEVED CONTEXT:  {retrieved_context}\\nCONTEXT INFO AND METADATA MEANING: - knowledge_type: Specifies whether the resource is about files within the dbt project configuration, or data models used in the pipeline. - name: Name of the file. - path: Path within the repository where the file or resource is located, relative to the project root. - source: The original dataset, from which the model or resource pulls its data. Only for the first layer of the model. - parents: dbt models  that serve as input dependencies for the current model or file. - children: dbt models that depend on this resource as an input for their logic or data processing. - config: Configuration parameters defined in the file, specifying behaviors, sort keys, materialization... - materialized: Indicates how the data model is materialized (e.g., as a table, view, or ephemeral model) in the pipeline. - is_snapshot: Boolean flag that identifies whether the file represents a snapshot dbt model. - model_category: The logical dbt categorization of the model inside the project, such as base, staging, intermediate, marts... - vertical: Business domain or vertical to which the resource belongs, such as e-commerce, finance, supply... - has_tests: Boolean flag indicating if there are tests associated with this resource in the model .yaml file or in the tests folder. - has_select_all_in_last_select: Specifies if the final SQL query in the file uses a SELECT * statement. So all columns of previous CTEs will be consider as output of the model. - has_group_by: Boolean flag indicating if the SQL code includes a GROUP BY clause for aggregating data. - is_filtered: Boolean flag that specifies whether the resource applies filters to its data with WHERE, HAVING, JOIN... - is_source_model: The model uses the source macro to extract the data from the dbt project sources. It belongs to the first layer of all dbt models. - is_seed: Specifies if the resource is a seed file. - is_end_model: Boolean flag identifying if this is a terminal model in the data pipeline, representing the final output. - is_macro: Boolean flag indicating if the file defines a macro, used for reusable logic across the project. It does not count macros ref to call other models or source to connect to sources. - is_test: Indicates whether the file is a test sql definition used for testing purposes of other dbt models. - macros: Name of the macros used in the dbt model if any. It does not count macros ref to call other models or source to connect to sources. - packages: List of external packages or dependencies required for the resource or project functionality. Only not None in the packages.yml\\trow. Determine: - The primary dbt model directly affected (e.g., where a field is added, a modification is made, or information is requested). - Whether upstream models (UP), downstream models (DOWN), or both (ALL) are necessary to handle this request correctly.\\nConsider the following cases: 1. If a new field is added, specify the model where the field will be added and indicate UP for upstream models needed to populate the field. 2. If an existing field is modified, specify the model where the change occurs and indicate DOWN for downstream models affected by the change. 3. If information is requested, specify the model containing the requested information and indicate UP, DOWN, or ALL based on the context of the data needed. 4. If a field or model is removed, specify the model being affected and indicate DOWN for downstream dependencies impacted.\\n      \\n', 'expected_output': \"Return the following json format: {{'model':model name, 'scope':UP/DOWN/ALL}} REMEMBER: Provide no extra commentary or explanation, just the minimal information required.\\n\"}, 'plan_task': {'description': \"Based on this evaluation:  {evaluation}\\nThe analysis of the most impacted model and the part of the lineage that is affected or has related info  (UP = Upstream models(parent models)/DOWN = Downstream models(childen models)/ALL = both):  {lineage_analysis}\\nRETRIEVED CONTEXT:  {retrieved_context}\\nthe retrieved context, remember the meaning of all the context data: CONTEXT INFO AND METADATA MEANING: - knowledge_type: Specifies whether the resource is about files within the dbt project configuration, or data models used in the pipeline. - name: Name of the file. - path: Path within the repository where the file or resource is located, relative to the project root. - source: The original dataset, from which the model or resource pulls its data. Only for the first layer of the model. - parents: dbt models  that serve as input dependencies for the current model or file. - children: dbt models that depend on this resource as an input for their logic or data processing. - config: Configuration parameters defined in the file, specifying behaviors, sort keys, materialization... - materialized: Indicates how the data model is materialized (e.g., as a table, view, or ephemeral model) in the pipeline. - is_snapshot: Boolean flag that identifies whether the file represents a snapshot dbt model. - model_category: The logical dbt categorization of the model inside the project, such as base, staging, intermediate, marts... - vertical: Business domain or vertical to which the resource belongs, such as e-commerce, finance, supply... - has_tests: Boolean flag indicating if there are tests associated with this resource in the model .yaml file or in the tests folder. - has_select_all_in_last_select: Specifies if the final SQL query in the file uses a SELECT * statement. So all columns of previous CTEs will be consider as output of the model. - has_group_by: Boolean flag indicating if the SQL code includes a GROUP BY clause for aggregating data. - is_filtered: Boolean flag that specifies whether the resource applies filters to its data with WHERE, HAVING, JOIN... - is_source_model: The model uses the source macro to extract the data from the dbt project sources. It belongs to the first layer of all dbt models. - is_seed: Specifies if the resource is a seed file. - is_end_model: Boolean flag identifying if this is a terminal model in the data pipeline, representing the final output. - is_macro: Boolean flag indicating if the file defines a macro, used for reusable logic across the project. It does not count macros ref to call other models or source to connect to sources. - is_test: Indicates whether the file is a test sql definition used for testing purposes of other dbt models. - macros: Name of the macros used in the dbt model if any. It does not count macros ref to call other models or source to connect to sources. - packages: List of external packages or dependencies required for the resource or project functionality. Only not None in the packages.yml\\trow.\\nand the examples about the some of the sources and the seeds (if any): {retrieved_csv_sources_context}\\nCreate a detailed step-by-step plan of the changes required in the existing models or files within the repository. To implement the requested change accurately. 1. Ensure that you only refer to files, models, or fields that are explicitly mentioned in the retrieved information. 2. Do not invent new models, fields, or dependencies.  3. Focus on:\\n  - Identifying the exact files or models that need modifications, based on the retrieved context.\\n  - Specifying what changes should be made, such as adding fields, updating logic, or modifying relationships.\\n  - Highlighting any dependencies between models or files and describing how these should be handled.\\n  - Extract children or parent models affected.\\n  - If code fragments are provided in the retrieved context, incorporate them where applicable and explain their role.\\n  - If no specific code or file is mentioned in the retrieved information, state that no changes should be made to existing files.\\n  - Ensure the changes align with the dbt project's standards, such as conventions in `dbt_project.yml`, and do not introduce schema-breaking modifications.\\n\\n4. No extra checks or steps that are not on this list. 5. Provide precise and actionable recommendations, avoiding any assumptions beyond the retrieved information.\\n      \\n\", 'expected_output': 'Return a summary of all the process, with the reflection, plan and context and the changes tht are neeeded to perfor\\n      - The original request.\\n      - The interpretarion of the request.\\n      - The affected models with the info of the context and the lineage.\\n      - All the necceasry changes step by step with a clear and short explanaiton of why is needed.\\nReturn only useful information, no additional filler text or unnecessary explanations, get to the point. REMEMBER: Provide no extra commentary or explanation, just the minimal information required.'}}\n"
     ]
    }
   ],
   "source": [
    "# Define file paths for YAML configurations\n",
    "files = {\n",
    "    'agents': '../config/agents.yml',\n",
    "    'tasks': '../config/tasks.yml'\n",
    "}\n",
    "\n",
    "# Load configurations from YAML files\n",
    "configs = {}\n",
    "for config_type, file_path in files.items():\n",
    "    with open(file_path, 'r') as file:\n",
    "        configs[config_type] = yaml.safe_load(file)\n",
    "\n",
    "# Assign loaded configurations to specific variables\n",
    "agents_config = configs['agents']\n",
    "tasks_config = configs['tasks']\n",
    "\n",
    "print(agents_config)\n",
    "print(tasks_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from crewai import Agent, Task, Crew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Creating Agents\n",
    "interpretation_agent = Agent(\n",
    "  config=agents_config['interpretation_agent'],\n",
    ")\n",
    "\n",
    "evaluation_agent = Agent(\n",
    "  config=agents_config['evaluation_agent'],\n",
    ")\n",
    "\n",
    "lineage_agent = Agent(\n",
    "  config=agents_config['lineage_agent'],\n",
    ")\n",
    "\n",
    "plan_agent = Agent(\n",
    "  config=agents_config['plan_agent'],\n",
    ")\n",
    "\n",
    "# Creating Tasks\n",
    "interpretation_task = Task(\n",
    "  config=tasks_config['interpretation_task'],\n",
    "  agent=interpretation_agent\n",
    ")\n",
    "\n",
    "evaluation_task = Task(\n",
    "  config=tasks_config['evaluation_task'],\n",
    "  agent=evaluation_agent\n",
    ")\n",
    "\n",
    "lineage_task = Task(\n",
    "  config=tasks_config['lineage_task'],\n",
    "  agent=lineage_agent\n",
    ")\n",
    "\n",
    "plan_task = Task(\n",
    "  config=tasks_config['plan_task'],\n",
    "  agent=plan_agent\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "crew = Crew(\n",
    "  agents=[\n",
    "    interpretation_agent,\n",
    "    evaluation_agent\n",
    "  ],\n",
    "  tasks=[\n",
    "    interpretation_task,\n",
    "    evaluation_task\n",
    "  ],\n",
    "  verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mRequest Interpreter\u001b[00m\n",
      "\u001b[95m## Task:\u001b[00m \u001b[92mEvaluate the user's request: Give me all the information about the models related with customers and determine the required action: - Adding a field: Identify where the field is currently available (if provided),\n",
      "  determine how to propagate it through necessary models or transformations,\n",
      "  and assess the impact on related models and dependencies.\n",
      "- Modifying an existing model: Identify specific changes needed, evaluate the\n",
      "  impact on structure, relationships, and downstream dependencies.\n",
      "- Retrieving specific information: Identify models containing relevant data,\n",
      "  analyze relationships, and determine queries or transformations needed.\n",
      "\n",
      "Reflect on the request to generate a concise plan for the approach and provide a clear summary of the required action and its implications.\n",
      "\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mRequest Interpreter\u001b[00m\n",
      "\u001b[95m## Final Answer:\u001b[00m \u001b[92m\n",
      "RETRIEVE_INFO - The work required involves identifying all models related to customers, analyzing their relationships, and determining the specific queries or transformations needed to extract information about them. This includes understanding the data flow and the schema pertaining to customer-related models.\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mEvaluation Specialist\u001b[00m\n",
      "\u001b[95m## Task:\u001b[00m \u001b[92mEvaluate the interpretation of the user request and determine the necessary steps to handle it effectively.  Include only relevant considerations based on the following topics: - Target models or files - Field existence - Documentation needs - Dependencies and relationships - Performance/design considerations - Tests - dbt project config - Code or logic generation\n",
      "Possible actions and considerations may include: - Identifying the target models or files affected by the request. - Analyzing upstream and downstream dependencies to locate the change points. - Assessing the need for adjustments in documentation, tests, or configuration files. - Evaluating performance impacts and ensuring maintainability. - Generating necessary SQL logic fragments (e.g., CTEs, columns). - Determining if an intermediate model is required for the changes. - Highlighting potential granularity changes or unique key impacts.\n",
      "\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mEvaluation Specialist\u001b[00m\n",
      "\u001b[95m## Final Answer:\u001b[00m \u001b[92m\n",
      "To fulfill the request regarding customer-related models, follow these actionable steps:\n",
      "\n",
      "1. **Identify Target Models**: \n",
      "   - Locate all dbt models associated with customers. This includes any models that directly reference customer data as well as those indirectly related through views or transformations.\n",
      "\n",
      "2. **Analyze Relationships**:\n",
      "   - Review the dependency graph to assess upstream and downstream models that interact with the identified customer models. This will help pinpoint the relationships and data flow you need to consider.\n",
      "\n",
      "3. **Field Existence Check**:\n",
      "   - Verify the existence of relevant fields within each identified model. This includes ensuring that required customer attributes (e.g., customer_id, name) are present and correctly formatted.\n",
      "\n",
      "4. **Documentation Needs**:\n",
      "   - Update model documentation to include insights on the transformations applied to customer data, relationships with other models, and any business logic being implemented. Ensure that this documentation is clear and accessible for future reference.\n",
      "\n",
      "5. **Dependencies and Relationships**:\n",
      "   - Assess the impact of any changes to these customer-related models on other models that depend on them. Document any potential cascading effects that may arise from modifications to customer-related logic or data flows.\n",
      "\n",
      "6. **Performance/Design Considerations**:\n",
      "   - Evaluate performance implications of the queries or transformations related to customer data. Consider whether additional indices or materializations are required to maintain efficiency in data retrieval and transformations.\n",
      "\n",
      "7. **Testing Requirements**:\n",
      "   - Define tests for key integrity checks to ensure the correct functioning of customer-related transformations. This may include uniqueness tests on customer IDs or validating certain data quality metrics.\n",
      "\n",
      "8. **dbt Project Configuration**:\n",
      "   - Review any adjustments needed in the dbt project configuration. This includes ensuring models are correctly defined in `dbt_project.yml` regarding materialization strategies and ensuring they align with data governance protocols.\n",
      "\n",
      "9. **Code or Logic Generation**:\n",
      "   - Design the required SQL logic to extract and transform customer data based on these analyses. This may involve crafting SQL CTEs and selecting appropriate fields to accommodate the desired output.\n",
      "\n",
      "10. **Determine Intermediate Models**:\n",
      "    - Assess whether an intermediate model is necessary for transformations to streamline the workflow and enhance readability and maintainability of your SQL code.\n",
      "\n",
      "11. **Highlight Key Impacts**:\n",
      "    - Identify any changes in granularity or unique key considerations resulting from this analysis. Ensure these are documented and clearly indicated in the transformed datasets to prevent issues down the line.\n",
      "\n",
      "By following these steps, you can ensure a comprehensive approach to analyzing and transforming customer-related data within your dbt project.\u001b[00m\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_input = 'Give me all the information about the models related with customers'\n",
    "\n",
    "inputs = {\n",
    "  'request': user_input\n",
    "}\n",
    "\n",
    "# Run the crew\n",
    "result = crew.kickoff(\n",
    "  inputs=inputs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Flows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from crewai import Flow\n",
    "from crewai.flow.flow import listen, start\n",
    "\n",
    "class dbtChatFlow(Flow):\n",
    "    @start()\n",
    "    def interpret_prompt(self):\n",
    "        user_prompt = self.state[\"user_input\"]\n",
    "        print(user_prompt)\n",
    "        interpretation_result = crew.kickoff(inputs = {'request': user_prompt} )\n",
    "        self.state[\"interpretation_result\"] = interpretation_result\n",
    "        return interpretation_result\n",
    "\n",
    "    @listen(lambda state: \"interpretation_result\" in state)\n",
    "    def evaluate_interpretation(self):\n",
    "        interpretation_result = self.state.get(\"interpretation_result\")\n",
    "        evaluation_result = crew.agents[1].kickoff({\"request\": interpretation_result})\n",
    "        self.state[\"evaluation_result\"] = evaluation_result\n",
    "        return evaluation_result\n",
    "\n",
    "flow = dbtChatFlow()\n",
    "#flow.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Give me all the information about the models related with customers\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mRequest Interpreter\u001b[00m\n",
      "\u001b[95m## Task:\u001b[00m \u001b[92mEvaluate the user's request: Give me all the information about the models related with customers and determine the required action: - Adding a field: Identify where the field is currently available (if provided),\n",
      "  determine how to propagate it through necessary models or transformations,\n",
      "  and assess the impact on related models and dependencies.\n",
      "- Modifying an existing model: Identify specific changes needed, evaluate the\n",
      "  impact on structure, relationships, and downstream dependencies.\n",
      "- Retrieving specific information: Identify models containing relevant data,\n",
      "  analyze relationships, and determine queries or transformations needed.\n",
      "\n",
      "Reflect on the request to generate a concise plan for the approach and provide a clear summary of the required action and its implications.\n",
      "\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mRequest Interpreter\u001b[00m\n",
      "\u001b[95m## Final Answer:\u001b[00m \u001b[92m\n",
      "RETRIEVE_INFO - The user is requesting all information about models related to customers. This will involve identifying the specific models that contain customer-related data, analyzing their relationships, and potentially formulating queries to extract the required information efficiently.\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mEvaluation Specialist\u001b[00m\n",
      "\u001b[95m## Task:\u001b[00m \u001b[92mEvaluate the interpretation of the user request and determine the necessary steps to handle it effectively.  Include only relevant considerations based on the following topics: - Target models or files - Field existence - Documentation needs - Dependencies and relationships - Performance/design considerations - Tests - dbt project config - Code or logic generation\n",
      "Possible actions and considerations may include: - Identifying the target models or files affected by the request. - Analyzing upstream and downstream dependencies to locate the change points. - Assessing the need for adjustments in documentation, tests, or configuration files. - Evaluating performance impacts and ensuring maintainability. - Generating necessary SQL logic fragments (e.g., CTEs, columns). - Determining if an intermediate model is required for the changes. - Highlighting potential granularity changes or unique key impacts.\n",
      "\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mEvaluation Specialist\u001b[00m\n",
      "\u001b[95m## Final Answer:\u001b[00m \u001b[92m\n",
      "To fulfill the user request for all information about models related to customers, follow these high-level actionable steps:\n",
      "\n",
      "1. **Identify Target Models or Files**: \n",
      "   - Locate and list all dbt models that contain customer-related data. This could include models directly named 'customers' as well as any associated models such as 'customer_orders', 'customer_segments', or 'customer_addresses'.\n",
      "\n",
      "2. **Analyze Dependencies and Relationships**: \n",
      "   - Examine upstream and downstream dependencies for each identified model using dbt's lineage graph. Document which models feed into customer-related models and which models derive data from them.\n",
      "\n",
      "3. **Field Existence Evaluation**: \n",
      "   - Check the schema of each customer-related model to ensure that all necessary fields (e.g., customer ID, name, contact information) are present and correctly defined.\n",
      "\n",
      "4. **Assess Documentation Needs**: \n",
      "   - Review existing documentation for the identified models. Identify gaps in documentation and create or update documentation to clearly describe model purpose, field definitions, and relationships.\n",
      "\n",
      "5. **Performance/Design Considerations**: \n",
      "   - Evaluate if any of the models require optimization for performance. Consider whether changes in model granularity or keys (e.g., introducing unique keys) may enhance performance or facilitate better analytics.\n",
      "\n",
      "6. **Tests**: \n",
      "   - Review existing tests on customer-related models. Identify any necessary tests to ensure data quality, such as unique key tests or not-null constraints, and implement them if missing.\n",
      "\n",
      "7. **dbt Project Config**: \n",
      "   - Ensure that the dbt configuration files (dbt_project.yml, etc.) reflect appropriate settings for the identified models, including materializations and configurations that might impact performance.\n",
      "\n",
      "8. **Code or Logic Generation**: \n",
      "   - If necessary, prepare SQL queries or CTEs that extract relevant customer-related data efficiently. This may also involve creating intermediate models if data transformations are complex.\n",
      "\n",
      "By following these steps, you will effectively gather all pertinent information regarding customer-related models, ensuring comprehensive coverage of dependencies, documentation, and performance considerations.\u001b[00m\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CrewOutput(raw=\"To fulfill the user request for all information about models related to customers, follow these high-level actionable steps:\\n\\n1. **Identify Target Models or Files**: \\n   - Locate and list all dbt models that contain customer-related data. This could include models directly named 'customers' as well as any associated models such as 'customer_orders', 'customer_segments', or 'customer_addresses'.\\n\\n2. **Analyze Dependencies and Relationships**: \\n   - Examine upstream and downstream dependencies for each identified model using dbt's lineage graph. Document which models feed into customer-related models and which models derive data from them.\\n\\n3. **Field Existence Evaluation**: \\n   - Check the schema of each customer-related model to ensure that all necessary fields (e.g., customer ID, name, contact information) are present and correctly defined.\\n\\n4. **Assess Documentation Needs**: \\n   - Review existing documentation for the identified models. Identify gaps in documentation and create or update documentation to clearly describe model purpose, field definitions, and relationships.\\n\\n5. **Performance/Design Considerations**: \\n   - Evaluate if any of the models require optimization for performance. Consider whether changes in model granularity or keys (e.g., introducing unique keys) may enhance performance or facilitate better analytics.\\n\\n6. **Tests**: \\n   - Review existing tests on customer-related models. Identify any necessary tests to ensure data quality, such as unique key tests or not-null constraints, and implement them if missing.\\n\\n7. **dbt Project Config**: \\n   - Ensure that the dbt configuration files (dbt_project.yml, etc.) reflect appropriate settings for the identified models, including materializations and configurations that might impact performance.\\n\\n8. **Code or Logic Generation**: \\n   - If necessary, prepare SQL queries or CTEs that extract relevant customer-related data efficiently. This may also involve creating intermediate models if data transformations are complex.\\n\\nBy following these steps, you will effectively gather all pertinent information regarding customer-related models, ensuring comprehensive coverage of dependencies, documentation, and performance considerations.\", pydantic=None, json_dict=None, tasks_output=[TaskOutput(description=\"Evaluate the user's request: Give me all the information about the models related with customers and determine the required action: - Adding a field: Identify where the field is currently available (if provided),\\n  determine how to propagate it through necessary models or transformations,\\n  and assess the impact on related models and dependencies.\\n- Modifying an existing model: Identify specific changes needed, evaluate the\\n  impact on structure, relationships, and downstream dependencies.\\n- Retrieving specific information: Identify models containing relevant data,\\n  analyze relationships, and determine queries or transformations needed.\\n\\nReflect on the request to generate a concise plan for the approach and provide a clear summary of the required action and its implications.\\n\", name=None, expected_output='Return one of the following actions: - ADD_COLUMN - MODIFY_MODEL - RETRIVE_INFO\\nAdditionally, include a short explanation of the work required for the action. The output should be minimal and focused, avoiding extra commentary or filler text.\\n', summary=\"Evaluate the user's request: Give me all the information about...\", raw='RETRIEVE_INFO - The user is requesting all information about models related to customers. This will involve identifying the specific models that contain customer-related data, analyzing their relationships, and potentially formulating queries to extract the required information efficiently.', pydantic=None, json_dict=None, agent='Request Interpreter\\n', output_format=<OutputFormat.RAW: 'raw'>), TaskOutput(description='Evaluate the interpretation of the user request and determine the necessary steps to handle it effectively.  Include only relevant considerations based on the following topics: - Target models or files - Field existence - Documentation needs - Dependencies and relationships - Performance/design considerations - Tests - dbt project config - Code or logic generation\\nPossible actions and considerations may include: - Identifying the target models or files affected by the request. - Analyzing upstream and downstream dependencies to locate the change points. - Assessing the need for adjustments in documentation, tests, or configuration files. - Evaluating performance impacts and ensuring maintainability. - Generating necessary SQL logic fragments (e.g., CTEs, columns). - Determining if an intermediate model is required for the changes. - Highlighting potential granularity changes or unique key impacts.\\n', name=None, expected_output='A concise summary of the high-level tasks required to fulfill the request. If additional context (e.g., project structure or lineage) is needed, clearly state this and provide only reflections based on the given information.\\nAvoid filler text or unnecessary explanations, and focus exclusively on actionable insights.', summary='Evaluate the interpretation of the user request and determine the...', raw=\"To fulfill the user request for all information about models related to customers, follow these high-level actionable steps:\\n\\n1. **Identify Target Models or Files**: \\n   - Locate and list all dbt models that contain customer-related data. This could include models directly named 'customers' as well as any associated models such as 'customer_orders', 'customer_segments', or 'customer_addresses'.\\n\\n2. **Analyze Dependencies and Relationships**: \\n   - Examine upstream and downstream dependencies for each identified model using dbt's lineage graph. Document which models feed into customer-related models and which models derive data from them.\\n\\n3. **Field Existence Evaluation**: \\n   - Check the schema of each customer-related model to ensure that all necessary fields (e.g., customer ID, name, contact information) are present and correctly defined.\\n\\n4. **Assess Documentation Needs**: \\n   - Review existing documentation for the identified models. Identify gaps in documentation and create or update documentation to clearly describe model purpose, field definitions, and relationships.\\n\\n5. **Performance/Design Considerations**: \\n   - Evaluate if any of the models require optimization for performance. Consider whether changes in model granularity or keys (e.g., introducing unique keys) may enhance performance or facilitate better analytics.\\n\\n6. **Tests**: \\n   - Review existing tests on customer-related models. Identify any necessary tests to ensure data quality, such as unique key tests or not-null constraints, and implement them if missing.\\n\\n7. **dbt Project Config**: \\n   - Ensure that the dbt configuration files (dbt_project.yml, etc.) reflect appropriate settings for the identified models, including materializations and configurations that might impact performance.\\n\\n8. **Code or Logic Generation**: \\n   - If necessary, prepare SQL queries or CTEs that extract relevant customer-related data efficiently. This may also involve creating intermediate models if data transformations are complex.\\n\\nBy following these steps, you will effectively gather all pertinent information regarding customer-related models, ensuring comprehensive coverage of dependencies, documentation, and performance considerations.\", pydantic=None, json_dict=None, agent='Evaluation Specialist\\n', output_format=<OutputFormat.RAW: 'raw'>)], token_usage=UsageMetrics(total_tokens=2859, prompt_tokens=1795, cached_prompt_tokens=0, completion_tokens=1064, successful_requests=4))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_input = 'Give me all the information about the models related with customers'\n",
    "flow.kickoff(inputs={\"user_input\": user_input})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CREATE AGENT CHAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from crewai import Flow\n",
    "from crewai.flow.flow import listen, start, and_, or_, router\n",
    "\n",
    "class SalesPipeline(Flow):\n",
    "    \n",
    "  @start()\n",
    "  def fetch_leads(self):\n",
    "    # Pull our leads from the database\n",
    "    # This is a mock, in a real-world scenario, this is where you would\n",
    "    # fetch leads from a database\n",
    "    leads = [\n",
    "      {\n",
    "        \"lead_data\": {\n",
    "          \"name\": \"JoÃ£o Moura\",\n",
    "          \"job_title\": \"Director of Engineering\",\n",
    "          \"company\": \"Clearbit\",\n",
    "          \"email\": \"joao@clearbit.com\",\n",
    "          \"use_case\": \"Using AI Agent to do better data enrichment.\"\n",
    "        },\n",
    "      },\n",
    "    ]\n",
    "    return leads\n",
    "\n",
    "  @listen(fetch_leads)\n",
    "  def score_leads(self, leads):\n",
    "    scores = lead_scoring_crew.kickoff_for_each(leads)\n",
    "    self.state[\"score_crews_results\"] = scores\n",
    "    return scores\n",
    "\n",
    "  @listen(score_leads)\n",
    "  def store_leads_score(self, scores):\n",
    "    # Here we would store the scores in the database\n",
    "    return scores\n",
    "\n",
    "  @listen(score_leads)\n",
    "  def filter_leads(self, scores):\n",
    "    return [score for score in scores if score['lead_score'].score > 70]\n",
    "\n",
    "  @listen(and_(filter_leads, store_leads_score))\n",
    "  def log_leads(self, leads):\n",
    "    print(f\"Leads: {leads}\")\n",
    "\n",
    "  @router(filter_leads, paths=[\"high\", \"medium\", \"low\"])\n",
    "  def count_leads(self, scores):\n",
    "    if len(scores) > 10:\n",
    "      return 'high'\n",
    "    elif len(scores) > 5:\n",
    "      return 'medium'\n",
    "    else:\n",
    "      return 'low'\n",
    "\n",
    "  @listen('high')\n",
    "  def store_in_salesforce(self, leads):\n",
    "    return leads\n",
    "\n",
    "  @listen('medium')\n",
    "  def send_to_sales_team(self, leads):\n",
    "    return leads\n",
    "\n",
    "  @listen('low')\n",
    "  def write_email(self, leads):\n",
    "    scored_leads = [lead.to_dict() for lead in leads]\n",
    "    emails = email_writing_crew.kickoff_for_each(scored_leads)\n",
    "    return emails\n",
    "\n",
    "  @listen(write_email)\n",
    "  def send_email(self, emails):\n",
    "    # Here we would send the emails to the leads\n",
    "    return emails"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
