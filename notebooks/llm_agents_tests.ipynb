{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import yaml\n",
    "import sqlparse\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_repo_root_path():\n",
    "    import os\n",
    "    import sys\n",
    "    repo_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "    if repo_root not in sys.path:\n",
    "        sys.path.append(repo_root)\n",
    "        \n",
    "add_repo_root_path()\n",
    "from src import generate_knowledge\n",
    "from src import create_rag_db\n",
    "from src import llm_chain_tools\n",
    "from src.enhanced_retriever import EnhancedRetriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_columns', 50)\n",
    "#pd.set_option('display.width', None)\n",
    "#pd.set_option('display.max_colwidth', 10) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_knowledge.add_repo_root_path()\n",
    "import openai_setup\n",
    "\n",
    "OPENAI_API_KEY = openai_setup.conf['key']\n",
    "OPENAI_PROJECT = openai_setup.conf['project']\n",
    "OPENAI_ORGANIZATION = openai_setup.conf['organization']\n",
    "DEFAULT_LLM_MODEL = \"gpt-4o-mini\"\n",
    "CHROMADB_DIRECTORY = '../chromadb'\n",
    "COLLECTION_NAME = \"my_chromadb\" \n",
    "\n",
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
    "os.environ['OPENAI_MODEL_NAME'] = DEFAULT_LLM_MODEL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "langchain_openai_embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY, model=\"text-embedding-ada-002\")\n",
    "langchain_openai_llm = ChatOpenAI(model=DEFAULT_LLM_MODEL, temperature=0.1, openai_api_key=OPENAI_API_KEY, openai_organization = OPENAI_ORGANIZATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'interpretation_agent': {'role': 'Request Interpreter\\n', 'goal': 'Interpret user requests related to dbt projects and translate them into actionable decisions. Use expertise in dbt, data modeling, and analytics engineering to determine the type of action required.\\n', 'backstory': \"You specialize in analyzing requests to identify whether the action involves adding a field, modifying an existing model, or retrieving specific information. Your goal is to provide concise and actionable outputs tailored to the user's needs.\\n\", 'verbose': True, 'allow_delegation': False}, 'evaluation_agent': {'role': 'Evaluation Specialist\\n', 'goal': 'Evaluate user requests related to dbt projects and provide concise, actionable insights and steps required to address the request. Leverage expertise in data modeling, dbt project structure, and dependency analysis to ensure accurate evaluations.\\n', 'backstory': 'You specialize in analyzing interpreted requests and breaking them down into specific, actionable tasks while considering dependencies, performance, documentation, and configuration impacts within the dbt project. The goal is to ensure efficient and effective implementations.\\n', 'verbose': True, 'allow_delegation': False}, 'lineage_agent': {'role': 'Lineage Analysis Agent\\n', 'goal': 'Determine the primary dbt model directly affected by the request and identify the scope of models (UP, DOWN, or ALL) that need to be considered for handling the request effectively.\\n', 'backstory': 'You are an expert in evaluating dbt model dependencies, analyzing lineage, and interpreting metadata to define the scope of the request.  Your expertise ensures precise and actionable recommendations for managing the relationships between dbt models and their impact.\\n', 'verbose': True, 'allow_delegation': False}, 'plan_agent': {'role': 'Planning Agent\\n', 'goal': 'Create a detailed, step-by-step plan for implementing requested changes in the dbt project based on the lineage analysis and retrieved context.\\n', 'backstory': \"You specialize in breaking down complex requests into actionable plans that ensure alignment with the dbt project's structure,  dependencies, and conventions, while preventing schema-breaking changes.\\n\", 'verbose': True, 'allow_delegation': False}}\n",
      "{'interpretation_task': {'description': \"Evaluate the user's request: {request} and based on the evaluation, determine the required action: 1) adding a field -> return ADD_COLUMN 2) modifying an existing model -> return MODIFY_MODEL 3) retrieving and returning specific information. -> return RETRIVE_INFO\\nReflect on the request and provide a concise plan for the approach: - If the action involves adding a field: Identify where the field is currently available, if provided. - Determine how to propagate the field through the necessary models or transformations to integrate it into the target model. - Consider the impact on related models and dependencies. - If the action involves modifying an existing model: Identify the specific changes required. - Assess how these changes affect the structure, relationships, and downstream dependencies of the model. - If the action involves retrieving or returning information: Identify the models containing the relevant data. - Analyze how these models are related, and determine the queries or transformations needed to extract the requested information.\\n\", 'expected_output': 'Return one of the following actions: - ADD_COLUMN - MODIFY_MODEL - RETRIEVE_INFO\\nReturn the required action from the three proposed, and a short explanation of the required work. REMEMBER: Provide no extra commentary or explanation, just the minimal information required,   return only useful information, no additional filler text or unnecessary explanations, get to the point.\\n'}, 'evaluation_task': {'description': \"Based on the interpretation of an expert dbt and problem interpreter:  {interpretation}\\nof the original request {request}\\nInclude only these topics if relevant: - Target models or files - Field existence - Documentation needs - Dependencies and relationships - Performance/design considerations - Tests - dbt project config - Code or logic generation\\nSummarize only the necessary actions, no filler. Think about all the considerations and steps required to handle this request effectively. Your evaluation could include the following actions: - Identify the target model or models: Analyze the dependency tree to locate the model where the change should occur. - Understand its upstream sources (seeds, sources or base models) - Understand downstream dependencies of the model. - Documentation: Assess if it's going to be necessary to add adjustments to the documentation, and which would be this changes. - Unique keys and IDs: Examine the available unique keys and identifiers in the initial, intermediate, and final models. - Decide how these can be used to integrate the field or establish relationships between models. - Check if the granularity of the model can be altered through the changes added. - Evaluate performance and design: Review the data pipeline from start to finish. - Decide where the change or addition would be most optimal in terms of performance, data modeling, and maintainability. - Project state and impact: Consider the current state of the dbt project and how the changes might affect the broader model chain. - Macros and seeds: Check if relevant macros or seed data exist that can help transform or derive the required field or model. - Tests: Identify existing tests for the field or model and determine whether new tests need to be added or adjusted to validate the changes. - dbt project configuration: Review the general configuration (e.g., variables, environments, conventions) in the dbt_project.yml file to ensure the changes align with project standards and won't disrupt the schema. - Code generation: fragments of SQL logic needed, including CTEs or columns. - Evaluate whether an intermediate model is necessary or if the logic can be handled within the existing pipeline. - Documentation generation: Specify the documentation needed for any fields, models, or logic added or updated as part of this request. Only the things to add or change.\\n\", 'expected_output': \"Provide a concise summary of the high-level tasks based on your analysis, with the reflection of each one of them to be prepared to completed in the next steps when context is provided. If you don't have the info to perform the action because it's necessary context of the project, code or the lineage of the models, don't answer it, in next steps context will be provide as input to answer it properly. Mark it as it's necessart the context to responde it, and only do the reflection part using your logic as dbt expert. No extra checks or steps that are not on this list, select only the needed actions for the request. Return only useful information, no additional filler text or unnecessary explanations, get to the point.\\n      \\nREMEMBER: Provide no extra commentary or explanation, just the minimal information required.\\n\"}, 'lineage_task': {'description': 'Based on this user request: {request}\\n, the dbt expert evaluation of the request: {evaluation}\\nand the retrieved context about the request,  {retrieved_context}\\n(remember the meaning of all the context data: CONTEXT INFO AND METADATA MEANING: - knowledge_type: Specifies whether the resource is about files within the dbt project configuration, or data models used in the pipeline. - name: Name of the file. - path: Path within the repository where the file or resource is located, relative to the project root. - source: The original dataset, from which the model or resource pulls its data. Only for the first layer of the model. - parents: dbt models  that serve as input dependencies for the current model or file. - children: dbt models that depend on this resource as an input for their logic or data processing. - config: Configuration parameters defined in the file, specifying behaviors, sort keys, materialization... - materialized: Indicates how the data model is materialized (e.g., as a table, view, or ephemeral model) in the pipeline. - is_snapshot: Boolean flag that identifies whether the file represents a snapshot dbt model. - model_category: The logical dbt categorization of the model inside the project, such as base, staging, intermediate, marts... - vertical: Business domain or vertical to which the resource belongs, such as e-commerce, finance, supply... - has_tests: Boolean flag indicating if there are tests associated with this resource in the model .yaml file or in the tests folder. - has_select_all_in_last_select: Specifies if the final SQL query in the file uses a SELECT * statement. So all columns of previous CTEs will be consider as output of the model. - has_group_by: Boolean flag indicating if the SQL code includes a GROUP BY clause for aggregating data. - is_filtered: Boolean flag that specifies whether the resource applies filters to its data with WHERE, HAVING, JOIN... - is_source_model: The model uses the source macro to extract the data from the dbt project sources. It belongs to the first layer of all dbt models. - is_seed: Specifies if the resource is a seed file. - is_end_model: Boolean flag identifying if this is a terminal model in the data pipeline, representing the final output. - is_macro: Boolean flag indicating if the file defines a macro, used for reusable logic across the project. It does not count macros ref to call other models or source to connect to sources. - is_test: Indicates whether the file is a test sql definition used for testing purposes of other dbt models. - macros: Name of the macros used in the dbt model if any. It does not count macros ref to call other models or source to connect to sources. - packages: List of external packages or dependencies required for the resource or project functionality. Only not None in the packages.yml\\trow. )\\nDETERMINE: - The primary dbt model directly affected (e.g., where a field is added, a modification is made, or information is requested). - Whether upstream models (UP), downstream models (DOWN), or both (ALL) are necessary to handle this request correctly.\\nConsider the following cases: 1. If a new field is added, specify the model where the field will be added and indicate UP for upstream models needed to populate the field. 2. If an existing field is modified, specify the model where the change occurs and indicate DOWN for downstream models affected by the change. 3. If information is requested, specify the model containing the requested information and indicate UP, DOWN, or ALL based on the context of the data needed. 4. If a field or model is removed, specify the model being affected and indicate DOWN for downstream dependencies impacted.\\n      \\n', 'expected_output': \"Return the following json format: {{'model':model name, 'scope':UP/DOWN/ALL}} REMEMBER: Provide no extra commentary or explanation, just the minimal information required.\\n\"}, 'plan_task': {'description': \"Based on this evaluation:  {evaluation}\\nThe analysis of the most impacted model and the part of the lineage that is affected or has related info  (UP = Upstream models(parent models)/DOWN = Downstream models(childen models)/ALL = both):  {lineage_analysis}\\nRETRIEVED CONTEXT:  {retrieved_context}\\nthe retrieved context, remember the meaning of all the context data: CONTEXT INFO AND METADATA MEANING: - knowledge_type: Specifies whether the resource is about files within the dbt project configuration, or data models used in the pipeline. - name: Name of the file. - path: Path within the repository where the file or resource is located, relative to the project root. - source: The original dataset, from which the model or resource pulls its data. Only for the first layer of the model. - parents: dbt models  that serve as input dependencies for the current model or file. - children: dbt models that depend on this resource as an input for their logic or data processing. - config: Configuration parameters defined in the file, specifying behaviors, sort keys, materialization... - materialized: Indicates how the data model is materialized (e.g., as a table, view, or ephemeral model) in the pipeline. - is_snapshot: Boolean flag that identifies whether the file represents a snapshot dbt model. - model_category: The logical dbt categorization of the model inside the project, such as base, staging, intermediate, marts... - vertical: Business domain or vertical to which the resource belongs, such as e-commerce, finance, supply... - has_tests: Boolean flag indicating if there are tests associated with this resource in the model .yaml file or in the tests folder. - has_select_all_in_last_select: Specifies if the final SQL query in the file uses a SELECT * statement. So all columns of previous CTEs will be consider as output of the model. - has_group_by: Boolean flag indicating if the SQL code includes a GROUP BY clause for aggregating data. - is_filtered: Boolean flag that specifies whether the resource applies filters to its data with WHERE, HAVING, JOIN... - is_source_model: The model uses the source macro to extract the data from the dbt project sources. It belongs to the first layer of all dbt models. - is_seed: Specifies if the resource is a seed file. - is_end_model: Boolean flag identifying if this is a terminal model in the data pipeline, representing the final output. - is_macro: Boolean flag indicating if the file defines a macro, used for reusable logic across the project. It does not count macros ref to call other models or source to connect to sources. - is_test: Indicates whether the file is a test sql definition used for testing purposes of other dbt models. - macros: Name of the macros used in the dbt model if any. It does not count macros ref to call other models or source to connect to sources. - packages: List of external packages or dependencies required for the resource or project functionality. Only not None in the packages.yml\\trow.\\nand the examples about the some of the sources and the seeds (if any): {retrieved_csv_sources_context}\\nCreate a detailed step-by-step plan of the changes required in the existing models or files within the repository. To implement the requested change accurately. 1. Ensure that you only refer to files, models, or fields that are explicitly mentioned in the retrieved information. 2. Do not invent new models, fields, or dependencies.  3. Focus on:\\n  - Identifying the exact files or models that need modifications, based on the retrieved context.\\n  - Specifying what changes should be made, such as adding fields, updating logic, or modifying relationships.\\n  - Highlighting any dependencies between models or files and describing how these should be handled.\\n  - Extract children or parent models affected.\\n  - If code fragments are provided in the retrieved context, incorporate them where applicable and explain their role.\\n  - If no specific code or file is mentioned in the retrieved information, state that no changes should be made to existing files.\\n  - Ensure the changes align with the dbt project's standards, such as conventions in `dbt_project.yml`, and do not introduce schema-breaking modifications.\\n\\n4. No extra checks or steps that are not on this list. 5. Provide precise and actionable recommendations, avoiding any assumptions beyond the retrieved information.\\n      \\n\", 'expected_output': 'Return a summary of all the process, with the reflection, plan and context and the changes tht are neeeded to perfor\\n      - The original request.\\n      - The interpretarion of the request.\\n      - The affected models with the info of the context and the lineage.\\n      - All the necceasry changes step by step with a clear and short explanaiton of why is needed.\\nReturn only useful information, no additional filler text or unnecessary explanations, get to the point. REMEMBER: Provide no extra commentary or explanation, just the minimal information required.'}}\n"
     ]
    }
   ],
   "source": [
    "# Define file paths for YAML configurations\n",
    "files = {\n",
    "    'agents': '../config/agents.yml',\n",
    "    'tasks': '../config/tasks.yml'\n",
    "}\n",
    "\n",
    "# Load configurations from YAML files\n",
    "configs = {}\n",
    "for config_type, file_path in files.items():\n",
    "    with open(file_path, 'r') as file:\n",
    "        configs[config_type] = yaml.safe_load(file)\n",
    "\n",
    "# Assign loaded configurations to specific variables\n",
    "agents_config = configs['agents']\n",
    "tasks_config = configs['tasks']\n",
    "\n",
    "print(agents_config)\n",
    "print(tasks_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from crewai import Agent, Task, Crew"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Creating Agents\n",
    "interpretation_agent = Agent(\n",
    "  config=agents_config['interpretation_agent'],\n",
    ")\n",
    "\n",
    "evaluation_agent = Agent(\n",
    "  config=agents_config['evaluation_agent'],\n",
    ")\n",
    "\n",
    "lineage_agent = Agent(\n",
    "  config=agents_config['lineage_agent'],\n",
    ")\n",
    "\n",
    "plan_agent = Agent(\n",
    "  config=agents_config['plan_agent'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Tasks\n",
    "interpretation_task = Task(\n",
    "  config=tasks_config['interpretation_task'],\n",
    "  agent=interpretation_agent\n",
    ")\n",
    "\n",
    "evaluation_task = Task(\n",
    "  config=tasks_config['evaluation_task'],\n",
    "  agent=evaluation_agent\n",
    ")\n",
    "\n",
    "lineage_task = Task(\n",
    "  config=tasks_config['lineage_task'],\n",
    "  agent=lineage_agent\n",
    ")\n",
    "\n",
    "plan_task = Task(\n",
    "  config=tasks_config['plan_task'],\n",
    "  agent=plan_agent\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "crew = Crew(\n",
    "  agents=[\n",
    "    interpretation_agent,\n",
    "    evaluation_agent\n",
    "  ],\n",
    "  tasks=[\n",
    "    interpretation_task,\n",
    "    evaluation_task\n",
    "  ],\n",
    "  verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Missing required template variable 'interpretation' in description",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/opt/anaconda3/envs/rag-env/lib/python3.10/site-packages/crewai/task.py:477\u001b[0m, in \u001b[0;36mTask.interpolate_inputs\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    476\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 477\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdescription \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_original_description\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    478\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[0;31mKeyError\u001b[0m: 'interpretation'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 8\u001b[0m\n\u001b[1;32m      3\u001b[0m inputs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      4\u001b[0m   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrequest\u001b[39m\u001b[38;5;124m'\u001b[39m: user_input\n\u001b[1;32m      5\u001b[0m }\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Run the crew\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mcrew\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkickoff\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m  \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/rag-env/lib/python3.10/site-packages/crewai/crew.py:524\u001b[0m, in \u001b[0;36mCrew.kickoff\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    522\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inputs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    523\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inputs \u001b[38;5;241m=\u001b[39m inputs\n\u001b[0;32m--> 524\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_interpolate_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    525\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_tasks_callbacks()\n\u001b[1;32m    527\u001b[0m i18n \u001b[38;5;241m=\u001b[39m I18N(prompt_file\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprompt_file)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/rag-env/lib/python3.10/site-packages/crewai/crew.py:1048\u001b[0m, in \u001b[0;36mCrew._interpolate_inputs\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   1046\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_interpolate_inputs\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1047\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Interpolates the inputs in the tasks and agents.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1048\u001b[0m     [\n\u001b[1;32m   1049\u001b[0m         task\u001b[38;5;241m.\u001b[39minterpolate_inputs(\n\u001b[1;32m   1050\u001b[0m             \u001b[38;5;66;03m# type: ignore # \"interpolate_inputs\" of \"Task\" does not return a value (it only ever returns None)\u001b[39;00m\n\u001b[1;32m   1051\u001b[0m             inputs\n\u001b[1;32m   1052\u001b[0m         )\n\u001b[1;32m   1053\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtasks\n\u001b[1;32m   1054\u001b[0m     ]\n\u001b[1;32m   1055\u001b[0m     \u001b[38;5;66;03m# type: ignore # \"interpolate_inputs\" of \"Agent\" does not return a value (it only ever returns None)\u001b[39;00m\n\u001b[1;32m   1056\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m agent \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magents:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/rag-env/lib/python3.10/site-packages/crewai/crew.py:1049\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1046\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_interpolate_inputs\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1047\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Interpolates the inputs in the tasks and agents.\"\"\"\u001b[39;00m\n\u001b[1;32m   1048\u001b[0m     [\n\u001b[0;32m-> 1049\u001b[0m         \u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterpolate_inputs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1050\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# type: ignore # \"interpolate_inputs\" of \"Task\" does not return a value (it only ever returns None)\u001b[39;49;00m\n\u001b[1;32m   1051\u001b[0m \u001b[43m            \u001b[49m\u001b[43minputs\u001b[49m\n\u001b[1;32m   1052\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1053\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtasks\n\u001b[1;32m   1054\u001b[0m     ]\n\u001b[1;32m   1055\u001b[0m     \u001b[38;5;66;03m# type: ignore # \"interpolate_inputs\" of \"Agent\" does not return a value (it only ever returns None)\u001b[39;00m\n\u001b[1;32m   1056\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m agent \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magents:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/rag-env/lib/python3.10/site-packages/crewai/task.py:479\u001b[0m, in \u001b[0;36mTask.interpolate_inputs\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdescription \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_original_description\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\n\u001b[1;32m    478\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 479\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    480\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required template variable \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m in description\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    481\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError interpolating description: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: Missing required template variable 'interpretation' in description"
     ]
    }
   ],
   "source": [
    "user_input = 'Give me all the information about the models related with customers'\n",
    "\n",
    "inputs = {\n",
    "  'request': user_input\n",
    "}\n",
    "\n",
    "# Run the crew\n",
    "result = crew.kickoff(\n",
    "  inputs=inputs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Flows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from crewai import Flow\n",
    "from crewai.flow.flow import listen, start\n",
    "\n",
    "class dbtChatFlow(Flow):\n",
    "    @start()\n",
    "    def interpret_prompt(self):\n",
    "        user_prompt = self.state[\"user_input\"]\n",
    "        print(user_prompt)\n",
    "        interpretation_result = crew.kickoff(inputs = {'request': user_prompt} )\n",
    "        self.state[\"interpretation_result\"] = interpretation_result\n",
    "        return interpretation_result\n",
    "\n",
    "    @listen(lambda state: \"interpretation_result\" in state)\n",
    "    def evaluate_interpretation(self):\n",
    "        interpretation_result = self.state.get(\"interpretation_result\")\n",
    "        evaluation_result = crew.agents[1].kickoff({\"request\": interpretation_result})\n",
    "        self.state[\"evaluation_result\"] = evaluation_result\n",
    "        return evaluation_result\n",
    "\n",
    "flow = dbtChatFlow()\n",
    "#flow.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Give me all the information about the models related with customers\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mRequest Interpreter\u001b[00m\n",
      "\u001b[95m## Task:\u001b[00m \u001b[92mEvaluate the user's request: Give me all the information about the models related with customers and determine the required action: - Adding a field: Identify where the field is currently available (if provided),\n",
      "  determine how to propagate it through necessary models or transformations,\n",
      "  and assess the impact on related models and dependencies.\n",
      "- Modifying an existing model: Identify specific changes needed, evaluate the\n",
      "  impact on structure, relationships, and downstream dependencies.\n",
      "- Retrieving specific information: Identify models containing relevant data,\n",
      "  analyze relationships, and determine queries or transformations needed.\n",
      "\n",
      "Reflect on the request to generate a concise plan for the approach and provide a clear summary of the required action and its implications.\n",
      "\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mRequest Interpreter\u001b[00m\n",
      "\u001b[95m## Final Answer:\u001b[00m \u001b[92m\n",
      "RETRIEVE_INFO - The user is requesting all information about models related to customers. This will involve identifying the specific models that contain customer-related data, analyzing their relationships, and potentially formulating queries to extract the required information efficiently.\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mEvaluation Specialist\u001b[00m\n",
      "\u001b[95m## Task:\u001b[00m \u001b[92mEvaluate the interpretation of the user request and determine the necessary steps to handle it effectively.  Include only relevant considerations based on the following topics: - Target models or files - Field existence - Documentation needs - Dependencies and relationships - Performance/design considerations - Tests - dbt project config - Code or logic generation\n",
      "Possible actions and considerations may include: - Identifying the target models or files affected by the request. - Analyzing upstream and downstream dependencies to locate the change points. - Assessing the need for adjustments in documentation, tests, or configuration files. - Evaluating performance impacts and ensuring maintainability. - Generating necessary SQL logic fragments (e.g., CTEs, columns). - Determining if an intermediate model is required for the changes. - Highlighting potential granularity changes or unique key impacts.\n",
      "\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mEvaluation Specialist\u001b[00m\n",
      "\u001b[95m## Final Answer:\u001b[00m \u001b[92m\n",
      "To fulfill the user request for all information about models related to customers, follow these high-level actionable steps:\n",
      "\n",
      "1. **Identify Target Models or Files**: \n",
      "   - Locate and list all dbt models that contain customer-related data. This could include models directly named 'customers' as well as any associated models such as 'customer_orders', 'customer_segments', or 'customer_addresses'.\n",
      "\n",
      "2. **Analyze Dependencies and Relationships**: \n",
      "   - Examine upstream and downstream dependencies for each identified model using dbt's lineage graph. Document which models feed into customer-related models and which models derive data from them.\n",
      "\n",
      "3. **Field Existence Evaluation**: \n",
      "   - Check the schema of each customer-related model to ensure that all necessary fields (e.g., customer ID, name, contact information) are present and correctly defined.\n",
      "\n",
      "4. **Assess Documentation Needs**: \n",
      "   - Review existing documentation for the identified models. Identify gaps in documentation and create or update documentation to clearly describe model purpose, field definitions, and relationships.\n",
      "\n",
      "5. **Performance/Design Considerations**: \n",
      "   - Evaluate if any of the models require optimization for performance. Consider whether changes in model granularity or keys (e.g., introducing unique keys) may enhance performance or facilitate better analytics.\n",
      "\n",
      "6. **Tests**: \n",
      "   - Review existing tests on customer-related models. Identify any necessary tests to ensure data quality, such as unique key tests or not-null constraints, and implement them if missing.\n",
      "\n",
      "7. **dbt Project Config**: \n",
      "   - Ensure that the dbt configuration files (dbt_project.yml, etc.) reflect appropriate settings for the identified models, including materializations and configurations that might impact performance.\n",
      "\n",
      "8. **Code or Logic Generation**: \n",
      "   - If necessary, prepare SQL queries or CTEs that extract relevant customer-related data efficiently. This may also involve creating intermediate models if data transformations are complex.\n",
      "\n",
      "By following these steps, you will effectively gather all pertinent information regarding customer-related models, ensuring comprehensive coverage of dependencies, documentation, and performance considerations.\u001b[00m\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CrewOutput(raw=\"To fulfill the user request for all information about models related to customers, follow these high-level actionable steps:\\n\\n1. **Identify Target Models or Files**: \\n   - Locate and list all dbt models that contain customer-related data. This could include models directly named 'customers' as well as any associated models such as 'customer_orders', 'customer_segments', or 'customer_addresses'.\\n\\n2. **Analyze Dependencies and Relationships**: \\n   - Examine upstream and downstream dependencies for each identified model using dbt's lineage graph. Document which models feed into customer-related models and which models derive data from them.\\n\\n3. **Field Existence Evaluation**: \\n   - Check the schema of each customer-related model to ensure that all necessary fields (e.g., customer ID, name, contact information) are present and correctly defined.\\n\\n4. **Assess Documentation Needs**: \\n   - Review existing documentation for the identified models. Identify gaps in documentation and create or update documentation to clearly describe model purpose, field definitions, and relationships.\\n\\n5. **Performance/Design Considerations**: \\n   - Evaluate if any of the models require optimization for performance. Consider whether changes in model granularity or keys (e.g., introducing unique keys) may enhance performance or facilitate better analytics.\\n\\n6. **Tests**: \\n   - Review existing tests on customer-related models. Identify any necessary tests to ensure data quality, such as unique key tests or not-null constraints, and implement them if missing.\\n\\n7. **dbt Project Config**: \\n   - Ensure that the dbt configuration files (dbt_project.yml, etc.) reflect appropriate settings for the identified models, including materializations and configurations that might impact performance.\\n\\n8. **Code or Logic Generation**: \\n   - If necessary, prepare SQL queries or CTEs that extract relevant customer-related data efficiently. This may also involve creating intermediate models if data transformations are complex.\\n\\nBy following these steps, you will effectively gather all pertinent information regarding customer-related models, ensuring comprehensive coverage of dependencies, documentation, and performance considerations.\", pydantic=None, json_dict=None, tasks_output=[TaskOutput(description=\"Evaluate the user's request: Give me all the information about the models related with customers and determine the required action: - Adding a field: Identify where the field is currently available (if provided),\\n  determine how to propagate it through necessary models or transformations,\\n  and assess the impact on related models and dependencies.\\n- Modifying an existing model: Identify specific changes needed, evaluate the\\n  impact on structure, relationships, and downstream dependencies.\\n- Retrieving specific information: Identify models containing relevant data,\\n  analyze relationships, and determine queries or transformations needed.\\n\\nReflect on the request to generate a concise plan for the approach and provide a clear summary of the required action and its implications.\\n\", name=None, expected_output='Return one of the following actions: - ADD_COLUMN - MODIFY_MODEL - RETRIVE_INFO\\nAdditionally, include a short explanation of the work required for the action. The output should be minimal and focused, avoiding extra commentary or filler text.\\n', summary=\"Evaluate the user's request: Give me all the information about...\", raw='RETRIEVE_INFO - The user is requesting all information about models related to customers. This will involve identifying the specific models that contain customer-related data, analyzing their relationships, and potentially formulating queries to extract the required information efficiently.', pydantic=None, json_dict=None, agent='Request Interpreter\\n', output_format=<OutputFormat.RAW: 'raw'>), TaskOutput(description='Evaluate the interpretation of the user request and determine the necessary steps to handle it effectively.  Include only relevant considerations based on the following topics: - Target models or files - Field existence - Documentation needs - Dependencies and relationships - Performance/design considerations - Tests - dbt project config - Code or logic generation\\nPossible actions and considerations may include: - Identifying the target models or files affected by the request. - Analyzing upstream and downstream dependencies to locate the change points. - Assessing the need for adjustments in documentation, tests, or configuration files. - Evaluating performance impacts and ensuring maintainability. - Generating necessary SQL logic fragments (e.g., CTEs, columns). - Determining if an intermediate model is required for the changes. - Highlighting potential granularity changes or unique key impacts.\\n', name=None, expected_output='A concise summary of the high-level tasks required to fulfill the request. If additional context (e.g., project structure or lineage) is needed, clearly state this and provide only reflections based on the given information.\\nAvoid filler text or unnecessary explanations, and focus exclusively on actionable insights.', summary='Evaluate the interpretation of the user request and determine the...', raw=\"To fulfill the user request for all information about models related to customers, follow these high-level actionable steps:\\n\\n1. **Identify Target Models or Files**: \\n   - Locate and list all dbt models that contain customer-related data. This could include models directly named 'customers' as well as any associated models such as 'customer_orders', 'customer_segments', or 'customer_addresses'.\\n\\n2. **Analyze Dependencies and Relationships**: \\n   - Examine upstream and downstream dependencies for each identified model using dbt's lineage graph. Document which models feed into customer-related models and which models derive data from them.\\n\\n3. **Field Existence Evaluation**: \\n   - Check the schema of each customer-related model to ensure that all necessary fields (e.g., customer ID, name, contact information) are present and correctly defined.\\n\\n4. **Assess Documentation Needs**: \\n   - Review existing documentation for the identified models. Identify gaps in documentation and create or update documentation to clearly describe model purpose, field definitions, and relationships.\\n\\n5. **Performance/Design Considerations**: \\n   - Evaluate if any of the models require optimization for performance. Consider whether changes in model granularity or keys (e.g., introducing unique keys) may enhance performance or facilitate better analytics.\\n\\n6. **Tests**: \\n   - Review existing tests on customer-related models. Identify any necessary tests to ensure data quality, such as unique key tests or not-null constraints, and implement them if missing.\\n\\n7. **dbt Project Config**: \\n   - Ensure that the dbt configuration files (dbt_project.yml, etc.) reflect appropriate settings for the identified models, including materializations and configurations that might impact performance.\\n\\n8. **Code or Logic Generation**: \\n   - If necessary, prepare SQL queries or CTEs that extract relevant customer-related data efficiently. This may also involve creating intermediate models if data transformations are complex.\\n\\nBy following these steps, you will effectively gather all pertinent information regarding customer-related models, ensuring comprehensive coverage of dependencies, documentation, and performance considerations.\", pydantic=None, json_dict=None, agent='Evaluation Specialist\\n', output_format=<OutputFormat.RAW: 'raw'>)], token_usage=UsageMetrics(total_tokens=2859, prompt_tokens=1795, cached_prompt_tokens=0, completion_tokens=1064, successful_requests=4))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_input = 'Give me all the information about the models related with customers'\n",
    "flow.kickoff(inputs={\"user_input\": user_input})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CREATE AGENT CHAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configure Prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/hv/ckh3m6gn1sd45q2qctrqcwzh0000gn/T/ipykernel_1563/2024057643.py:3: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  loaded_vectorstore = Chroma(\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "loaded_vectorstore = Chroma(\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    persist_directory=CHROMADB_DIRECTORY,\n",
    "    embedding_function=langchain_openai_embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, repo_name = generate_knowledge.extract_owner_and_repo('https://github.com/dbt-labs/jaffle-shop')\n",
    "dbt_models_df = pd.read_csv('../data/dbt_models_' + repo_name + '.csv')\n",
    "dbt_project_df = pd.read_csv('../data/dbt_project_' + repo_name + '.csv')\n",
    "dbt_repo_knowledge_df = create_rag_db.merge_dbt_models_and_project_dfs(dbt_models_df, dbt_project_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = EnhancedRetriever(vectorstore = loaded_vectorstore, embedding_function= langchain_openai_embeddings)\n",
    "\n",
    "query = \"give me all the models related with the dbt model orders\"\n",
    "final_context, top_documents = retriever.retrieve(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create agents, tasks and flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Agents\n",
    "interpretation_agent = Agent(\n",
    "  config=agents_config['interpretation_agent'],\n",
    ")\n",
    "\n",
    "evaluation_agent = Agent(\n",
    "  config=agents_config['evaluation_agent'],\n",
    ")\n",
    "\n",
    "lineage_agent = Agent(\n",
    "  config=agents_config['lineage_agent'],\n",
    ")\n",
    "\n",
    "plan_agent = Agent(\n",
    "  config=agents_config['plan_agent'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Tasks\n",
    "interpretation_task = Task(\n",
    "  config=tasks_config['interpretation_task'],\n",
    "  agent=interpretation_agent\n",
    ")\n",
    "\n",
    "evaluation_task = Task(\n",
    "  config=tasks_config['evaluation_task'],\n",
    "  agent=evaluation_agent\n",
    ")\n",
    "\n",
    "lineage_task = Task(\n",
    "  config=tasks_config['lineage_task'],\n",
    "  agent=lineage_agent\n",
    ")\n",
    "\n",
    "plan_task = Task(\n",
    "  config=tasks_config['plan_task'],\n",
    "  agent=plan_agent\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overriding of current TracerProvider is not allowed\n",
      "Overriding of current TracerProvider is not allowed\n",
      "Overriding of current TracerProvider is not allowed\n",
      "Overriding of current TracerProvider is not allowed\n"
     ]
    }
   ],
   "source": [
    "interpretation_crew = Crew(agents = [interpretation_agent], tasks = [interpretation_task], verbose = True)\n",
    "evaluation_crew = Crew(agents = [evaluation_agent], tasks = [evaluation_task], verbose = True)\n",
    "lineage_crew = Crew(agents = [lineage_agent], tasks = [lineage_task], verbose = True)\n",
    "plan_crew = Crew(agents = [plan_agent], tasks = [plan_task], verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plot saved as crewai_flow.html\n"
     ]
    }
   ],
   "source": [
    "from crewai import Flow\n",
    "from crewai.flow.flow import listen, start\n",
    "\n",
    "class dbtChatFlow(Flow):\n",
    "    @start()\n",
    "    def interpret_prompt(self):\n",
    "        request = self.state[\"request\"]\n",
    "        interpretation = interpretation_crew.kickoff(inputs = {'request': request})\n",
    "        self.state[\"interpretation\"] = interpretation\n",
    "        return interpretation\n",
    "\n",
    "    @listen(interpret_prompt)\n",
    "    def evaluate_interpretation(self):\n",
    "        request = self.state[\"request\"]\n",
    "        interpretation = self.state.get(\"interpretation\")\n",
    "        evaluation = evaluation_crew.kickoff(inputs = {'request': request, \"interpretation\": interpretation})\n",
    "        self.state[\"evaluation\"] = evaluation\n",
    "        return evaluation\n",
    "    \n",
    "    @listen(evaluate_interpretation)\n",
    "    def retrieve_general_context_for_lineage_calculation(self):\n",
    "        request = self.state[\"request\"]\n",
    "        vectorstore = self.state[\"vectorstore\"]\n",
    "        embedding_function = self.state[\"embedding_function\"]\n",
    "        retriever = EnhancedRetriever(vectorstore = vectorstore, embedding_function= embedding_function)\n",
    "        retrieved_context, retrieved_documents = retriever.retrieve(request)\n",
    "        retrieved_context = \"\\n\".join([doc.page_content for doc in retrieved_documents if hasattr(doc, 'page_content')])\n",
    "        self.state[\"retrieved_context\"] = retrieved_context\n",
    "        return retrieved_context\n",
    "\n",
    "    @listen(retrieve_general_context_for_lineage_calculation)\n",
    "    def get_lineage(self):\n",
    "        request = self.state[\"request\"]\n",
    "        evaluation = self.state.get(\"evaluation\")\n",
    "        \n",
    "        retrieved_context = self.state.get(\"retrieved_context\")\n",
    "        lineage_analysis = lineage_crew.kickoff(inputs = {'request': request, 'evaluation': str(evaluation), 'retrieved_context':retrieved_context})\n",
    "        self.state[\"lineage_analysis\"] = lineage_analysis\n",
    "        return lineage_analysis\n",
    "\n",
    "flow = dbtChatFlow()\n",
    "flow.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mRequest Interpreter\u001b[00m\n",
      "\u001b[95m## Task:\u001b[00m \u001b[92mEvaluate the user's request: Give me all the information about the models related with customers and based on the evaluation, determine the required action: 1) adding a field -> return ADD_COLUMN 2) modifying an existing model -> return MODIFY_MODEL 3) retrieving and returning specific information. -> return RETRIVE_INFO\n",
      "Reflect on the request and provide a concise plan for the approach: - If the action involves adding a field: Identify where the field is currently available, if provided. - Determine how to propagate the field through the necessary models or transformations to integrate it into the target model. - Consider the impact on related models and dependencies. - If the action involves modifying an existing model: Identify the specific changes required. - Assess how these changes affect the structure, relationships, and downstream dependencies of the model. - If the action involves retrieving or returning information: Identify the models containing the relevant data. - Analyze how these models are related, and determine the queries or transformations needed to extract the requested information.\n",
      "\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mRequest Interpreter\u001b[00m\n",
      "\u001b[95m## Final Answer:\u001b[00m \u001b[92m\n",
      "RETRIEVE_INFO - The request is to gather and return all relevant information about models related to customers, which involves analyzing relationships and extracting data from those models without modifying or adding fields.\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mEvaluation Specialist\u001b[00m\n",
      "\u001b[95m## Task:\u001b[00m \u001b[92mBased on the interpretation of an expert dbt and problem interpreter:  RETRIEVE_INFO - The request is to gather and return all relevant information about models related to customers, which involves analyzing relationships and extracting data from those models without modifying or adding fields.\n",
      "of the original request Give me all the information about the models related with customers\n",
      "Include only these topics if relevant: - Target models or files - Field existence - Documentation needs - Dependencies and relationships - Performance/design considerations - Tests - dbt project config - Code or logic generation\n",
      "Summarize only the necessary actions, no filler. Think about all the considerations and steps required to handle this request effectively. Your evaluation could include the following actions: - Identify the target model or models: Analyze the dependency tree to locate the model where the change should occur. - Understand its upstream sources (seeds, sources or base models) - Understand downstream dependencies of the model. - Documentation: Assess if it's going to be necessary to add adjustments to the documentation, and which would be this changes. - Unique keys and IDs: Examine the available unique keys and identifiers in the initial, intermediate, and final models. - Decide how these can be used to integrate the field or establish relationships between models. - Check if the granularity of the model can be altered through the changes added. - Evaluate performance and design: Review the data pipeline from start to finish. - Decide where the change or addition would be most optimal in terms of performance, data modeling, and maintainability. - Project state and impact: Consider the current state of the dbt project and how the changes might affect the broader model chain. - Macros and seeds: Check if relevant macros or seed data exist that can help transform or derive the required field or model. - Tests: Identify existing tests for the field or model and determine whether new tests need to be added or adjusted to validate the changes. - dbt project configuration: Review the general configuration (e.g., variables, environments, conventions) in the dbt_project.yml file to ensure the changes align with project standards and won't disrupt the schema. - Code generation: fragments of SQL logic needed, including CTEs or columns. - Evaluate whether an intermediate model is necessary or if the logic can be handled within the existing pipeline. - Documentation generation: Specify the documentation needed for any fields, models, or logic added or updated as part of this request. Only the things to add or change.\n",
      "\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mEvaluation Specialist\u001b[00m\n",
      "\u001b[95m## Final Answer:\u001b[00m \u001b[92m\n",
      "1. Identify Target Models: Analyze the dbt project to find all models directly related to customers.  \n",
      "2. Understand Upstream Sources: Map out seeds, sources, or base models used by these customer-related models.  \n",
      "3. Assess Downstream Dependencies: Identify models that depend on the customer-related models to understand the full impact of potential changes.  \n",
      "4. Documentation Needs: Review existing documentation for customer models and determine if new documentation or amendments are required.  \n",
      "5. Examine Unique Keys and IDs: Identify unique identifiers in the customer models to establish relationships and integration points.  \n",
      "6. Evaluate Granularity: Determine if existing models can accommodate the required granularity for relevant customer data.  \n",
      "7. Performance and Design: Analyze the existing data pipeline to identify potential improvements or optimizations.  \n",
      "8. Check for Existing Macros and Seeds: Investigate if there are any existing macros or seed data that could assist in deriving customer-related information.  \n",
      "9. Identify Existing Tests: Review current tests for customer models to assess if they need modifications for validation after the information retrieval.  \n",
      "10. Review dbt Project Config: Examine the dbt_project.yml file for relevant configurations to ensure alignment with any documentation or model changes.  \n",
      "11. Code Generation: Consider what SQL logic fragments (CTEs, columns) might be needed to extract relevant customer information.  \n",
      "12. Evaluate Intermediate Model Necessity: Decide if an intermediate model is warranted for handling the information extraction process.  \n",
      "13. Documentation Generation: Specify any new documentation required for fields or models altered in this request.  \n",
      "\n",
      "Note: Additional context from the dbt project is necessary to complete the specific actions outlined above.\u001b[00m\n",
      "\n",
      "\n",
      "[Flow._execute_single_listener] Error in method get_lineage: Error interpolating expected_output: Error during string interpolation: Value for key 'evaluation' must be a string, integer, or float, got CrewOutput\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/jobandtalent/opt/anaconda3/envs/rag-env/lib/python3.10/site-packages/crewai/task.py\", line 535, in interpolate_only\n",
      "    raise ValueError(\n",
      "ValueError: Value for key 'evaluation' must be a string, integer, or float, got CrewOutput\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/jobandtalent/opt/anaconda3/envs/rag-env/lib/python3.10/site-packages/crewai/task.py\", line 486, in interpolate_inputs\n",
      "    self.expected_output = self.interpolate_only(\n",
      "  File \"/Users/jobandtalent/opt/anaconda3/envs/rag-env/lib/python3.10/site-packages/crewai/task.py\", line 550, in interpolate_only\n",
      "    raise ValueError(f\"Error during string interpolation: {str(e)}\") from e\n",
      "ValueError: Error during string interpolation: Value for key 'evaluation' must be a string, integer, or float, got CrewOutput\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/jobandtalent/opt/anaconda3/envs/rag-env/lib/python3.10/site-packages/crewai/flow/flow.py\", line 656, in _execute_single_listener\n",
      "    listener_result = await self._execute_method(listener_name, method)\n",
      "  File \"/Users/jobandtalent/opt/anaconda3/envs/rag-env/lib/python3.10/site-packages/crewai/flow/flow.py\", line 493, in _execute_method\n",
      "    else method(*args, **kwargs)\n",
      "  File \"/var/folders/hv/ckh3m6gn1sd45q2qctrqcwzh0000gn/T/ipykernel_1563/3843378181.py\", line 37, in get_lineage\n",
      "    lineage_analysis = lineage_crew.kickoff(inputs = {'request': request, 'evaluation': evaluation, 'retrieved_context':retrieved_context})\n",
      "  File \"/Users/jobandtalent/opt/anaconda3/envs/rag-env/lib/python3.10/site-packages/crewai/crew.py\", line 524, in kickoff\n",
      "    self._interpolate_inputs(inputs)\n",
      "  File \"/Users/jobandtalent/opt/anaconda3/envs/rag-env/lib/python3.10/site-packages/crewai/crew.py\", line 1048, in _interpolate_inputs\n",
      "    [\n",
      "  File \"/Users/jobandtalent/opt/anaconda3/envs/rag-env/lib/python3.10/site-packages/crewai/crew.py\", line 1049, in <listcomp>\n",
      "    task.interpolate_inputs(\n",
      "  File \"/Users/jobandtalent/opt/anaconda3/envs/rag-env/lib/python3.10/site-packages/crewai/task.py\", line 490, in interpolate_inputs\n",
      "    raise ValueError(f\"Error interpolating expected_output: {str(e)}\") from e\n",
      "ValueError: Error interpolating expected_output: Error during string interpolation: Value for key 'evaluation' must be a string, integer, or float, got CrewOutput\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'{\\'models\\': [{\\'name\\': \\'customers\\', \\'description\\': \\'Customer overview data mart, offering key details for each unique customer. One row per customer.\\', \\'data_tests\\': [{\\'dbt_utils.expression_is_true\\': {\\'expression\\': \\'lifetime_spend_pretax + lifetime_tax_paid = lifetime_spend\\'}}], \\'columns\\': [{\\'name\\': \\'customer_id\\', \\'description\\': \\'The unique key of the orders mart.\\', \\'data_tests\\': [\\'not_null\\', \\'unique\\']}, {\\'name\\': \\'customer_name\\', \\'description\\': \"Customers\\' full name.\"}, {\\'name\\':\\n<MODEL CODE>:\\n        .sql Code:\\n        WITH customers AS\\n  (SELECT *\\n   FROM {{ ref(\\'stg_customers\\') }}),\\n     orders AS\\n  (SELECT *\\n   FROM {{ ref(\\'orders\\') }}),\\n     customer_orders_summary AS\\n  (SELECT orders.customer_id,\\n          count(DISTINCT orders.order_id) AS count_lifetime_orders,\\n          count(DISTINCT orders.order_id) > 1 AS is_repeat_buyer,\\n          min(orders.ordered_at) AS first_ordered_at,\\n          max(orders.ordered_at) AS last_ordered_at,\\n<MODEL CODE>:\\n        .sql Code:\\n                                          id                             customer          ordered_at                             store_id  subtotal  tax_paid  order_total\\n9bed808a-5074-4dfb-b1eb-388e2e60a6da 50a2d1c4-d788-4498-a6f7-dd75d4db588f 2016-09-01T15:01:00 4b6c2304-2b9e-41e4-942a-cf11a1819378       700        42          742\\n<MODEL DEPENDENCIES>:\\n        Downstream models:\\n        [\\'customers\\']\\n\\n        Upstream models:\\n        [\\'stg_orders\\', \\'order_items\\']\\n{\\'models\\': [{\\'name\\': \\'orders\\', \\'description\\': \"Order overview data mart, offering key details for each order inlcluding if it\\'s a customer\\'s first order and a food vs. drink item breakdown. One row per order.\", \\'data_tests\\': [{\\'dbt_utils.expression_is_true\\': {\\'expression\\': \\'order_items_subtotal = subtotal\\'}}, {\\'dbt_utils.expression_is_true\\': {\\'expression\\': \\'order_total = subtotal + tax_paid\\'}}], \\'columns\\': [{\\'name\\': \\'order_id\\', \\'description\\': \\'The unique key of the orders mart.\\','"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_input = 'Give me all the information about the models related with customers'\n",
    "flow.kickoff(inputs={\"request\": user_input, \"vectorstore\": loaded_vectorstore, \"embedding_function\":langchain_openai_embeddings})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from crewai import Flow\n",
    "from crewai.flow.flow import listen, start, and_, or_, router\n",
    "\n",
    "class SalesPipeline(Flow):\n",
    "    \n",
    "  @start()\n",
    "  def fetch_leads(self):\n",
    "    # Pull our leads from the database\n",
    "    # This is a mock, in a real-world scenario, this is where you would\n",
    "    # fetch leads from a database\n",
    "    leads = [\n",
    "      {\n",
    "        \"lead_data\": {\n",
    "          \"name\": \"Joo Moura\",\n",
    "          \"job_title\": \"Director of Engineering\",\n",
    "          \"company\": \"Clearbit\",\n",
    "          \"email\": \"joao@clearbit.com\",\n",
    "          \"use_case\": \"Using AI Agent to do better data enrichment.\"\n",
    "        },\n",
    "      },\n",
    "    ]\n",
    "    return leads\n",
    "\n",
    "  @listen(fetch_leads)\n",
    "  def score_leads(self, leads):\n",
    "    scores = lead_scoring_crew.kickoff_for_each(leads)\n",
    "    self.state[\"score_crews_results\"] = scores\n",
    "    return scores\n",
    "\n",
    "  @listen(score_leads)\n",
    "  def store_leads_score(self, scores):\n",
    "    # Here we would store the scores in the database\n",
    "    return scores\n",
    "\n",
    "  @listen(score_leads)\n",
    "  def filter_leads(self, scores):\n",
    "    return [score for score in scores if score['lead_score'].score > 70]\n",
    "\n",
    "  @listen(and_(filter_leads, store_leads_score))\n",
    "  def log_leads(self, leads):\n",
    "    print(f\"Leads: {leads}\")\n",
    "\n",
    "  @router(filter_leads, paths=[\"high\", \"medium\", \"low\"])\n",
    "  def count_leads(self, scores):\n",
    "    if len(scores) > 10:\n",
    "      return 'high'\n",
    "    elif len(scores) > 5:\n",
    "      return 'medium'\n",
    "    else:\n",
    "      return 'low'\n",
    "\n",
    "  @listen('high')\n",
    "  def store_in_salesforce(self, leads):\n",
    "    return leads\n",
    "\n",
    "  @listen('medium')\n",
    "  def send_to_sales_team(self, leads):\n",
    "    return leads\n",
    "\n",
    "  @listen('low')\n",
    "  def write_email(self, leads):\n",
    "    scored_leads = [lead.to_dict() for lead in leads]\n",
    "    emails = email_writing_crew.kickoff_for_each(scored_leads)\n",
    "    return emails\n",
    "\n",
    "  @listen(write_email)\n",
    "  def send_email(self, emails):\n",
    "    # Here we would send the emails to the leads\n",
    "    return emails"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
